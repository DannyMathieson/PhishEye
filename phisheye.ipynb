{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PhishEye"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import dnstwist\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2158"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzz = dnstwist.Fuzzer(\"www.google.com\")\n",
    "fuzz.generate()\n",
    "len(fuzz.permutations())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dnstwist.run(domain='google.com', registered=True, format='null')\n",
    "reg = [d['domain'] for d in data]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_non = dnstwist.run(domain='google.com', unregistered=True, format='null')\n",
    "nonreg = [d['domain'] for d in data_non]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersection(lst1, lst2):\n",
    "    return list(set(lst1) & set(lst2))\n",
    "\n",
    "intersection(reg, nonreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (f'The number of registered permutations is: {len(reg)}')\n",
    "print (f'The number of non registered permutations is: {len(nonreg)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (reg[::15])\n",
    "print (nonreg[::150])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domains_df = pd.read_csv('./top-1m.csv', header=None, index_col=0)\n",
    "domains_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dicts(domain_list):\n",
    "    reg_dict, nonreg_dict = {}, {}\n",
    "    for domain in domain_list:\n",
    "        data_reg = dnstwist.run(domain=f'{domain}', registered=True, format='null')\n",
    "        reg = [d['domain'] for d in data_reg]\n",
    "        reg_dict[domain]  = len(reg)\n",
    "        data_nonreg = dnstwist.run(domain=f'{domain}', unregistered=True, format='null')\n",
    "        nonreg = [d['domain'] for d in data_nonreg]\n",
    "        nonreg_dict[domain]  = len(nonreg)\n",
    "    return reg_dict, nonreg_dict\n",
    "eda_reg, eda_nonreg = get_dicts(list(domains_df[1].values[:10]))\n",
    "eda_reg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = list(eda_reg.keys())\n",
    "Yreg = list(eda_reg.values())\n",
    "Znonreg= list(eda_nonreg.values())\n",
    "X_axis = np.arange(len(x))\n",
    "  \n",
    "plt.bar(x, Znonreg, color='steelblue')\n",
    "plt.bar(x, Yreg, bottom=Znonreg, color='darkorange')\n",
    "  \n",
    "plt.xlabel(\"Domains\")\n",
    "plt.ylabel(\"Number of Permutations\")\n",
    "plt.title(\"Number of Registered and Non Registered Domain Permutations\")\n",
    "plt.xticks(rotation=30)\n",
    "\n",
    "plt.legend(labels = ['Non Registered: Benign', 'Registered: Malicious'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_twist_dict(domains):\n",
    "    twist_dict = {}\n",
    "    for domain in domains:\n",
    "        #twist_dict[domain] = [[],[]]\n",
    "        data_reg = dnstwist.run(domain=f'{domain}', registered=True, format='null')\n",
    "        reg = [d['domain'] for d in data_reg]\n",
    "        for homograph in reg:\n",
    "            twist_dict[homograph] = [domain, True]\n",
    "\n",
    "        data_nonreg = dnstwist.run(domain=f'{domain}', unregistered=True, format='null')\n",
    "        nonreg = [d['domain'] for d in data_nonreg]\n",
    "        for homograph in nonreg:\n",
    "            twist_dict[homograph] = [domain, False]\n",
    "    return twist_dict\n",
    "\n",
    "twisted_dict = create_twist_dict(list(domains_df[1].values[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twisted_df = pd.DataFrame.from_dict(twisted_dict, orient='index').reset_index()\n",
    "twisted_df.columns = ['Homograph', 'Domain', 'Registered']\n",
    "twisted_df.to_csv('twisted.csv')\n",
    "twisted_df\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linguistic Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'reg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/x3/7twk98h15d59hbdjjl0xw9g40000gn/T/ipykernel_26515/1853765964.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mstrsimpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcosine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCosine\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtest_string1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# google.com\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mtest_string2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnonreg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# g00qle.com\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'reg' is not defined"
     ]
    }
   ],
   "source": [
    "from strsimpy.levenshtein import Levenshtein\n",
    "from strsimpy.jaro_winkler import JaroWinkler\n",
    "from strsimpy.sorensen_dice import SorensenDice\n",
    "from strsimpy.cosine import Cosine\n",
    "\n",
    "test_string1 = reg[0] # google.com\n",
    "test_string2 = nonreg[1] # g00qle.com\n",
    "\n",
    "levenshtein = Levenshtein()\n",
    "print(levenshtein.distance(test_string1, test_string2))\n",
    "\n",
    "jarowinkler = JaroWinkler()\n",
    "print(jarowinkler.distance(test_string1, test_string2))\n",
    "\n",
    "sorensondice = SorensenDice()\n",
    "print(sorensondice.distance(test_string1, test_string2))\n",
    "\n",
    "cosine = Cosine(2)\n",
    "a = cosine.get_profile(test_string1)\n",
    "b = cosine.get_profile(test_string2)\n",
    "print(cosine.similarity_profiles(a,b))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in twisted_df.iterrows():\n",
    "    twisted_df.loc[index,'Levenshtein'] = levenshtein.distance(row['Domain'], row['Homograph'])\n",
    "    twisted_df.loc[index,'Jaro-Winkler'] = jarowinkler.distance(row['Domain'], row['Homograph'])\n",
    "    twisted_df.loc[index,'Sorenson-Dice'] = sorensondice.distance(row['Domain'], row['Homograph'])\n",
    "    str_to_vect_a= cosine.get_profile(row['Domain'])\n",
    "    str_to_vect_b= cosine.get_profile(row['Homograph'])\n",
    "    twisted_df.loc[index,'Cosine'] = cosine.similarity_profiles(str_to_vect_a, str_to_vect_b)\n",
    "\n",
    "twisted_df    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to rewrite dataset\n",
    "# twisted_df.to_csv('twisted_text_distance.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Checkpoint\n",
    "twisted_df = pd.read_csv('twisted_text_distance.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw, ImageFont"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test sizing\n",
    "lengths = [len(s) for s in twisted_df.Homograph]\n",
    "longest_idx= lengths.index(max(lengths))\n",
    "text = twisted_df.Homograph[longest_idx]\n",
    "img = Image.new('RGB', (1024, 128))\n",
    "# use bold font\n",
    "font = ImageFont.truetype(f\"./fonts/arial bold.ttf\",70)\n",
    "# draw image\n",
    "d1 = ImageDraw.Draw(img)\n",
    "# Center text in image\n",
    "xpos = (img.size[0] / 2) - (font.getsize(text)[0]/2)\n",
    "ypos = (img.size[1] / 2) - (font.getsize(text)[1]/2)\n",
    "d1.text((xpos, ypos), text, fill =(255, 255, 255), font=font)\n",
    "# show image\n",
    "img.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "\n",
    "def create_image(string, font='arial.ttf', show=False):\n",
    "    if not os.path.isfile(f'./images/{string}.jpeg'):\n",
    "        img = Image.new('RGB', (1024, 128))\n",
    "        text = string\n",
    "        # use declared font\n",
    "        font = ImageFont.truetype(f\"./fonts/{font}\",70)\n",
    "        # draw image\n",
    "        d1 = ImageDraw.Draw(img)\n",
    "        # Center text in image\n",
    "        xpos = (img.size[0] / 2) - (font.getsize(text)[0]/2)\n",
    "        ypos = (img.size[1] / 2) - (font.getsize(text)[1]/2)\n",
    "        d1.text((xpos, ypos), text, fill =(255, 255, 255), font=font)\n",
    "        # show and save the image\n",
    "        if show:\n",
    "            img.show()\n",
    "        img.save(f'images/{string}.jpeg')\n",
    "\n",
    "# for test_string in [test_string1, test_string2]:\n",
    "#     create_image(test_string, show=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Similarity Embeddings: MSE, SSIM, ResNet-50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.metrics import structural_similarity as ssim, mean_squared_error as mse\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "def calculate_similarity(string_a, string_b):\n",
    "    imageA = cv2.imread(f'./images/{string_a}.jpeg')\n",
    "    imageB= cv2.imread(f'./images/{string_b}.jpeg')\n",
    "    gsA = cv2.cvtColor(imageA, cv2.COLOR_BGR2GRAY)\n",
    "    gsB = cv2.cvtColor(imageB, cv2.COLOR_BGR2GRAY)\n",
    "    # Calculate the MSE, SSIM, COR\n",
    "    m = mse(gsA, gsB)\n",
    "    s = ssim(gsA, gsB)\n",
    "    return  m, s\n",
    "# calculate_similarity(test_string1, test_string2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/x3/7twk98h15d59hbdjjl0xw9g40000gn/T/ipykernel_6436/4075930908.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mcreate_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Domain'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mcreate_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Homograph'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Domain'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Homograph'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mtwisted_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'IMG_MSE'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mtwisted_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'IMG_MSE'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/x3/7twk98h15d59hbdjjl0xw9g40000gn/T/ipykernel_6436/2335189845.py\u001b[0m in \u001b[0;36mcalculate_similarity\u001b[0;34m(string_a, string_b)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# Calculate the MSE, SSIM, COR\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgsA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgsB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mssim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgsA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgsB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0;32mreturn\u001b[0m  \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# calculate_similarity(test_string1, test_string2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/skimage/metrics/_structural_similarity.py\u001b[0m in \u001b[0;36mstructural_similarity\u001b[0;34m(im1, im2, win_size, gradient, data_range, multichannel, gaussian_weights, full, **kwargs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0mvx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcov_norm\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0muxx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mux\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mux\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0mvy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcov_norm\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0muyy\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0muy\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0muy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m     \u001b[0mvxy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcov_norm\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0muxy\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mux\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0muy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0mR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_range\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for index, row in twisted_df.iterrows():\n",
    "    create_image(row['Domain'])\n",
    "    create_image(row['Homograph'])\n",
    "    m, s = calculate_similarity(row['Domain'], row['Homograph'])\n",
    "    twisted_df.loc[index,'IMG_MSE'] = m\n",
    "    twisted_df.loc[index,'IMG_SSM'] = s\n",
    "twisted_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Homograph</th>\n",
       "      <th>Domain</th>\n",
       "      <th>Registered</th>\n",
       "      <th>Levenshtein</th>\n",
       "      <th>Jaro-Winkler</th>\n",
       "      <th>Sorenson-Dice</th>\n",
       "      <th>Cosine</th>\n",
       "      <th>MSE</th>\n",
       "      <th>SSM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>google.com</td>\n",
       "      <td>google.com</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>google7.com</td>\n",
       "      <td>google.com</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.013774</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.843274</td>\n",
       "      <td>3573.467743</td>\n",
       "      <td>0.848874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>googlea.com</td>\n",
       "      <td>google.com</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.013774</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.843274</td>\n",
       "      <td>3562.883354</td>\n",
       "      <td>0.851078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>googled.com</td>\n",
       "      <td>google.com</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.013774</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.843274</td>\n",
       "      <td>3576.705872</td>\n",
       "      <td>0.849858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>googlej.com</td>\n",
       "      <td>google.com</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.013774</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.843274</td>\n",
       "      <td>3890.821388</td>\n",
       "      <td>0.849594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47009</th>\n",
       "      <td>i.nstagram.com</td>\n",
       "      <td>instagram.com</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.022109</td>\n",
       "      <td>0.130435</td>\n",
       "      <td>0.880705</td>\n",
       "      <td>4724.995277</td>\n",
       "      <td>0.821886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47010</th>\n",
       "      <td>in.stagram.com</td>\n",
       "      <td>instagram.com</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.020408</td>\n",
       "      <td>0.217391</td>\n",
       "      <td>0.880705</td>\n",
       "      <td>4730.435272</td>\n",
       "      <td>0.820904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47011</th>\n",
       "      <td>inst.agram.com</td>\n",
       "      <td>instagram.com</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.071952</td>\n",
       "      <td>0.217391</td>\n",
       "      <td>0.880705</td>\n",
       "      <td>4727.817451</td>\n",
       "      <td>0.821145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47012</th>\n",
       "      <td>insta.gram.com</td>\n",
       "      <td>instagram.com</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.048273</td>\n",
       "      <td>0.217391</td>\n",
       "      <td>0.880705</td>\n",
       "      <td>4748.178596</td>\n",
       "      <td>0.821017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47013</th>\n",
       "      <td>instag.ram.com</td>\n",
       "      <td>instagram.com</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.042909</td>\n",
       "      <td>0.217391</td>\n",
       "      <td>0.880705</td>\n",
       "      <td>4728.070107</td>\n",
       "      <td>0.821479</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>47014 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Homograph         Domain  Registered  Levenshtein  Jaro-Winkler  \\\n",
       "0          google.com     google.com        True          0.0      0.000000   \n",
       "1         google7.com     google.com        True          1.0      0.013774   \n",
       "2         googlea.com     google.com        True          1.0      0.013774   \n",
       "3         googled.com     google.com        True          1.0      0.013774   \n",
       "4         googlej.com     google.com        True          1.0      0.013774   \n",
       "...               ...            ...         ...          ...           ...   \n",
       "47009  i.nstagram.com  instagram.com       False          1.0      0.022109   \n",
       "47010  in.stagram.com  instagram.com       False          1.0      0.020408   \n",
       "47011  inst.agram.com  instagram.com       False          1.0      0.071952   \n",
       "47012  insta.gram.com  instagram.com       False          1.0      0.048273   \n",
       "47013  instag.ram.com  instagram.com       False          1.0      0.042909   \n",
       "\n",
       "       Sorenson-Dice    Cosine          MSE       SSM  \n",
       "0           0.000000  1.000000     0.000000  1.000000  \n",
       "1           0.294118  0.843274  3573.467743  0.848874  \n",
       "2           0.294118  0.843274  3562.883354  0.851078  \n",
       "3           0.294118  0.843274  3576.705872  0.849858  \n",
       "4           0.294118  0.843274  3890.821388  0.849594  \n",
       "...              ...       ...          ...       ...  \n",
       "47009       0.130435  0.880705  4724.995277  0.821886  \n",
       "47010       0.217391  0.880705  4730.435272  0.820904  \n",
       "47011       0.217391  0.880705  4727.817451  0.821145  \n",
       "47012       0.217391  0.880705  4748.178596  0.821017  \n",
       "47013       0.217391  0.880705  4728.070107  0.821479  \n",
       "\n",
       "[47014 rows x 9 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataset Checkpoint\n",
    "\n",
    "twisted_df = pd.read_csv('twisted_viz_sim.csv', index_col=0)\n",
    "twisted_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import models, transforms\n",
    "\n",
    "def siamese_similarity(string_a, string_b):\n",
    "    # preprocess images\n",
    "    imgA = Image.open(f'./images/{string_a}.jpeg').convert('RGB')\n",
    "    imgB = Image.open(f'./images/{string_b}.jpeg').convert('RGB')\n",
    "\n",
    "    transform = transforms.ToTensor()\n",
    "    imgA = transform(imgA).unsqueeze(0)\n",
    "    imgB = transform(imgB).unsqueeze(0)\n",
    "\n",
    "    net = models.resnet50(weights='ResNet50_Weights.DEFAULT')\n",
    "    embedding_size = net.fc.in_features\n",
    "    net.fc = torch.nn.Linear(embedding_size, 256)\n",
    "\n",
    "    embedding1 = net(imgA).detach()\n",
    "    embedding2 = net(imgB).detach()\n",
    "    e = torch.nn.functional.pairwise_distance(embedding1, embedding2).item()\n",
    "    c = torch.nn.functional.cosine_similarity(embedding1, embedding2).item()\n",
    "    l = torch.nn.functional.l1_loss(embedding1, embedding2).item()\n",
    "    return e, c, l\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Homograph</th>\n",
       "      <th>Domain</th>\n",
       "      <th>Registered</th>\n",
       "      <th>Levenshtein</th>\n",
       "      <th>Jaro-Winkler</th>\n",
       "      <th>Sorenson-Dice</th>\n",
       "      <th>Cosine</th>\n",
       "      <th>MSE</th>\n",
       "      <th>SSM</th>\n",
       "      <th>EMBD_EUC</th>\n",
       "      <th>EMBD_COS</th>\n",
       "      <th>EMBD_L1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>google.com</td>\n",
       "      <td>google.com</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>google7.com</td>\n",
       "      <td>google.com</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.013774</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.843274</td>\n",
       "      <td>3573.467743</td>\n",
       "      <td>0.848874</td>\n",
       "      <td>0.464163</td>\n",
       "      <td>0.801736</td>\n",
       "      <td>0.023069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>googlea.com</td>\n",
       "      <td>google.com</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.013774</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.843274</td>\n",
       "      <td>3562.883354</td>\n",
       "      <td>0.851078</td>\n",
       "      <td>0.456447</td>\n",
       "      <td>0.780336</td>\n",
       "      <td>0.022861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>googled.com</td>\n",
       "      <td>google.com</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.013774</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.843274</td>\n",
       "      <td>3576.705872</td>\n",
       "      <td>0.849858</td>\n",
       "      <td>0.523681</td>\n",
       "      <td>0.818360</td>\n",
       "      <td>0.026033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>googlej.com</td>\n",
       "      <td>google.com</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.013774</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.843274</td>\n",
       "      <td>3890.821388</td>\n",
       "      <td>0.849594</td>\n",
       "      <td>0.409549</td>\n",
       "      <td>0.863152</td>\n",
       "      <td>0.020503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47009</th>\n",
       "      <td>i.nstagram.com</td>\n",
       "      <td>instagram.com</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.022109</td>\n",
       "      <td>0.130435</td>\n",
       "      <td>0.880705</td>\n",
       "      <td>4724.995277</td>\n",
       "      <td>0.821886</td>\n",
       "      <td>0.568484</td>\n",
       "      <td>0.827922</td>\n",
       "      <td>0.029100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47010</th>\n",
       "      <td>in.stagram.com</td>\n",
       "      <td>instagram.com</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.020408</td>\n",
       "      <td>0.217391</td>\n",
       "      <td>0.880705</td>\n",
       "      <td>4730.435272</td>\n",
       "      <td>0.820904</td>\n",
       "      <td>0.730263</td>\n",
       "      <td>0.771949</td>\n",
       "      <td>0.035950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47011</th>\n",
       "      <td>inst.agram.com</td>\n",
       "      <td>instagram.com</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.071952</td>\n",
       "      <td>0.217391</td>\n",
       "      <td>0.880705</td>\n",
       "      <td>4727.817451</td>\n",
       "      <td>0.821145</td>\n",
       "      <td>0.609621</td>\n",
       "      <td>0.828346</td>\n",
       "      <td>0.030572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47012</th>\n",
       "      <td>insta.gram.com</td>\n",
       "      <td>instagram.com</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.048273</td>\n",
       "      <td>0.217391</td>\n",
       "      <td>0.880705</td>\n",
       "      <td>4748.178596</td>\n",
       "      <td>0.821017</td>\n",
       "      <td>0.690725</td>\n",
       "      <td>0.784406</td>\n",
       "      <td>0.034384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47013</th>\n",
       "      <td>instag.ram.com</td>\n",
       "      <td>instagram.com</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.042909</td>\n",
       "      <td>0.217391</td>\n",
       "      <td>0.880705</td>\n",
       "      <td>4728.070107</td>\n",
       "      <td>0.821479</td>\n",
       "      <td>0.651304</td>\n",
       "      <td>0.822353</td>\n",
       "      <td>0.032712</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>47014 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Homograph         Domain  Registered  Levenshtein  Jaro-Winkler  \\\n",
       "0          google.com     google.com        True          0.0      0.000000   \n",
       "1         google7.com     google.com        True          1.0      0.013774   \n",
       "2         googlea.com     google.com        True          1.0      0.013774   \n",
       "3         googled.com     google.com        True          1.0      0.013774   \n",
       "4         googlej.com     google.com        True          1.0      0.013774   \n",
       "...               ...            ...         ...          ...           ...   \n",
       "47009  i.nstagram.com  instagram.com       False          1.0      0.022109   \n",
       "47010  in.stagram.com  instagram.com       False          1.0      0.020408   \n",
       "47011  inst.agram.com  instagram.com       False          1.0      0.071952   \n",
       "47012  insta.gram.com  instagram.com       False          1.0      0.048273   \n",
       "47013  instag.ram.com  instagram.com       False          1.0      0.042909   \n",
       "\n",
       "       Sorenson-Dice    Cosine          MSE       SSM  EMBD_EUC  EMBD_COS  \\\n",
       "0           0.000000  1.000000     0.000000  1.000000  0.000016  1.000000   \n",
       "1           0.294118  0.843274  3573.467743  0.848874  0.464163  0.801736   \n",
       "2           0.294118  0.843274  3562.883354  0.851078  0.456447  0.780336   \n",
       "3           0.294118  0.843274  3576.705872  0.849858  0.523681  0.818360   \n",
       "4           0.294118  0.843274  3890.821388  0.849594  0.409549  0.863152   \n",
       "...              ...       ...          ...       ...       ...       ...   \n",
       "47009       0.130435  0.880705  4724.995277  0.821886  0.568484  0.827922   \n",
       "47010       0.217391  0.880705  4730.435272  0.820904  0.730263  0.771949   \n",
       "47011       0.217391  0.880705  4727.817451  0.821145  0.609621  0.828346   \n",
       "47012       0.217391  0.880705  4748.178596  0.821017  0.690725  0.784406   \n",
       "47013       0.217391  0.880705  4728.070107  0.821479  0.651304  0.822353   \n",
       "\n",
       "        EMBD_L1  \n",
       "0      0.000000  \n",
       "1      0.023069  \n",
       "2      0.022861  \n",
       "3      0.026033  \n",
       "4      0.020503  \n",
       "...         ...  \n",
       "47009  0.029100  \n",
       "47010  0.035950  \n",
       "47011  0.030572  \n",
       "47012  0.034384  \n",
       "47013  0.032712  \n",
       "\n",
       "[47014 rows x 12 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for index, row in twisted_df.iterrows():\n",
    "    create_image(row['Domain'])\n",
    "    create_image(row['Homograph'])\n",
    "    e, c, l = siamese_similarity(row['Domain'], row['Homograph'])\n",
    "    twisted_df.loc[index,'EMBD_EUC'] = e\n",
    "    twisted_df.loc[index,'EMBD_COS'] = c\n",
    "    twisted_df.loc[index,'EMBD_L1'] = l\n",
    "\n",
    "twisted_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to rewrite dataset\n",
    "# twisted_df.to_csv('twisted_viz_sim_siamese.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45105"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataset Checkpoint\n",
    "twisted_df = pd.read_csv('twisted_viz_sim_siamese.csv', index_col=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "twisted_df.set_index(twisted_df.apply(lambda row: (row['Homograph'], row['Domain']), axis=1), inplace=True)\n",
    "twisted_df.to_csv('twisted_viz_sim_siamese_lev.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Homograph</th>\n",
       "      <th>Domain</th>\n",
       "      <th>Registered</th>\n",
       "      <th>Levenshtein</th>\n",
       "      <th>Jaro-Winkler</th>\n",
       "      <th>Sorenson-Dice</th>\n",
       "      <th>Cosine</th>\n",
       "      <th>MSE</th>\n",
       "      <th>SSM</th>\n",
       "      <th>EMBD_EUC</th>\n",
       "      <th>EMBD_COS</th>\n",
       "      <th>EMBD_L1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>(google.com, google.com)</th>\n",
       "      <td>google.com</td>\n",
       "      <td>google.com</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(google7.com, google.com)</th>\n",
       "      <td>google7.com</td>\n",
       "      <td>google.com</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.013774</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.843274</td>\n",
       "      <td>3573.467743</td>\n",
       "      <td>0.848874</td>\n",
       "      <td>0.464163</td>\n",
       "      <td>0.801736</td>\n",
       "      <td>0.023069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(googlea.com, google.com)</th>\n",
       "      <td>googlea.com</td>\n",
       "      <td>google.com</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.013774</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.843274</td>\n",
       "      <td>3562.883354</td>\n",
       "      <td>0.851078</td>\n",
       "      <td>0.456447</td>\n",
       "      <td>0.780336</td>\n",
       "      <td>0.022861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(googled.com, google.com)</th>\n",
       "      <td>googled.com</td>\n",
       "      <td>google.com</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.013774</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.843274</td>\n",
       "      <td>3576.705872</td>\n",
       "      <td>0.849858</td>\n",
       "      <td>0.523681</td>\n",
       "      <td>0.818360</td>\n",
       "      <td>0.026033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(googlej.com, google.com)</th>\n",
       "      <td>googlej.com</td>\n",
       "      <td>google.com</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.013774</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.843274</td>\n",
       "      <td>3890.821388</td>\n",
       "      <td>0.849594</td>\n",
       "      <td>0.409549</td>\n",
       "      <td>0.863152</td>\n",
       "      <td>0.020503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(i.nstagram.com, instagram.com)</th>\n",
       "      <td>i.nstagram.com</td>\n",
       "      <td>instagram.com</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.022109</td>\n",
       "      <td>0.130435</td>\n",
       "      <td>0.880705</td>\n",
       "      <td>4724.995277</td>\n",
       "      <td>0.821886</td>\n",
       "      <td>0.568484</td>\n",
       "      <td>0.827922</td>\n",
       "      <td>0.029100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(in.stagram.com, instagram.com)</th>\n",
       "      <td>in.stagram.com</td>\n",
       "      <td>instagram.com</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.020408</td>\n",
       "      <td>0.217391</td>\n",
       "      <td>0.880705</td>\n",
       "      <td>4730.435272</td>\n",
       "      <td>0.820904</td>\n",
       "      <td>0.730263</td>\n",
       "      <td>0.771949</td>\n",
       "      <td>0.035950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(inst.agram.com, instagram.com)</th>\n",
       "      <td>inst.agram.com</td>\n",
       "      <td>instagram.com</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.071952</td>\n",
       "      <td>0.217391</td>\n",
       "      <td>0.880705</td>\n",
       "      <td>4727.817451</td>\n",
       "      <td>0.821145</td>\n",
       "      <td>0.609621</td>\n",
       "      <td>0.828346</td>\n",
       "      <td>0.030572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(insta.gram.com, instagram.com)</th>\n",
       "      <td>insta.gram.com</td>\n",
       "      <td>instagram.com</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.048273</td>\n",
       "      <td>0.217391</td>\n",
       "      <td>0.880705</td>\n",
       "      <td>4748.178596</td>\n",
       "      <td>0.821017</td>\n",
       "      <td>0.690725</td>\n",
       "      <td>0.784406</td>\n",
       "      <td>0.034384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(instag.ram.com, instagram.com)</th>\n",
       "      <td>instag.ram.com</td>\n",
       "      <td>instagram.com</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.042909</td>\n",
       "      <td>0.217391</td>\n",
       "      <td>0.880705</td>\n",
       "      <td>4728.070107</td>\n",
       "      <td>0.821479</td>\n",
       "      <td>0.651304</td>\n",
       "      <td>0.822353</td>\n",
       "      <td>0.032712</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>47014 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Homograph         Domain  Registered  \\\n",
       "(google.com, google.com)             google.com     google.com        True   \n",
       "(google7.com, google.com)           google7.com     google.com        True   \n",
       "(googlea.com, google.com)           googlea.com     google.com        True   \n",
       "(googled.com, google.com)           googled.com     google.com        True   \n",
       "(googlej.com, google.com)           googlej.com     google.com        True   \n",
       "...                                         ...            ...         ...   \n",
       "(i.nstagram.com, instagram.com)  i.nstagram.com  instagram.com       False   \n",
       "(in.stagram.com, instagram.com)  in.stagram.com  instagram.com       False   \n",
       "(inst.agram.com, instagram.com)  inst.agram.com  instagram.com       False   \n",
       "(insta.gram.com, instagram.com)  insta.gram.com  instagram.com       False   \n",
       "(instag.ram.com, instagram.com)  instag.ram.com  instagram.com       False   \n",
       "\n",
       "                                 Levenshtein  Jaro-Winkler  Sorenson-Dice  \\\n",
       "(google.com, google.com)                 0.0      0.000000       0.000000   \n",
       "(google7.com, google.com)                1.0      0.013774       0.294118   \n",
       "(googlea.com, google.com)                1.0      0.013774       0.294118   \n",
       "(googled.com, google.com)                1.0      0.013774       0.294118   \n",
       "(googlej.com, google.com)                1.0      0.013774       0.294118   \n",
       "...                                      ...           ...            ...   \n",
       "(i.nstagram.com, instagram.com)          1.0      0.022109       0.130435   \n",
       "(in.stagram.com, instagram.com)          1.0      0.020408       0.217391   \n",
       "(inst.agram.com, instagram.com)          1.0      0.071952       0.217391   \n",
       "(insta.gram.com, instagram.com)          1.0      0.048273       0.217391   \n",
       "(instag.ram.com, instagram.com)          1.0      0.042909       0.217391   \n",
       "\n",
       "                                   Cosine          MSE       SSM  EMBD_EUC  \\\n",
       "(google.com, google.com)         1.000000     0.000000  1.000000  0.000016   \n",
       "(google7.com, google.com)        0.843274  3573.467743  0.848874  0.464163   \n",
       "(googlea.com, google.com)        0.843274  3562.883354  0.851078  0.456447   \n",
       "(googled.com, google.com)        0.843274  3576.705872  0.849858  0.523681   \n",
       "(googlej.com, google.com)        0.843274  3890.821388  0.849594  0.409549   \n",
       "...                                   ...          ...       ...       ...   \n",
       "(i.nstagram.com, instagram.com)  0.880705  4724.995277  0.821886  0.568484   \n",
       "(in.stagram.com, instagram.com)  0.880705  4730.435272  0.820904  0.730263   \n",
       "(inst.agram.com, instagram.com)  0.880705  4727.817451  0.821145  0.609621   \n",
       "(insta.gram.com, instagram.com)  0.880705  4748.178596  0.821017  0.690725   \n",
       "(instag.ram.com, instagram.com)  0.880705  4728.070107  0.821479  0.651304   \n",
       "\n",
       "                                 EMBD_COS   EMBD_L1  \n",
       "(google.com, google.com)         1.000000  0.000000  \n",
       "(google7.com, google.com)        0.801736  0.023069  \n",
       "(googlea.com, google.com)        0.780336  0.022861  \n",
       "(googled.com, google.com)        0.818360  0.026033  \n",
       "(googlej.com, google.com)        0.863152  0.020503  \n",
       "...                                   ...       ...  \n",
       "(i.nstagram.com, instagram.com)  0.827922  0.029100  \n",
       "(in.stagram.com, instagram.com)  0.771949  0.035950  \n",
       "(inst.agram.com, instagram.com)  0.828346  0.030572  \n",
       "(insta.gram.com, instagram.com)  0.784406  0.034384  \n",
       "(instag.ram.com, instagram.com)  0.822353  0.032712  \n",
       "\n",
       "[47014 rows x 12 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twisted_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Classifiers: Linguistic and Visual Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score, roc_curve, auc\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.tree import export_text, DecisionTreeClassifier\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for running and validating supervised classification models\n",
    "import time\n",
    "def model_eval(df, target, model, curves=False, training_runs=1):\n",
    "\n",
    "    if training_runs > 1:\n",
    "        multi_recall = []\n",
    "        multi_precision = []\n",
    "        multi_f1 = []\n",
    "        multi_auc_score = []\n",
    "        multi_run_time = []\n",
    "    for i in range(training_runs):\n",
    "        kf = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "\n",
    "        # Arrays to store metrics\n",
    "        recall = []\n",
    "        precision = []\n",
    "        f1 = []\n",
    "        auc_score = []\n",
    "        run_time = []\n",
    "\n",
    "        for train_index, test_index in kf.split(df, target):\n",
    "            X_train, X_test = df.iloc[train_index], df.iloc[test_index]\n",
    "            y_train, y_test = target.iloc[train_index], target.iloc[test_index]\n",
    "            model.fit(X_train, y_train)\n",
    "            t0 = time.time()\n",
    "            y_pred = model.predict(X_test)\n",
    "            t = time.time() - t0\n",
    "\n",
    "            recall += [recall_score(y_pred, y_test, zero_division=0)]\n",
    "            precision += [precision_score(y_pred, y_test)]\n",
    "            f1 += [f1_score(y_pred, y_test)]\n",
    "            run_time += [t]\n",
    "            try:\n",
    "                auc_score += [roc_auc_score(y_pred, y_test)]\n",
    "            except: \n",
    "                ValueError\n",
    "            if curves:\n",
    "                fpr, tpr, _ = roc_curve(y_test, y_pred)\n",
    "                aucm = auc(fpr, tpr)\n",
    "                return fpr, tpr, aucm\n",
    "        if training_runs > 1:    \n",
    "            multi_recall.append(np.mean(recall))\n",
    "            multi_precision.append(np.mean(precision))\n",
    "            multi_f1.append(np.mean(f1))\n",
    "            multi_auc_score.append(np.mean(auc_score))\n",
    "            multi_run_time.append(np.mean(run_time))\n",
    "        print(f'-------------- Training Run {i+1} of {training_runs} --------------')\n",
    "        print(\"precision = {:.4f} ±{:.4f}\".format(np.mean(precision), np.std(precision)))\n",
    "        print(\"recall    = {:.4f} ±{:.4f}\".format(np.mean(recall), np.std(recall)))\n",
    "        print(\"f1        = {:.4f} ±{:.4f}\".format(np.mean(f1), np.std(f1)))\n",
    "        print(\"auc        = {:.4f} ±{:.4f}\".format(np.mean(auc_score), np.std(auc_score)))\n",
    "        print(\"run_time    = {:.6f}\".format(np.mean(run_time)))\n",
    "    \n",
    "    if training_runs > 1:\n",
    "        print(f'-------------- Average Over All Runs --------------')\n",
    "        print(\"precision = {:.4f} ±{:.4f}\".format(np.mean(multi_precision), np.std(multi_precision)))\n",
    "        print(\"recall    = {:.4f} ±{:.4f}\".format(np.mean(multi_recall), np.std(multi_recall)))\n",
    "        print(\"f1        = {:.4f} ±{:.4f}\".format(np.mean(multi_f1), np.std(multi_f1)))\n",
    "        print(\"auc        = {:.4f} ±{:.4f}\".format(np.mean(multi_auc_score), np.std(multi_auc_score)))\n",
    "        print(\"run_time    = {:.6f}\".format(np.mean(multi_run_time)))\n",
    "\n",
    "\n",
    "    return  X_test, y_test, model.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Levenshtein\n",
      "-------------- Training Run 1 of 1 --------------\n",
      "precision = 0.8109 ±0.0280\n",
      "recall    = 0.6688 ±0.0180\n",
      "f1        = 0.7324 ±0.0058\n",
      "auc        = 0.8304 ±0.0085\n",
      "run_time    = 0.001211\n",
      "\n",
      "|--- Levenshtein <= 1.5000000000\n",
      "|   |--- class: True\n",
      "|--- Levenshtein >  1.5000000000\n",
      "|   |--- class: False\n",
      "\n",
      "Jaro-Winkler\n",
      "-------------- Training Run 1 of 1 --------------\n",
      "precision = 0.8261 ±0.0202\n",
      "recall    = 0.6426 ±0.0162\n",
      "f1        = 0.7228 ±0.0150\n",
      "auc        = 0.8176 ±0.0083\n",
      "run_time    = 0.000985\n",
      "\n",
      "|--- Jaro-Winkler <= 0.1100206599\n",
      "|   |--- class: True\n",
      "|--- Jaro-Winkler >  0.1100206599\n",
      "|   |--- class: False\n",
      "\n",
      "Sorenson-Dice\n",
      "-------------- Training Run 1 of 1 --------------\n",
      "precision = 0.8198 ±0.0241\n",
      "recall    = 0.6348 ±0.0169\n",
      "f1        = 0.7153 ±0.0139\n",
      "auc        = 0.8136 ±0.0084\n",
      "run_time    = 0.001503\n",
      "\n",
      "|--- Sorenson-Dice <= 0.4330357164\n",
      "|   |--- class: True\n",
      "|--- Sorenson-Dice >  0.4330357164\n",
      "|   |--- class: False\n",
      "\n",
      "Cosine\n",
      "-------------- Training Run 1 of 1 --------------\n",
      "precision = 0.8292 ±0.0100\n",
      "recall    = 0.6626 ±0.0171\n",
      "f1        = 0.7366 ±0.0141\n",
      "auc        = 0.8276 ±0.0087\n",
      "run_time    = 0.001140\n",
      "\n",
      "|--- Cosine <= 0.7476779819\n",
      "|   |--- class: False\n",
      "|--- Cosine >  0.7476779819\n",
      "|   |--- class: True\n",
      "\n",
      "MSE\n",
      "-------------- Training Run 1 of 1 --------------\n",
      "precision = 0.6433 ±0.0183\n",
      "recall    = 0.5152 ±0.0187\n",
      "f1        = 0.5720 ±0.0171\n",
      "auc        = 0.7500 ±0.0096\n",
      "run_time    = 0.001118\n",
      "\n",
      "|--- MSE <= 3937.8059082031\n",
      "|   |--- class: True\n",
      "|--- MSE >  3937.8059082031\n",
      "|   |--- class: False\n",
      "\n",
      "SSM\n",
      "-------------- Training Run 1 of 1 --------------\n",
      "precision = 0.7187 ±0.0232\n",
      "recall    = 0.5861 ±0.0201\n",
      "f1        = 0.6454 ±0.0173\n",
      "auc        = 0.7870 ±0.0102\n",
      "run_time    = 0.001118\n",
      "\n",
      "|--- SSM <= 0.8319787681\n",
      "|   |--- class: False\n",
      "|--- SSM >  0.8319787681\n",
      "|   |--- class: True\n",
      "\n",
      "EMBD_EUC\n",
      "-------------- Training Run 1 of 1 --------------\n",
      "precision = 0.4159 ±0.0180\n",
      "recall    = 0.6417 ±0.0207\n",
      "f1        = 0.5041 ±0.0089\n",
      "auc        = 0.8087 ±0.0101\n",
      "run_time    = 0.001058\n",
      "\n",
      "|--- EMBD_EUC <= 0.4916363209\n",
      "|   |--- class: True\n",
      "|--- EMBD_EUC >  0.4916363209\n",
      "|   |--- class: False\n",
      "\n",
      "EMBD_COS\n",
      "-------------- Training Run 1 of 1 --------------\n",
      "precision = 0.2927 ±0.2390\n",
      "recall    = 0.3069 ±0.2506\n",
      "f1        = 0.2996 ±0.2447\n",
      "auc        = 0.7449 ±0.0030\n",
      "run_time    = 0.001212\n",
      "\n",
      "|--- EMBD_COS <= 0.8122633398\n",
      "|   |--- class: False\n",
      "|--- EMBD_COS >  0.8122633398\n",
      "|   |--- class: False\n",
      "\n",
      "EMBD_L1\n",
      "-------------- Training Run 1 of 1 --------------\n",
      "precision = 0.4385 ±0.0246\n",
      "recall    = 0.5848 ±0.0301\n",
      "f1        = 0.4999 ±0.0101\n",
      "auc        = 0.7806 ±0.0146\n",
      "run_time    = 0.001047\n",
      "\n",
      "|--- EMBD_L1 <= 0.0248482050\n",
      "|   |--- class: True\n",
      "|--- EMBD_L1 >  0.0248482050\n",
      "|   |--- class: False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dt_model = DecisionTreeClassifier(max_depth=1)\n",
    "features = twisted_df.drop(['Domain', 'Homograph', 'Registered'], axis=1)\n",
    "for col in features.columns:\n",
    "    print (col)\n",
    "    model_eval(twisted_df[[col]], twisted_df['Registered'] , dt_model)\n",
    "    print()\n",
    "    print(export_text(dt_model, feature_names=[col], decimals=10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Levenshtein\n",
      "-------------- Training Run 1 of 1 --------------\n",
      "precision = 0.8125 ±0.0282\n",
      "recall    = 0.6693 ±0.0210\n",
      "f1        = 0.7332 ±0.0047\n",
      "auc        = 0.8306 ±0.0100\n",
      "run_time    = 0.001207\n",
      "Jaro-Winkler\n",
      "-------------- Training Run 1 of 1 --------------\n",
      "precision = 0.8245 ±0.0136\n",
      "recall    = 0.6426 ±0.0127\n",
      "f1        = 0.7221 ±0.0064\n",
      "auc        = 0.8175 ±0.0062\n",
      "run_time    = 0.000969\n",
      "Sorenson-Dice\n",
      "-------------- Training Run 1 of 1 --------------\n",
      "precision = 0.8083 ±0.0201\n",
      "recall    = 0.6505 ±0.0167\n",
      "f1        = 0.7207 ±0.0156\n",
      "auc        = 0.8211 ±0.0085\n",
      "run_time    = 0.001113\n",
      "Cosine\n",
      "-------------- Training Run 1 of 1 --------------\n",
      "precision = 0.8292 ±0.0098\n",
      "recall    = 0.6626 ±0.0111\n",
      "f1        = 0.7365 ±0.0056\n",
      "auc        = 0.8276 ±0.0054\n",
      "run_time    = 0.001005\n",
      "MSE\n",
      "-------------- Training Run 1 of 1 --------------\n",
      "precision = 0.6443 ±0.0155\n",
      "recall    = 0.5154 ±0.0093\n",
      "f1        = 0.5725 ±0.0029\n",
      "auc        = 0.7501 ±0.0044\n",
      "run_time    = 0.001063\n",
      "SSM\n",
      "-------------- Training Run 1 of 1 --------------\n",
      "precision = 0.7250 ±0.0184\n",
      "recall    = 0.5763 ±0.0194\n",
      "f1        = 0.6418 ±0.0115\n",
      "auc        = 0.7823 ±0.0096\n",
      "run_time    = 0.001038\n",
      "EMBD_EUC\n",
      "-------------- Training Run 1 of 1 --------------\n",
      "precision = 0.4164 ±0.0220\n",
      "recall    = 0.6377 ±0.0394\n",
      "f1        = 0.5037 ±0.0274\n",
      "auc        = 0.8067 ±0.0201\n",
      "run_time    = 0.001003\n",
      "EMBD_COS\n",
      "-------------- Training Run 1 of 1 --------------\n",
      "precision = 0.1893 ±0.2322\n",
      "recall    = 0.1904 ±0.2333\n",
      "f1        = 0.1898 ±0.2326\n",
      "auc        = 0.7269 ±0.0028\n",
      "run_time    = 0.001132\n",
      "EMBD_L1\n",
      "-------------- Training Run 1 of 1 --------------\n",
      "precision = 0.4259 ±0.0288\n",
      "recall    = 0.5943 ±0.0294\n",
      "f1        = 0.4958 ±0.0266\n",
      "auc        = 0.7851 ±0.0151\n",
      "run_time    = 0.001102\n"
     ]
    }
   ],
   "source": [
    "dt_model = DecisionTreeClassifier(max_depth=1)\n",
    "features = twisted_df.drop(['Domain', 'Homograph', 'Registered'], axis=1)\n",
    "for col in features.columns:\n",
    "    print (col)\n",
    "    model_eval(twisted_df[[col]], twisted_df['Registered'] , dt_model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PhishEye Classification Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "# from sklearn.svm import SVC"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameterization: GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSE</th>\n",
       "      <th>SSM</th>\n",
       "      <th>EMBD_EUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>(google.com, google.com)</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(google7.com, google.com)</th>\n",
       "      <td>3573.467743</td>\n",
       "      <td>0.848874</td>\n",
       "      <td>0.464163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(googlea.com, google.com)</th>\n",
       "      <td>3562.883354</td>\n",
       "      <td>0.851078</td>\n",
       "      <td>0.456447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(googled.com, google.com)</th>\n",
       "      <td>3576.705872</td>\n",
       "      <td>0.849858</td>\n",
       "      <td>0.523681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(googlej.com, google.com)</th>\n",
       "      <td>3890.821388</td>\n",
       "      <td>0.849594</td>\n",
       "      <td>0.409549</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   MSE       SSM  EMBD_EUC\n",
       "(google.com, google.com)      0.000000  1.000000  0.000016\n",
       "(google7.com, google.com)  3573.467743  0.848874  0.464163\n",
       "(googlea.com, google.com)  3562.883354  0.851078  0.456447\n",
       "(googled.com, google.com)  3576.705872  0.849858  0.523681\n",
       "(googlej.com, google.com)  3890.821388  0.849594  0.409549"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = features[['MSE', 'SSM','EMBD_EUC']]\n",
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "\n",
    "def grid_search(df, target, model, param_grid):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df, target, test_size=0.2)\n",
    "    grid_search = GridSearchCV(model, param_grid, scoring='f1')\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    return grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'criterion': 'gini', 'max_depth': 10, 'n_estimators': 100}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [5, 10],\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "rf_model = RandomForestClassifier()\n",
    "grid_search(features[['MSE', 'SSM','EMBD_EUC']], twisted_df['Registered'] , rf_model, param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'activation': 'relu', 'hidden_layer_sizes': (100,), 'solver': 'adam'}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(10,), (50,), (100,)],\n",
    "    'activation': ['relu', 'tanh'],\n",
    "    'solver': ['sgd', 'adam'],\n",
    "}\n",
    "mlp_model = MLPClassifier()\n",
    "grid_search(features[['MSE', 'SSM','EMBD_EUC']], twisted_df['Registered'] , mlp_model, param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'C': 1, 'max_iter': 100, 'penalty': 'l1', 'solver': 'liblinear'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'C': [0.1, 1, 10],\n",
    "    'solver': ['liblinear', 'saga'],\n",
    "    'max_iter': [100, 200, 300],\n",
    "}\n",
    "lr_model = LogisticRegression()\n",
    "grid_search(features[['MSE', 'SSM','EMBD_EUC']], twisted_df['Registered'] , lr_model, param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 50}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'learning_rate': [0.1, 0.01],\n",
    "    'max_depth': [3, 6, 9],\n",
    "    'n_estimators': [50, 100, 200]\n",
    "}\n",
    "xgb_model = GradientBoostingClassifier()\n",
    "grid_search(features[['MSE', 'SSM','EMBD_EUC']], twisted_df['Registered'] , xgb_model, param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.svm import SVC\n",
    "# param_grid = {\n",
    "#     'C': [0.1, 1, 10],\n",
    "#     'kernel': ['linear', 'rbf'],\n",
    "#     'gamma': ['scale', 'auto']\n",
    "# }\n",
    "# svm_model = SVC()\n",
    "# grid_search(features[['MSE', 'SSM','EMBD_EUC']], twisted_df['Registered'] , svm_model, param_grid)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier Experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- Training Run 1 of 100 --------------\n",
      "precision = 0.6386 ±0.0330\n",
      "recall    = 0.7386 ±0.0167\n",
      "f1        = 0.6843 ±0.0181\n",
      "auc        = 0.8617 ±0.0081\n",
      "run_time    = 0.058532\n",
      "-------------- Training Run 2 of 100 --------------\n",
      "precision = 0.6333 ±0.0202\n",
      "recall    = 0.7400 ±0.0161\n",
      "f1        = 0.6822 ±0.0110\n",
      "auc        = 0.8623 ±0.0079\n",
      "run_time    = 0.062050\n",
      "-------------- Training Run 3 of 100 --------------\n",
      "precision = 0.6406 ±0.0228\n",
      "recall    = 0.7439 ±0.0221\n",
      "f1        = 0.6881 ±0.0178\n",
      "auc        = 0.8644 ±0.0112\n",
      "run_time    = 0.059608\n",
      "-------------- Training Run 4 of 100 --------------\n",
      "precision = 0.6307 ±0.0277\n",
      "recall    = 0.7361 ±0.0307\n",
      "f1        = 0.6786 ±0.0187\n",
      "auc        = 0.8603 ±0.0153\n",
      "run_time    = 0.063130\n",
      "-------------- Training Run 5 of 100 --------------\n",
      "precision = 0.6427 ±0.0257\n",
      "recall    = 0.7383 ±0.0219\n",
      "f1        = 0.6869 ±0.0189\n",
      "auc        = 0.8616 ±0.0111\n",
      "run_time    = 0.058919\n",
      "-------------- Training Run 6 of 100 --------------\n",
      "precision = 0.6448 ±0.0162\n",
      "recall    = 0.7433 ±0.0236\n",
      "f1        = 0.6905 ±0.0173\n",
      "auc        = 0.8642 ±0.0120\n",
      "run_time    = 0.063697\n",
      "-------------- Training Run 7 of 100 --------------\n",
      "precision = 0.6396 ±0.0238\n",
      "recall    = 0.7516 ±0.0141\n",
      "f1        = 0.6908 ±0.0160\n",
      "auc        = 0.8682 ±0.0072\n",
      "run_time    = 0.060713\n",
      "-------------- Training Run 8 of 100 --------------\n",
      "precision = 0.6422 ±0.0236\n",
      "recall    = 0.7452 ±0.0129\n",
      "f1        = 0.6895 ±0.0131\n",
      "auc        = 0.8651 ±0.0063\n",
      "run_time    = 0.060945\n",
      "-------------- Training Run 9 of 100 --------------\n",
      "precision = 0.6375 ±0.0306\n",
      "recall    = 0.7360 ±0.0092\n",
      "f1        = 0.6830 ±0.0206\n",
      "auc        = 0.8604 ±0.0051\n",
      "run_time    = 0.060011\n",
      "-------------- Training Run 10 of 100 --------------\n",
      "precision = 0.6380 ±0.0097\n",
      "recall    = 0.7427 ±0.0102\n",
      "f1        = 0.6864 ±0.0091\n",
      "auc        = 0.8637 ±0.0052\n",
      "run_time    = 0.060285\n",
      "-------------- Training Run 11 of 100 --------------\n",
      "precision = 0.6276 ±0.0260\n",
      "recall    = 0.7359 ±0.0150\n",
      "f1        = 0.6770 ±0.0150\n",
      "auc        = 0.8601 ±0.0074\n",
      "run_time    = 0.059391\n",
      "-------------- Training Run 12 of 100 --------------\n",
      "precision = 0.6459 ±0.0118\n",
      "recall    = 0.7481 ±0.0126\n",
      "f1        = 0.6931 ±0.0057\n",
      "auc        = 0.8666 ±0.0062\n",
      "run_time    = 0.061216\n",
      "-------------- Training Run 13 of 100 --------------\n",
      "precision = 0.6396 ±0.0231\n",
      "recall    = 0.7412 ±0.0246\n",
      "f1        = 0.6865 ±0.0213\n",
      "auc        = 0.8630 ±0.0126\n",
      "run_time    = 0.060999\n",
      "-------------- Training Run 14 of 100 --------------\n",
      "precision = 0.6459 ±0.0284\n",
      "recall    = 0.7444 ±0.0165\n",
      "f1        = 0.6911 ±0.0137\n",
      "auc        = 0.8647 ±0.0080\n",
      "run_time    = 0.059346\n",
      "-------------- Training Run 15 of 100 --------------\n",
      "precision = 0.6422 ±0.0155\n",
      "recall    = 0.7386 ±0.0132\n",
      "f1        = 0.6870 ±0.0125\n",
      "auc        = 0.8618 ±0.0068\n",
      "run_time    = 0.060635\n",
      "-------------- Training Run 16 of 100 --------------\n",
      "precision = 0.6427 ±0.0221\n",
      "recall    = 0.7435 ±0.0163\n",
      "f1        = 0.6890 ±0.0082\n",
      "auc        = 0.8643 ±0.0078\n",
      "run_time    = 0.058949\n",
      "-------------- Training Run 17 of 100 --------------\n",
      "precision = 0.6422 ±0.0378\n",
      "recall    = 0.7451 ±0.0104\n",
      "f1        = 0.6894 ±0.0251\n",
      "auc        = 0.8650 ±0.0057\n",
      "run_time    = 0.059672\n",
      "-------------- Training Run 18 of 100 --------------\n",
      "precision = 0.6427 ±0.0091\n",
      "recall    = 0.7458 ±0.0176\n",
      "f1        = 0.6903 ±0.0096\n",
      "auc        = 0.8654 ±0.0088\n",
      "run_time    = 0.061819\n",
      "-------------- Training Run 19 of 100 --------------\n",
      "precision = 0.6485 ±0.0281\n",
      "recall    = 0.7346 ±0.0159\n",
      "f1        = 0.6885 ±0.0176\n",
      "auc        = 0.8599 ±0.0080\n",
      "run_time    = 0.059074\n",
      "-------------- Training Run 20 of 100 --------------\n",
      "precision = 0.6391 ±0.0237\n",
      "recall    = 0.7478 ±0.0126\n",
      "f1        = 0.6889 ±0.0140\n",
      "auc        = 0.8663 ±0.0062\n",
      "run_time    = 0.067601\n",
      "-------------- Training Run 21 of 100 --------------\n",
      "precision = 0.6338 ±0.0161\n",
      "recall    = 0.7396 ±0.0244\n",
      "f1        = 0.6823 ±0.0127\n",
      "auc        = 0.8621 ±0.0122\n",
      "run_time    = 0.063228\n",
      "-------------- Training Run 22 of 100 --------------\n",
      "precision = 0.6438 ±0.0117\n",
      "recall    = 0.7446 ±0.0181\n",
      "f1        = 0.6905 ±0.0137\n",
      "auc        = 0.8648 ±0.0092\n",
      "run_time    = 0.057585\n",
      "-------------- Training Run 23 of 100 --------------\n",
      "precision = 0.6349 ±0.0087\n",
      "recall    = 0.7384 ±0.0267\n",
      "f1        = 0.6826 ±0.0143\n",
      "auc        = 0.8615 ±0.0134\n",
      "run_time    = 0.064575\n",
      "-------------- Training Run 24 of 100 --------------\n",
      "precision = 0.6391 ±0.0133\n",
      "recall    = 0.7423 ±0.0382\n",
      "f1        = 0.6865 ±0.0211\n",
      "auc        = 0.8636 ±0.0192\n",
      "run_time    = 0.057765\n",
      "-------------- Training Run 25 of 100 --------------\n",
      "precision = 0.6380 ±0.0230\n",
      "recall    = 0.7431 ±0.0033\n",
      "f1        = 0.6864 ±0.0136\n",
      "auc        = 0.8640 ±0.0018\n",
      "run_time    = 0.059369\n",
      "-------------- Training Run 26 of 100 --------------\n",
      "precision = 0.6401 ±0.0081\n",
      "recall    = 0.7453 ±0.0239\n",
      "f1        = 0.6886 ±0.0128\n",
      "auc        = 0.8651 ±0.0120\n",
      "run_time    = 0.058454\n",
      "-------------- Training Run 27 of 100 --------------\n",
      "precision = 0.6323 ±0.0293\n",
      "recall    = 0.7459 ±0.0145\n",
      "f1        = 0.6841 ±0.0209\n",
      "auc        = 0.8652 ±0.0076\n",
      "run_time    = 0.058636\n",
      "-------------- Training Run 28 of 100 --------------\n",
      "precision = 0.6438 ±0.0293\n",
      "recall    = 0.7416 ±0.0098\n",
      "f1        = 0.6889 ±0.0190\n",
      "auc        = 0.8633 ±0.0052\n",
      "run_time    = 0.059327\n",
      "-------------- Training Run 29 of 100 --------------\n",
      "precision = 0.6380 ±0.0206\n",
      "recall    = 0.7381 ±0.0282\n",
      "f1        = 0.6843 ±0.0228\n",
      "auc        = 0.8614 ±0.0144\n",
      "run_time    = 0.059025\n",
      "-------------- Training Run 30 of 100 --------------\n",
      "precision = 0.6375 ±0.0231\n",
      "recall    = 0.7393 ±0.0265\n",
      "f1        = 0.6842 ±0.0179\n",
      "auc        = 0.8620 ±0.0133\n",
      "run_time    = 0.060485\n",
      "-------------- Training Run 31 of 100 --------------\n",
      "precision = 0.6370 ±0.0162\n",
      "recall    = 0.7460 ±0.0145\n",
      "f1        = 0.6869 ±0.0089\n",
      "auc        = 0.8654 ±0.0071\n",
      "run_time    = 0.058144\n",
      "-------------- Training Run 32 of 100 --------------\n",
      "precision = 0.6449 ±0.0148\n",
      "recall    = 0.7440 ±0.0214\n",
      "f1        = 0.6909 ±0.0169\n",
      "auc        = 0.8645 ±0.0110\n",
      "run_time    = 0.058789\n",
      "-------------- Training Run 33 of 100 --------------\n",
      "precision = 0.6448 ±0.0118\n",
      "recall    = 0.7418 ±0.0211\n",
      "f1        = 0.6899 ±0.0155\n",
      "auc        = 0.8634 ±0.0108\n",
      "run_time    = 0.057348\n",
      "-------------- Training Run 34 of 100 --------------\n",
      "precision = 0.6448 ±0.0185\n",
      "recall    = 0.7376 ±0.0182\n",
      "f1        = 0.6879 ±0.0132\n",
      "auc        = 0.8613 ±0.0091\n",
      "run_time    = 0.057732\n",
      "-------------- Training Run 35 of 100 --------------\n",
      "precision = 0.6396 ±0.0135\n",
      "recall    = 0.7476 ±0.0217\n",
      "f1        = 0.6893 ±0.0150\n",
      "auc        = 0.8662 ±0.0110\n",
      "run_time    = 0.057420\n",
      "-------------- Training Run 36 of 100 --------------\n",
      "precision = 0.6448 ±0.0092\n",
      "recall    = 0.7417 ±0.0205\n",
      "f1        = 0.6897 ±0.0102\n",
      "auc        = 0.8634 ±0.0103\n",
      "run_time    = 0.060571\n",
      "-------------- Training Run 37 of 100 --------------\n",
      "precision = 0.6469 ±0.0219\n",
      "recall    = 0.7505 ±0.0155\n",
      "f1        = 0.6945 ±0.0105\n",
      "auc        = 0.8678 ±0.0075\n",
      "run_time    = 0.060551\n",
      "-------------- Training Run 38 of 100 --------------\n",
      "precision = 0.6407 ±0.0355\n",
      "recall    = 0.7429 ±0.0282\n",
      "f1        = 0.6875 ±0.0284\n",
      "auc        = 0.8639 ±0.0144\n",
      "run_time    = 0.060727\n",
      "-------------- Training Run 39 of 100 --------------\n",
      "precision = 0.6323 ±0.0340\n",
      "recall    = 0.7424 ±0.0158\n",
      "f1        = 0.6825 ±0.0233\n",
      "auc        = 0.8635 ±0.0082\n",
      "run_time    = 0.057117\n",
      "-------------- Training Run 40 of 100 --------------\n",
      "precision = 0.6469 ±0.0145\n",
      "recall    = 0.7444 ±0.0158\n",
      "f1        = 0.6921 ±0.0094\n",
      "auc        = 0.8648 ±0.0078\n",
      "run_time    = 0.059693\n",
      "-------------- Training Run 41 of 100 --------------\n",
      "precision = 0.6469 ±0.0203\n",
      "recall    = 0.7448 ±0.0246\n",
      "f1        = 0.6923 ±0.0199\n",
      "auc        = 0.8650 ±0.0125\n",
      "run_time    = 0.057998\n",
      "-------------- Training Run 42 of 100 --------------\n",
      "precision = 0.6364 ±0.0253\n",
      "recall    = 0.7352 ±0.0245\n",
      "f1        = 0.6821 ±0.0228\n",
      "auc        = 0.8600 ±0.0126\n",
      "run_time    = 0.059068\n",
      "-------------- Training Run 43 of 100 --------------\n",
      "precision = 0.6464 ±0.0049\n",
      "recall    = 0.7420 ±0.0193\n",
      "f1        = 0.6908 ±0.0101\n",
      "auc        = 0.8636 ±0.0097\n",
      "run_time    = 0.058027\n",
      "-------------- Training Run 44 of 100 --------------\n",
      "precision = 0.6485 ±0.0161\n",
      "recall    = 0.7452 ±0.0145\n",
      "f1        = 0.6933 ±0.0104\n",
      "auc        = 0.8652 ±0.0072\n",
      "run_time    = 0.057678\n",
      "-------------- Training Run 45 of 100 --------------\n",
      "precision = 0.6469 ±0.0237\n",
      "recall    = 0.7371 ±0.0307\n",
      "f1        = 0.6884 ±0.0164\n",
      "auc        = 0.8611 ±0.0153\n",
      "run_time    = 0.057284\n",
      "-------------- Training Run 46 of 100 --------------\n",
      "precision = 0.6464 ±0.0238\n",
      "recall    = 0.7429 ±0.0172\n",
      "f1        = 0.6908 ±0.0089\n",
      "auc        = 0.8640 ±0.0082\n",
      "run_time    = 0.057441\n",
      "-------------- Training Run 47 of 100 --------------\n",
      "precision = 0.6459 ±0.0099\n",
      "recall    = 0.7424 ±0.0213\n",
      "f1        = 0.6904 ±0.0039\n",
      "auc        = 0.8637 ±0.0105\n",
      "run_time    = 0.057042\n",
      "-------------- Training Run 48 of 100 --------------\n",
      "precision = 0.6464 ±0.0197\n",
      "recall    = 0.7429 ±0.0304\n",
      "f1        = 0.6912 ±0.0235\n",
      "auc        = 0.8640 ±0.0155\n",
      "run_time    = 0.056656\n",
      "-------------- Training Run 49 of 100 --------------\n",
      "precision = 0.6475 ±0.0133\n",
      "recall    = 0.7420 ±0.0254\n",
      "f1        = 0.6914 ±0.0171\n",
      "auc        = 0.8636 ±0.0129\n",
      "run_time    = 0.057120\n",
      "-------------- Training Run 50 of 100 --------------\n",
      "precision = 0.6391 ±0.0247\n",
      "recall    = 0.7513 ±0.0206\n",
      "f1        = 0.6901 ±0.0114\n",
      "auc        = 0.8681 ±0.0100\n",
      "run_time    = 0.057430\n",
      "-------------- Training Run 51 of 100 --------------\n",
      "precision = 0.6312 ±0.0215\n",
      "recall    = 0.7394 ±0.0257\n",
      "f1        = 0.6808 ±0.0196\n",
      "auc        = 0.8619 ±0.0130\n",
      "run_time    = 0.056901\n",
      "-------------- Training Run 52 of 100 --------------\n",
      "precision = 0.6349 ±0.0175\n",
      "recall    = 0.7358 ±0.0181\n",
      "f1        = 0.6812 ±0.0067\n",
      "auc        = 0.8602 ±0.0088\n",
      "run_time    = 0.058521\n",
      "-------------- Training Run 53 of 100 --------------\n",
      "precision = 0.6391 ±0.0151\n",
      "recall    = 0.7428 ±0.0156\n",
      "f1        = 0.6869 ±0.0120\n",
      "auc        = 0.8638 ±0.0078\n",
      "run_time    = 0.059218\n",
      "-------------- Training Run 54 of 100 --------------\n",
      "precision = 0.6438 ±0.0238\n",
      "recall    = 0.7443 ±0.0104\n",
      "f1        = 0.6902 ±0.0172\n",
      "auc        = 0.8646 ±0.0055\n",
      "run_time    = 0.064976\n",
      "-------------- Training Run 55 of 100 --------------\n",
      "precision = 0.6407 ±0.0078\n",
      "recall    = 0.7452 ±0.0194\n",
      "f1        = 0.6889 ±0.0116\n",
      "auc        = 0.8650 ±0.0098\n",
      "run_time    = 0.074217\n",
      "-------------- Training Run 56 of 100 --------------\n",
      "precision = 0.6302 ±0.0271\n",
      "recall    = 0.7417 ±0.0262\n",
      "f1        = 0.6813 ±0.0258\n",
      "auc        = 0.8631 ±0.0136\n",
      "run_time    = 0.065935\n",
      "-------------- Training Run 57 of 100 --------------\n",
      "precision = 0.6359 ±0.0414\n",
      "recall    = 0.7342 ±0.0223\n",
      "f1        = 0.6807 ±0.0260\n",
      "auc        = 0.8594 ±0.0113\n",
      "run_time    = 0.061640\n",
      "-------------- Training Run 58 of 100 --------------\n",
      "precision = 0.6459 ±0.0292\n",
      "recall    = 0.7442 ±0.0141\n",
      "f1        = 0.6913 ±0.0193\n",
      "auc        = 0.8646 ±0.0073\n",
      "run_time    = 0.064423\n",
      "-------------- Training Run 59 of 100 --------------\n",
      "precision = 0.6386 ±0.0194\n",
      "recall    = 0.7408 ±0.0233\n",
      "f1        = 0.6858 ±0.0198\n",
      "auc        = 0.8628 ±0.0120\n",
      "run_time    = 0.068333\n",
      "-------------- Training Run 60 of 100 --------------\n",
      "precision = 0.6448 ±0.0252\n",
      "recall    = 0.7463 ±0.0118\n",
      "f1        = 0.6917 ±0.0190\n",
      "auc        = 0.8657 ±0.0063\n",
      "run_time    = 0.057201\n",
      "-------------- Training Run 61 of 100 --------------\n",
      "precision = 0.6422 ±0.0168\n",
      "recall    = 0.7375 ±0.0200\n",
      "f1        = 0.6864 ±0.0159\n",
      "auc        = 0.8612 ±0.0102\n",
      "run_time    = 0.058277\n",
      "-------------- Training Run 62 of 100 --------------\n",
      "precision = 0.6359 ±0.0167\n",
      "recall    = 0.7426 ±0.0183\n",
      "f1        = 0.6851 ±0.0165\n",
      "auc        = 0.8636 ±0.0094\n",
      "run_time    = 0.056690\n",
      "-------------- Training Run 63 of 100 --------------\n",
      "precision = 0.6422 ±0.0441\n",
      "recall    = 0.7397 ±0.0248\n",
      "f1        = 0.6866 ±0.0286\n",
      "auc        = 0.8623 ±0.0126\n",
      "run_time    = 0.057335\n",
      "-------------- Training Run 64 of 100 --------------\n",
      "precision = 0.6349 ±0.0273\n",
      "recall    = 0.7379 ±0.0153\n",
      "f1        = 0.6820 ±0.0133\n",
      "auc        = 0.8613 ±0.0074\n",
      "run_time    = 0.057345\n",
      "-------------- Training Run 65 of 100 --------------\n",
      "precision = 0.6401 ±0.0207\n",
      "recall    = 0.7409 ±0.0076\n",
      "f1        = 0.6868 ±0.0149\n",
      "auc        = 0.8629 ±0.0042\n",
      "run_time    = 0.057783\n",
      "-------------- Training Run 66 of 100 --------------\n",
      "precision = 0.6417 ±0.0078\n",
      "recall    = 0.7435 ±0.0109\n",
      "f1        = 0.6888 ±0.0068\n",
      "auc        = 0.8642 ±0.0055\n",
      "run_time    = 0.057122\n",
      "-------------- Training Run 67 of 100 --------------\n",
      "precision = 0.6286 ±0.0185\n",
      "recall    = 0.7335 ±0.0115\n",
      "f1        = 0.6767 ±0.0069\n",
      "auc        = 0.8589 ±0.0054\n",
      "run_time    = 0.056872\n",
      "-------------- Training Run 68 of 100 --------------\n",
      "precision = 0.6417 ±0.0183\n",
      "recall    = 0.7448 ±0.0100\n",
      "f1        = 0.6893 ±0.0119\n",
      "auc        = 0.8649 ±0.0051\n",
      "run_time    = 0.058497\n",
      "-------------- Training Run 69 of 100 --------------\n",
      "precision = 0.6464 ±0.0090\n",
      "recall    = 0.7473 ±0.0130\n",
      "f1        = 0.6931 ±0.0064\n",
      "auc        = 0.8662 ±0.0064\n",
      "run_time    = 0.057108\n",
      "-------------- Training Run 70 of 100 --------------\n",
      "precision = 0.6464 ±0.0209\n",
      "recall    = 0.7456 ±0.0198\n",
      "f1        = 0.6920 ±0.0103\n",
      "auc        = 0.8654 ±0.0097\n",
      "run_time    = 0.056921\n",
      "-------------- Training Run 71 of 100 --------------\n",
      "precision = 0.6380 ±0.0298\n",
      "recall    = 0.7439 ±0.0188\n",
      "f1        = 0.6867 ±0.0233\n",
      "auc        = 0.8643 ±0.0098\n",
      "run_time    = 0.056951\n",
      "-------------- Training Run 72 of 100 --------------\n",
      "precision = 0.6438 ±0.0187\n",
      "recall    = 0.7421 ±0.0254\n",
      "f1        = 0.6891 ±0.0159\n",
      "auc        = 0.8636 ±0.0128\n",
      "run_time    = 0.056779\n",
      "-------------- Training Run 73 of 100 --------------\n",
      "precision = 0.6433 ±0.0182\n",
      "recall    = 0.7471 ±0.0133\n",
      "f1        = 0.6912 ±0.0136\n",
      "auc        = 0.8660 ±0.0068\n",
      "run_time    = 0.056777\n",
      "-------------- Training Run 74 of 100 --------------\n",
      "precision = 0.6375 ±0.0259\n",
      "recall    = 0.7495 ±0.0130\n",
      "f1        = 0.6885 ±0.0117\n",
      "auc        = 0.8671 ±0.0062\n",
      "run_time    = 0.058359\n",
      "-------------- Training Run 75 of 100 --------------\n",
      "precision = 0.6365 ±0.0244\n",
      "recall    = 0.7391 ±0.0154\n",
      "f1        = 0.6838 ±0.0184\n",
      "auc        = 0.8619 ±0.0080\n",
      "run_time    = 0.056376\n",
      "-------------- Training Run 76 of 100 --------------\n",
      "precision = 0.6380 ±0.0154\n",
      "recall    = 0.7400 ±0.0198\n",
      "f1        = 0.6853 ±0.0173\n",
      "auc        = 0.8624 ±0.0102\n",
      "run_time    = 0.056188\n",
      "-------------- Training Run 77 of 100 --------------\n",
      "precision = 0.6427 ±0.0251\n",
      "recall    = 0.7433 ±0.0233\n",
      "f1        = 0.6888 ±0.0151\n",
      "auc        = 0.8641 ±0.0115\n",
      "run_time    = 0.061561\n",
      "-------------- Training Run 78 of 100 --------------\n",
      "precision = 0.6427 ±0.0092\n",
      "recall    = 0.7447 ±0.0116\n",
      "f1        = 0.6899 ±0.0078\n",
      "auc        = 0.8648 ±0.0058\n",
      "run_time    = 0.056635\n",
      "-------------- Training Run 79 of 100 --------------\n",
      "precision = 0.6386 ±0.0299\n",
      "recall    = 0.7375 ±0.0104\n",
      "f1        = 0.6841 ±0.0184\n",
      "auc        = 0.8612 ±0.0053\n",
      "run_time    = 0.057806\n",
      "-------------- Training Run 80 of 100 --------------\n",
      "precision = 0.6438 ±0.0248\n",
      "recall    = 0.7418 ±0.0201\n",
      "f1        = 0.6892 ±0.0204\n",
      "auc        = 0.8634 ±0.0103\n",
      "run_time    = 0.057443\n",
      "-------------- Training Run 81 of 100 --------------\n",
      "precision = 0.6380 ±0.0216\n",
      "recall    = 0.7379 ±0.0210\n",
      "f1        = 0.6842 ±0.0193\n",
      "auc        = 0.8613 ±0.0108\n",
      "run_time    = 0.056880\n",
      "-------------- Training Run 82 of 100 --------------\n",
      "precision = 0.6454 ±0.0266\n",
      "recall    = 0.7419 ±0.0116\n",
      "f1        = 0.6897 ±0.0113\n",
      "auc        = 0.8635 ±0.0053\n",
      "run_time    = 0.057049\n",
      "-------------- Training Run 83 of 100 --------------\n",
      "precision = 0.6354 ±0.0177\n",
      "recall    = 0.7355 ±0.0185\n",
      "f1        = 0.6814 ±0.0095\n",
      "auc        = 0.8601 ±0.0091\n",
      "run_time    = 0.056898\n",
      "-------------- Training Run 84 of 100 --------------\n",
      "precision = 0.6323 ±0.0277\n",
      "recall    = 0.7370 ±0.0241\n",
      "f1        = 0.6800 ±0.0160\n",
      "auc        = 0.8608 ±0.0119\n",
      "run_time    = 0.056979\n",
      "-------------- Training Run 85 of 100 --------------\n",
      "precision = 0.6448 ±0.0347\n",
      "recall    = 0.7375 ±0.0288\n",
      "f1        = 0.6873 ±0.0231\n",
      "auc        = 0.8613 ±0.0144\n",
      "run_time    = 0.056844\n",
      "-------------- Training Run 86 of 100 --------------\n",
      "precision = 0.6417 ±0.0333\n",
      "recall    = 0.7440 ±0.0160\n",
      "f1        = 0.6886 ±0.0220\n",
      "auc        = 0.8644 ±0.0082\n",
      "run_time    = 0.057206\n",
      "-------------- Training Run 87 of 100 --------------\n",
      "precision = 0.6490 ±0.0262\n",
      "recall    = 0.7379 ±0.0192\n",
      "f1        = 0.6905 ±0.0214\n",
      "auc        = 0.8616 ±0.0100\n",
      "run_time    = 0.057109\n",
      "-------------- Training Run 88 of 100 --------------\n",
      "precision = 0.6396 ±0.0253\n",
      "recall    = 0.7430 ±0.0069\n",
      "f1        = 0.6870 ±0.0129\n",
      "auc        = 0.8639 ±0.0031\n",
      "run_time    = 0.057089\n",
      "-------------- Training Run 89 of 100 --------------\n",
      "precision = 0.6490 ±0.0283\n",
      "recall    = 0.7413 ±0.0136\n",
      "f1        = 0.6919 ±0.0204\n",
      "auc        = 0.8633 ±0.0072\n",
      "run_time    = 0.057083\n",
      "-------------- Training Run 90 of 100 --------------\n",
      "precision = 0.6380 ±0.0326\n",
      "recall    = 0.7416 ±0.0254\n",
      "f1        = 0.6852 ±0.0199\n",
      "auc        = 0.8632 ±0.0126\n",
      "run_time    = 0.056539\n",
      "-------------- Training Run 91 of 100 --------------\n",
      "precision = 0.6537 ±0.0319\n",
      "recall    = 0.7482 ±0.0221\n",
      "f1        = 0.6969 ±0.0136\n",
      "auc        = 0.8668 ±0.0107\n",
      "run_time    = 0.056776\n",
      "-------------- Training Run 92 of 100 --------------\n",
      "precision = 0.6375 ±0.0160\n",
      "recall    = 0.7357 ±0.0132\n",
      "f1        = 0.6829 ±0.0089\n",
      "auc        = 0.8602 ±0.0065\n",
      "run_time    = 0.056805\n",
      "-------------- Training Run 93 of 100 --------------\n",
      "precision = 0.6406 ±0.0189\n",
      "recall    = 0.7422 ±0.0130\n",
      "f1        = 0.6875 ±0.0144\n",
      "auc        = 0.8635 ±0.0067\n",
      "run_time    = 0.056569\n",
      "-------------- Training Run 94 of 100 --------------\n",
      "precision = 0.6401 ±0.0128\n",
      "recall    = 0.7457 ±0.0155\n",
      "f1        = 0.6886 ±0.0048\n",
      "auc        = 0.8653 ±0.0075\n",
      "run_time    = 0.057019\n",
      "-------------- Training Run 95 of 100 --------------\n",
      "precision = 0.6359 ±0.0214\n",
      "recall    = 0.7380 ±0.0213\n",
      "f1        = 0.6829 ±0.0168\n",
      "auc        = 0.8614 ±0.0108\n",
      "run_time    = 0.056580\n",
      "-------------- Training Run 96 of 100 --------------\n",
      "precision = 0.6391 ±0.0248\n",
      "recall    = 0.7459 ±0.0175\n",
      "f1        = 0.6881 ±0.0187\n",
      "auc        = 0.8653 ±0.0090\n",
      "run_time    = 0.057001\n",
      "-------------- Training Run 97 of 100 --------------\n",
      "precision = 0.6401 ±0.0094\n",
      "recall    = 0.7453 ±0.0271\n",
      "f1        = 0.6886 ±0.0159\n",
      "auc        = 0.8651 ±0.0137\n",
      "run_time    = 0.057105\n",
      "-------------- Training Run 98 of 100 --------------\n",
      "precision = 0.6417 ±0.0208\n",
      "recall    = 0.7445 ±0.0090\n",
      "f1        = 0.6890 ±0.0110\n",
      "auc        = 0.8647 ±0.0044\n",
      "run_time    = 0.056430\n",
      "-------------- Training Run 99 of 100 --------------\n",
      "precision = 0.6448 ±0.0159\n",
      "recall    = 0.7392 ±0.0159\n",
      "f1        = 0.6886 ±0.0121\n",
      "auc        = 0.8621 ±0.0080\n",
      "run_time    = 0.056281\n",
      "-------------- Training Run 100 of 100 --------------\n",
      "precision = 0.6317 ±0.0150\n",
      "recall    = 0.7372 ±0.0169\n",
      "f1        = 0.6802 ±0.0092\n",
      "auc        = 0.8609 ±0.0084\n",
      "run_time    = 0.056687\n",
      "-------------- Average Over All Runs --------------\n",
      "precision = 0.6407 ±0.0050\n",
      "recall    = 0.7420 ±0.0039\n",
      "f1        = 0.6873 ±0.0039\n",
      "auc        = 0.8634 ±0.0020\n",
      "run_time    = 0.058960\n"
     ]
    }
   ],
   "source": [
    "rf_model = RandomForestClassifier(criterion='gini', max_depth=10, n_estimators=100)\n",
    "features = twisted_df.drop(['Domain', 'Homograph', 'Registered'], axis=1)\n",
    "model_eval(features[['MSE', 'SSM','EMBD_EUC']], twisted_df['Registered'] , rf_model, training_runs=100);   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- Training Run 1 of 100 --------------\n",
      "precision = 0.3740 ±0.0522\n",
      "recall    = 0.6900 ±0.0365\n",
      "f1        = 0.4834 ±0.0491\n",
      "auc        = 0.8320 ±0.0188\n",
      "run_time    = 0.007584\n",
      "-------------- Training Run 2 of 100 --------------\n",
      "precision = 0.3651 ±0.0200\n",
      "recall    = 0.6863 ±0.0364\n",
      "f1        = 0.4763 ±0.0232\n",
      "auc        = 0.8300 ±0.0185\n",
      "run_time    = 0.007571\n",
      "-------------- Training Run 3 of 100 --------------\n",
      "precision = 0.3577 ±0.0611\n",
      "recall    = 0.6849 ±0.0128\n",
      "f1        = 0.4669 ±0.0551\n",
      "auc        = 0.8291 ±0.0067\n",
      "run_time    = 0.007599\n",
      "-------------- Training Run 4 of 100 --------------\n",
      "precision = 0.3714 ±0.0296\n",
      "recall    = 0.6733 ±0.0172\n",
      "f1        = 0.4777 ±0.0225\n",
      "auc        = 0.8236 ±0.0082\n",
      "run_time    = 0.007445\n",
      "-------------- Training Run 5 of 100 --------------\n",
      "precision = 0.3557 ±0.0367\n",
      "recall    = 0.6946 ±0.0253\n",
      "f1        = 0.4685 ±0.0283\n",
      "auc        = 0.8339 ±0.0121\n",
      "run_time    = 0.006877\n",
      "-------------- Training Run 6 of 100 --------------\n",
      "precision = 0.3609 ±0.0343\n",
      "recall    = 0.6776 ±0.0345\n",
      "f1        = 0.4689 ±0.0250\n",
      "auc        = 0.8256 ±0.0167\n",
      "run_time    = 0.006500\n",
      "-------------- Training Run 7 of 100 --------------\n",
      "precision = 0.3478 ±0.0323\n",
      "recall    = 0.6888 ±0.0346\n",
      "f1        = 0.4620 ±0.0364\n",
      "auc        = 0.8309 ±0.0180\n",
      "run_time    = 0.007201\n",
      "-------------- Training Run 8 of 100 --------------\n",
      "precision = 0.3316 ±0.0494\n",
      "recall    = 0.7029 ±0.0567\n",
      "f1        = 0.4494 ±0.0552\n",
      "auc        = 0.8376 ±0.0291\n",
      "run_time    = 0.006906\n",
      "-------------- Training Run 9 of 100 --------------\n",
      "precision = 0.3483 ±0.0419\n",
      "recall    = 0.6908 ±0.0238\n",
      "f1        = 0.4612 ±0.0381\n",
      "auc        = 0.8319 ±0.0117\n",
      "run_time    = 0.006561\n",
      "-------------- Training Run 10 of 100 --------------\n",
      "precision = 0.3740 ±0.0163\n",
      "recall    = 0.6854 ±0.0288\n",
      "f1        = 0.4835 ±0.0142\n",
      "auc        = 0.8297 ±0.0144\n",
      "run_time    = 0.006515\n",
      "-------------- Training Run 11 of 100 --------------\n",
      "precision = 0.3688 ±0.0218\n",
      "recall    = 0.6794 ±0.0204\n",
      "f1        = 0.4774 ±0.0167\n",
      "auc        = 0.8266 ±0.0100\n",
      "run_time    = 0.006503\n",
      "-------------- Training Run 12 of 100 --------------\n",
      "precision = 0.2959 ±0.0586\n",
      "recall    = 0.7179 ±0.0131\n",
      "f1        = 0.4160 ±0.0598\n",
      "auc        = 0.8444 ±0.0072\n",
      "run_time    = 0.005849\n",
      "-------------- Training Run 13 of 100 --------------\n",
      "precision = 0.3725 ±0.0301\n",
      "recall    = 0.6784 ±0.0299\n",
      "f1        = 0.4800 ±0.0265\n",
      "auc        = 0.8262 ±0.0150\n",
      "run_time    = 0.007372\n",
      "-------------- Training Run 14 of 100 --------------\n",
      "precision = 0.3787 ±0.0599\n",
      "recall    = 0.6817 ±0.0207\n",
      "f1        = 0.4841 ±0.0471\n",
      "auc        = 0.8279 ±0.0103\n",
      "run_time    = 0.006322\n",
      "-------------- Training Run 15 of 100 --------------\n",
      "precision = 0.3751 ±0.0242\n",
      "recall    = 0.6780 ±0.0349\n",
      "f1        = 0.4828 ±0.0279\n",
      "auc        = 0.8260 ±0.0179\n",
      "run_time    = 0.006734\n",
      "-------------- Training Run 16 of 100 --------------\n",
      "precision = 0.3698 ±0.0258\n",
      "recall    = 0.6846 ±0.0214\n",
      "f1        = 0.4799 ±0.0249\n",
      "auc        = 0.8292 ±0.0110\n",
      "run_time    = 0.007070\n",
      "-------------- Training Run 17 of 100 --------------\n",
      "precision = 0.3368 ±0.0567\n",
      "recall    = 0.6957 ±0.0383\n",
      "f1        = 0.4504 ±0.0538\n",
      "auc        = 0.8341 ±0.0191\n",
      "run_time    = 0.006072\n",
      "-------------- Training Run 18 of 100 --------------\n",
      "precision = 0.3573 ±0.0323\n",
      "recall    = 0.6809 ±0.0144\n",
      "f1        = 0.4679 ±0.0300\n",
      "auc        = 0.8271 ±0.0076\n",
      "run_time    = 0.006209\n",
      "-------------- Training Run 19 of 100 --------------\n",
      "precision = 0.3416 ±0.0287\n",
      "recall    = 0.6950 ±0.0381\n",
      "f1        = 0.4572 ±0.0288\n",
      "auc        = 0.8338 ±0.0192\n",
      "run_time    = 0.005852\n",
      "-------------- Training Run 20 of 100 --------------\n",
      "precision = 0.3389 ±0.0493\n",
      "recall    = 0.6953 ±0.0268\n",
      "f1        = 0.4537 ±0.0455\n",
      "auc        = 0.8339 ±0.0136\n",
      "run_time    = 0.006572\n",
      "-------------- Training Run 21 of 100 --------------\n",
      "precision = 0.3662 ±0.0288\n",
      "recall    = 0.6829 ±0.0198\n",
      "f1        = 0.4763 ±0.0279\n",
      "auc        = 0.8283 ±0.0103\n",
      "run_time    = 0.006886\n",
      "-------------- Training Run 22 of 100 --------------\n",
      "precision = 0.3563 ±0.0778\n",
      "recall    = 0.6850 ±0.0340\n",
      "f1        = 0.4627 ±0.0658\n",
      "auc        = 0.8292 ±0.0167\n",
      "run_time    = 0.006376\n",
      "-------------- Training Run 23 of 100 --------------\n",
      "precision = 0.3892 ±0.0298\n",
      "recall    = 0.6743 ±0.0257\n",
      "f1        = 0.4930 ±0.0278\n",
      "auc        = 0.8245 ±0.0131\n",
      "run_time    = 0.007052\n",
      "-------------- Training Run 24 of 100 --------------\n",
      "precision = 0.3583 ±0.0445\n",
      "recall    = 0.7022 ±0.0253\n",
      "f1        = 0.4723 ±0.0400\n",
      "auc        = 0.8378 ±0.0123\n",
      "run_time    = 0.007166\n",
      "-------------- Training Run 25 of 100 --------------\n",
      "precision = 0.3463 ±0.0566\n",
      "recall    = 0.6997 ±0.0453\n",
      "f1        = 0.4586 ±0.0499\n",
      "auc        = 0.8363 ±0.0219\n",
      "run_time    = 0.006058\n",
      "-------------- Training Run 26 of 100 --------------\n",
      "precision = 0.3662 ±0.0352\n",
      "recall    = 0.6888 ±0.0223\n",
      "f1        = 0.4774 ±0.0345\n",
      "auc        = 0.8312 ±0.0116\n",
      "run_time    = 0.007513\n",
      "-------------- Training Run 27 of 100 --------------\n",
      "precision = 0.3373 ±0.0363\n",
      "recall    = 0.6974 ±0.0242\n",
      "f1        = 0.4535 ±0.0342\n",
      "auc        = 0.8350 ±0.0122\n",
      "run_time    = 0.005832\n",
      "-------------- Training Run 28 of 100 --------------\n",
      "precision = 0.3426 ±0.0556\n",
      "recall    = 0.6886 ±0.0322\n",
      "f1        = 0.4534 ±0.0440\n",
      "auc        = 0.8307 ±0.0152\n",
      "run_time    = 0.005429\n",
      "-------------- Training Run 29 of 100 --------------\n",
      "precision = 0.3353 ±0.0445\n",
      "recall    = 0.6871 ±0.0528\n",
      "f1        = 0.4469 ±0.0337\n",
      "auc        = 0.8298 ±0.0258\n",
      "run_time    = 0.006423\n",
      "-------------- Training Run 30 of 100 --------------\n",
      "precision = 0.3578 ±0.0142\n",
      "recall    = 0.6785 ±0.0216\n",
      "f1        = 0.4684 ±0.0156\n",
      "auc        = 0.8259 ±0.0110\n",
      "run_time    = 0.006969\n",
      "-------------- Training Run 31 of 100 --------------\n",
      "precision = 0.3630 ±0.0244\n",
      "recall    = 0.6829 ±0.0309\n",
      "f1        = 0.4733 ±0.0216\n",
      "auc        = 0.8282 ±0.0155\n",
      "run_time    = 0.006626\n",
      "-------------- Training Run 32 of 100 --------------\n",
      "precision = 0.3442 ±0.0338\n",
      "recall    = 0.6878 ±0.0316\n",
      "f1        = 0.4572 ±0.0276\n",
      "auc        = 0.8303 ±0.0156\n",
      "run_time    = 0.006278\n",
      "-------------- Training Run 33 of 100 --------------\n",
      "precision = 0.3787 ±0.0104\n",
      "recall    = 0.6833 ±0.0233\n",
      "f1        = 0.4872 ±0.0119\n",
      "auc        = 0.8287 ±0.0117\n",
      "run_time    = 0.006752\n",
      "-------------- Training Run 34 of 100 --------------\n",
      "precision = 0.3468 ±0.0530\n",
      "recall    = 0.6833 ±0.0090\n",
      "f1        = 0.4573 ±0.0488\n",
      "auc        = 0.8281 ±0.0041\n",
      "run_time    = 0.007371\n",
      "-------------- Training Run 35 of 100 --------------\n",
      "precision = 0.3253 ±0.0529\n",
      "recall    = 0.6986 ±0.0270\n",
      "f1        = 0.4413 ±0.0495\n",
      "auc        = 0.8354 ±0.0135\n",
      "run_time    = 0.006199\n",
      "-------------- Training Run 36 of 100 --------------\n",
      "precision = 0.3447 ±0.0580\n",
      "recall    = 0.6905 ±0.0350\n",
      "f1        = 0.4558 ±0.0518\n",
      "auc        = 0.8317 ±0.0170\n",
      "run_time    = 0.007513\n",
      "-------------- Training Run 37 of 100 --------------\n",
      "precision = 0.3557 ±0.0231\n",
      "recall    = 0.6834 ±0.0175\n",
      "f1        = 0.4672 ±0.0189\n",
      "auc        = 0.8283 ±0.0086\n",
      "run_time    = 0.006990\n",
      "-------------- Training Run 38 of 100 --------------\n",
      "precision = 0.3525 ±0.0444\n",
      "recall    = 0.6890 ±0.0404\n",
      "f1        = 0.4653 ±0.0445\n",
      "auc        = 0.8311 ±0.0207\n",
      "run_time    = 0.006643\n",
      "-------------- Training Run 39 of 100 --------------\n",
      "precision = 0.3740 ±0.0444\n",
      "recall    = 0.6832 ±0.0199\n",
      "f1        = 0.4818 ±0.0377\n",
      "auc        = 0.8286 ±0.0100\n",
      "run_time    = 0.006552\n",
      "-------------- Training Run 40 of 100 --------------\n",
      "precision = 0.3625 ±0.0362\n",
      "recall    = 0.6912 ±0.0191\n",
      "f1        = 0.4746 ±0.0328\n",
      "auc        = 0.8324 ±0.0098\n",
      "run_time    = 0.006882\n",
      "-------------- Training Run 41 of 100 --------------\n",
      "precision = 0.3211 ±0.0345\n",
      "recall    = 0.6997 ±0.0277\n",
      "f1        = 0.4386 ±0.0307\n",
      "auc        = 0.8358 ±0.0136\n",
      "run_time    = 0.005234\n",
      "-------------- Training Run 42 of 100 --------------\n",
      "precision = 0.3415 ±0.0593\n",
      "recall    = 0.6876 ±0.0296\n",
      "f1        = 0.4517 ±0.0506\n",
      "auc        = 0.8302 ±0.0138\n",
      "run_time    = 0.007269\n",
      "-------------- Training Run 43 of 100 --------------\n",
      "precision = 0.3887 ±0.0290\n",
      "recall    = 0.6736 ±0.0143\n",
      "f1        = 0.4920 ±0.0213\n",
      "auc        = 0.8241 ±0.0069\n",
      "run_time    = 0.006862\n",
      "-------------- Training Run 44 of 100 --------------\n",
      "precision = 0.3562 ±0.0365\n",
      "recall    = 0.6861 ±0.0330\n",
      "f1        = 0.4669 ±0.0307\n",
      "auc        = 0.8297 ±0.0161\n",
      "run_time    = 0.006749\n",
      "-------------- Training Run 45 of 100 --------------\n",
      "precision = 0.3557 ±0.0282\n",
      "recall    = 0.6937 ±0.0275\n",
      "f1        = 0.4699 ±0.0296\n",
      "auc        = 0.8335 ±0.0142\n",
      "run_time    = 0.006542\n",
      "-------------- Training Run 46 of 100 --------------\n",
      "precision = 0.3404 ±0.0574\n",
      "recall    = 0.6893 ±0.0244\n",
      "f1        = 0.4527 ±0.0552\n",
      "auc        = 0.8310 ±0.0123\n",
      "run_time    = 0.006029\n",
      "-------------- Training Run 47 of 100 --------------\n",
      "precision = 0.3457 ±0.0475\n",
      "recall    = 0.6831 ±0.0265\n",
      "f1        = 0.4567 ±0.0420\n",
      "auc        = 0.8280 ±0.0130\n",
      "run_time    = 0.006638\n",
      "-------------- Training Run 48 of 100 --------------\n",
      "precision = 0.3599 ±0.0162\n",
      "recall    = 0.6926 ±0.0167\n",
      "f1        = 0.4733 ±0.0126\n",
      "auc        = 0.8330 ±0.0082\n",
      "run_time    = 0.007066\n",
      "-------------- Training Run 49 of 100 --------------\n",
      "precision = 0.3845 ±0.0417\n",
      "recall    = 0.6738 ±0.0285\n",
      "f1        = 0.4879 ±0.0336\n",
      "auc        = 0.8241 ±0.0142\n",
      "run_time    = 0.007305\n",
      "-------------- Training Run 50 of 100 --------------\n",
      "precision = 0.3358 ±0.0405\n",
      "recall    = 0.6854 ±0.0216\n",
      "f1        = 0.4487 ±0.0364\n",
      "auc        = 0.8289 ±0.0104\n",
      "run_time    = 0.005446\n",
      "-------------- Training Run 51 of 100 --------------\n",
      "precision = 0.3552 ±0.0453\n",
      "recall    = 0.6926 ±0.0191\n",
      "f1        = 0.4678 ±0.0423\n",
      "auc        = 0.8329 ±0.0098\n",
      "run_time    = 0.007318\n",
      "-------------- Training Run 52 of 100 --------------\n",
      "precision = 0.3683 ±0.0348\n",
      "recall    = 0.6775 ±0.0446\n",
      "f1        = 0.4756 ±0.0312\n",
      "auc        = 0.8256 ±0.0223\n",
      "run_time    = 0.007085\n",
      "-------------- Training Run 53 of 100 --------------\n",
      "precision = 0.3594 ±0.0284\n",
      "recall    = 0.6833 ±0.0257\n",
      "f1        = 0.4698 ±0.0225\n",
      "auc        = 0.8284 ±0.0126\n",
      "run_time    = 0.005539\n",
      "-------------- Training Run 54 of 100 --------------\n",
      "precision = 0.3489 ±0.0595\n",
      "recall    = 0.6869 ±0.0194\n",
      "f1        = 0.4597 ±0.0573\n",
      "auc        = 0.8299 ±0.0101\n",
      "run_time    = 0.006690\n",
      "-------------- Training Run 55 of 100 --------------\n",
      "precision = 0.3782 ±0.0409\n",
      "recall    = 0.6741 ±0.0186\n",
      "f1        = 0.4827 ±0.0312\n",
      "auc        = 0.8241 ±0.0088\n",
      "run_time    = 0.006220\n",
      "-------------- Training Run 56 of 100 --------------\n",
      "precision = 0.3484 ±0.0684\n",
      "recall    = 0.6760 ±0.0413\n",
      "f1        = 0.4541 ±0.0641\n",
      "auc        = 0.8245 ±0.0202\n",
      "run_time    = 0.006546\n",
      "-------------- Training Run 57 of 100 --------------\n",
      "precision = 0.3221 ±0.0483\n",
      "recall    = 0.7057 ±0.0160\n",
      "f1        = 0.4404 ±0.0447\n",
      "auc        = 0.8388 ±0.0083\n",
      "run_time    = 0.006774\n",
      "-------------- Training Run 58 of 100 --------------\n",
      "precision = 0.3462 ±0.0421\n",
      "recall    = 0.6942 ±0.0399\n",
      "f1        = 0.4600 ±0.0379\n",
      "auc        = 0.8336 ±0.0199\n",
      "run_time    = 0.005928\n",
      "-------------- Training Run 59 of 100 --------------\n",
      "precision = 0.3803 ±0.0459\n",
      "recall    = 0.6795 ±0.0355\n",
      "f1        = 0.4862 ±0.0420\n",
      "auc        = 0.8269 ±0.0181\n",
      "run_time    = 0.007132\n",
      "-------------- Training Run 60 of 100 --------------\n",
      "precision = 0.3913 ±0.0361\n",
      "recall    = 0.6746 ±0.0415\n",
      "f1        = 0.4931 ±0.0237\n",
      "auc        = 0.8246 ±0.0203\n",
      "run_time    = 0.006557\n",
      "-------------- Training Run 61 of 100 --------------\n",
      "precision = 0.3808 ±0.0442\n",
      "recall    = 0.6845 ±0.0303\n",
      "f1        = 0.4865 ±0.0324\n",
      "auc        = 0.8294 ±0.0144\n",
      "run_time    = 0.006766\n",
      "-------------- Training Run 62 of 100 --------------\n",
      "precision = 0.3494 ±0.0564\n",
      "recall    = 0.6916 ±0.0217\n",
      "f1        = 0.4625 ±0.0531\n",
      "auc        = 0.8323 ±0.0119\n",
      "run_time    = 0.006694\n",
      "-------------- Training Run 63 of 100 --------------\n",
      "precision = 0.3850 ±0.0417\n",
      "recall    = 0.6699 ±0.0249\n",
      "f1        = 0.4879 ±0.0365\n",
      "auc        = 0.8222 ±0.0128\n",
      "run_time    = 0.007090\n",
      "-------------- Training Run 64 of 100 --------------\n",
      "precision = 0.3683 ±0.0281\n",
      "recall    = 0.6847 ±0.0345\n",
      "f1        = 0.4776 ±0.0217\n",
      "auc        = 0.8292 ±0.0170\n",
      "run_time    = 0.007055\n",
      "-------------- Training Run 65 of 100 --------------\n",
      "precision = 0.3625 ±0.0572\n",
      "recall    = 0.6834 ±0.0407\n",
      "f1        = 0.4717 ±0.0525\n",
      "auc        = 0.8285 ±0.0209\n",
      "run_time    = 0.006078\n",
      "-------------- Training Run 66 of 100 --------------\n",
      "precision = 0.3143 ±0.0477\n",
      "recall    = 0.7092 ±0.0245\n",
      "f1        = 0.4324 ±0.0424\n",
      "auc        = 0.8404 ±0.0114\n",
      "run_time    = 0.006003\n",
      "-------------- Training Run 67 of 100 --------------\n",
      "precision = 0.3677 ±0.0189\n",
      "recall    = 0.6819 ±0.0211\n",
      "f1        = 0.4775 ±0.0183\n",
      "auc        = 0.8278 ±0.0107\n",
      "run_time    = 0.007221\n",
      "-------------- Training Run 68 of 100 --------------\n",
      "precision = 0.3683 ±0.0262\n",
      "recall    = 0.6855 ±0.0202\n",
      "f1        = 0.4786 ±0.0239\n",
      "auc        = 0.8296 ±0.0102\n",
      "run_time    = 0.006666\n",
      "-------------- Training Run 69 of 100 --------------\n",
      "precision = 0.3373 ±0.0555\n",
      "recall    = 0.6867 ±0.0316\n",
      "f1        = 0.4488 ±0.0477\n",
      "auc        = 0.8296 ±0.0153\n",
      "run_time    = 0.006729\n",
      "-------------- Training Run 70 of 100 --------------\n",
      "precision = 0.3462 ±0.0416\n",
      "recall    = 0.6892 ±0.0189\n",
      "f1        = 0.4600 ±0.0402\n",
      "auc        = 0.8310 ±0.0102\n",
      "run_time    = 0.008215\n",
      "-------------- Training Run 71 of 100 --------------\n",
      "precision = 0.3636 ±0.0309\n",
      "recall    = 0.6835 ±0.0190\n",
      "f1        = 0.4736 ±0.0236\n",
      "auc        = 0.8285 ±0.0093\n",
      "run_time    = 0.006205\n",
      "-------------- Training Run 72 of 100 --------------\n",
      "precision = 0.3321 ±0.0338\n",
      "recall    = 0.6968 ±0.0265\n",
      "f1        = 0.4489 ±0.0338\n",
      "auc        = 0.8346 ±0.0135\n",
      "run_time    = 0.006229\n",
      "-------------- Training Run 73 of 100 --------------\n",
      "precision = 0.3651 ±0.0349\n",
      "recall    = 0.6844 ±0.0242\n",
      "f1        = 0.4750 ±0.0310\n",
      "auc        = 0.8290 ±0.0121\n",
      "run_time    = 0.006977\n",
      "-------------- Training Run 74 of 100 --------------\n",
      "precision = 0.3473 ±0.0576\n",
      "recall    = 0.6950 ±0.0358\n",
      "f1        = 0.4603 ±0.0517\n",
      "auc        = 0.8340 ±0.0180\n",
      "run_time    = 0.006214\n",
      "-------------- Training Run 75 of 100 --------------\n",
      "precision = 0.3499 ±0.0184\n",
      "recall    = 0.6927 ±0.0418\n",
      "f1        = 0.4642 ±0.0173\n",
      "auc        = 0.8329 ±0.0209\n",
      "run_time    = 0.007085\n",
      "-------------- Training Run 76 of 100 --------------\n",
      "precision = 0.3766 ±0.0177\n",
      "recall    = 0.6788 ±0.0195\n",
      "f1        = 0.4844 ±0.0185\n",
      "auc        = 0.8265 ±0.0100\n",
      "run_time    = 0.007455\n",
      "-------------- Training Run 77 of 100 --------------\n",
      "precision = 0.3656 ±0.0585\n",
      "recall    = 0.6790 ±0.0109\n",
      "f1        = 0.4723 ±0.0502\n",
      "auc        = 0.8263 ±0.0053\n",
      "run_time    = 0.006486\n",
      "-------------- Training Run 78 of 100 --------------\n",
      "precision = 0.3347 ±0.0376\n",
      "recall    = 0.6980 ±0.0220\n",
      "f1        = 0.4510 ±0.0350\n",
      "auc        = 0.8352 ±0.0109\n",
      "run_time    = 0.005175\n",
      "-------------- Training Run 79 of 100 --------------\n",
      "precision = 0.3730 ±0.0239\n",
      "recall    = 0.6825 ±0.0355\n",
      "f1        = 0.4822 ±0.0280\n",
      "auc        = 0.8282 ±0.0182\n",
      "run_time    = 0.006867\n",
      "-------------- Training Run 80 of 100 --------------\n",
      "precision = 0.3876 ±0.0117\n",
      "recall    = 0.6768 ±0.0345\n",
      "f1        = 0.4928 ±0.0170\n",
      "auc        = 0.8257 ±0.0174\n",
      "run_time    = 0.007331\n",
      "-------------- Training Run 81 of 100 --------------\n",
      "precision = 0.3389 ±0.0639\n",
      "recall    = 0.6860 ±0.0334\n",
      "f1        = 0.4507 ±0.0642\n",
      "auc        = 0.8293 ±0.0174\n",
      "run_time    = 0.006643\n",
      "-------------- Training Run 82 of 100 --------------\n",
      "precision = 0.3274 ±0.0403\n",
      "recall    = 0.6984 ±0.0372\n",
      "f1        = 0.4441 ±0.0408\n",
      "auc        = 0.8353 ±0.0187\n",
      "run_time    = 0.006392\n",
      "-------------- Training Run 83 of 100 --------------\n",
      "precision = 0.3400 ±0.0424\n",
      "recall    = 0.6842 ±0.0312\n",
      "f1        = 0.4525 ±0.0409\n",
      "auc        = 0.8284 ±0.0157\n",
      "run_time    = 0.006562\n",
      "-------------- Training Run 84 of 100 --------------\n",
      "precision = 0.3620 ±0.0106\n",
      "recall    = 0.6853 ±0.0390\n",
      "f1        = 0.4732 ±0.0117\n",
      "auc        = 0.8294 ±0.0195\n",
      "run_time    = 0.006132\n",
      "-------------- Training Run 85 of 100 --------------\n",
      "precision = 0.3856 ±0.0229\n",
      "recall    = 0.6799 ±0.0277\n",
      "f1        = 0.4914 ±0.0189\n",
      "auc        = 0.8272 ±0.0138\n",
      "run_time    = 0.006040\n",
      "-------------- Training Run 86 of 100 --------------\n",
      "precision = 0.3447 ±0.0249\n",
      "recall    = 0.6878 ±0.0282\n",
      "f1        = 0.4584 ±0.0214\n",
      "auc        = 0.8303 ±0.0140\n",
      "run_time    = 0.006448\n",
      "-------------- Training Run 87 of 100 --------------\n",
      "precision = 0.3536 ±0.0527\n",
      "recall    = 0.6927 ±0.0280\n",
      "f1        = 0.4662 ±0.0483\n",
      "auc        = 0.8329 ±0.0144\n",
      "run_time    = 0.006537\n",
      "-------------- Training Run 88 of 100 --------------\n",
      "precision = 0.3589 ±0.0431\n",
      "recall    = 0.6866 ±0.0295\n",
      "f1        = 0.4695 ±0.0380\n",
      "auc        = 0.8300 ±0.0147\n",
      "run_time    = 0.006081\n",
      "-------------- Training Run 89 of 100 --------------\n",
      "precision = 0.3290 ±0.0301\n",
      "recall    = 0.7022 ±0.0369\n",
      "f1        = 0.4469 ±0.0284\n",
      "auc        = 0.8372 ±0.0184\n",
      "run_time    = 0.005863\n",
      "-------------- Training Run 90 of 100 --------------\n",
      "precision = 0.3604 ±0.0394\n",
      "recall    = 0.6870 ±0.0303\n",
      "f1        = 0.4717 ±0.0363\n",
      "auc        = 0.8302 ±0.0154\n",
      "run_time    = 0.007310\n",
      "-------------- Training Run 91 of 100 --------------\n",
      "precision = 0.3337 ±0.0729\n",
      "recall    = 0.6777 ±0.0225\n",
      "f1        = 0.4424 ±0.0653\n",
      "auc        = 0.8251 ±0.0113\n",
      "run_time    = 0.005603\n",
      "-------------- Training Run 92 of 100 --------------\n",
      "precision = 0.3651 ±0.0650\n",
      "recall    = 0.6855 ±0.0204\n",
      "f1        = 0.4719 ±0.0592\n",
      "auc        = 0.8296 ±0.0095\n",
      "run_time    = 0.006825\n",
      "-------------- Training Run 93 of 100 --------------\n",
      "precision = 0.3934 ±0.0343\n",
      "recall    = 0.6756 ±0.0272\n",
      "f1        = 0.4965 ±0.0314\n",
      "auc        = 0.8252 ±0.0139\n",
      "run_time    = 0.007315\n",
      "-------------- Training Run 94 of 100 --------------\n",
      "precision = 0.3206 ±0.0538\n",
      "recall    = 0.6957 ±0.0438\n",
      "f1        = 0.4346 ±0.0482\n",
      "auc        = 0.8338 ±0.0213\n",
      "run_time    = 0.007205\n",
      "-------------- Training Run 95 of 100 --------------\n",
      "precision = 0.3651 ±0.0102\n",
      "recall    = 0.6833 ±0.0159\n",
      "f1        = 0.4757 ±0.0073\n",
      "auc        = 0.8285 ±0.0078\n",
      "run_time    = 0.007067\n",
      "-------------- Training Run 96 of 100 --------------\n",
      "precision = 0.3441 ±0.0531\n",
      "recall    = 0.6894 ±0.0222\n",
      "f1        = 0.4561 ±0.0459\n",
      "auc        = 0.8311 ±0.0107\n",
      "run_time    = 0.006210\n",
      "-------------- Training Run 97 of 100 --------------\n",
      "precision = 0.3641 ±0.0410\n",
      "recall    = 0.6809 ±0.0294\n",
      "f1        = 0.4724 ±0.0325\n",
      "auc        = 0.8272 ±0.0144\n",
      "run_time    = 0.006724\n",
      "-------------- Training Run 98 of 100 --------------\n",
      "precision = 0.3442 ±0.0544\n",
      "recall    = 0.6899 ±0.0270\n",
      "f1        = 0.4563 ±0.0476\n",
      "auc        = 0.8313 ±0.0133\n",
      "run_time    = 0.006902\n",
      "-------------- Training Run 99 of 100 --------------\n",
      "precision = 0.3489 ±0.0795\n",
      "recall    = 0.6728 ±0.0415\n",
      "f1        = 0.4540 ±0.0666\n",
      "auc        = 0.8229 ±0.0208\n",
      "run_time    = 0.006059\n",
      "-------------- Training Run 100 of 100 --------------\n",
      "precision = 0.3463 ±0.0345\n",
      "recall    = 0.6953 ±0.0383\n",
      "f1        = 0.4604 ±0.0277\n",
      "auc        = 0.8341 ±0.0188\n",
      "run_time    = 0.006366\n",
      "-------------- Average Over All Runs --------------\n",
      "precision = 0.3555 ±0.0186\n",
      "recall    = 0.6870 ±0.0085\n",
      "f1        = 0.4663 ±0.0154\n",
      "auc        = 0.8301 ±0.0040\n",
      "run_time    = 0.006646\n"
     ]
    }
   ],
   "source": [
    "mlp_model = MLPClassifier(activation='relu', hidden_layer_sizes=(100,), solver='adam')\n",
    "features = twisted_df.drop(['Domain', 'Homograph', 'Registered'], axis=1)\n",
    "model_eval(features[['EMBD_EUC','EMBD_L1', 'EMBD_COS']], twisted_df['Registered'] , mlp_model, training_runs=100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- Training Run 1 of 100 --------------\n",
      "precision = 0.2677 ±0.0156\n",
      "recall    = 0.7031 ±0.0329\n",
      "f1        = 0.3872 ±0.0155\n",
      "auc        = 0.8365 ±0.0164\n",
      "run_time    = 0.001335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- Training Run 2 of 100 --------------\n",
      "precision = 0.2703 ±0.0234\n",
      "recall    = 0.7041 ±0.0501\n",
      "f1        = 0.3902 ±0.0302\n",
      "auc        = 0.8370 ±0.0253\n",
      "run_time    = 0.001442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- Training Run 3 of 100 --------------\n",
      "precision = 0.2719 ±0.0114\n",
      "recall    = 0.7054 ±0.0299\n",
      "f1        = 0.3924 ±0.0160\n",
      "auc        = 0.8377 ±0.0151\n",
      "run_time    = 0.001311\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- Training Run 4 of 100 --------------\n",
      "precision = 0.2740 ±0.0073\n",
      "recall    = 0.7073 ±0.0315\n",
      "f1        = 0.3947 ±0.0087\n",
      "auc        = 0.8387 ±0.0158\n",
      "run_time    = 0.001286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- Training Run 5 of 100 --------------\n",
      "precision = 0.2719 ±0.0174\n",
      "recall    = 0.7026 ±0.0288\n",
      "f1        = 0.3916 ±0.0186\n",
      "auc        = 0.8363 ±0.0144\n",
      "run_time    = 0.001256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- Training Run 6 of 100 --------------\n",
      "precision = 0.2698 ±0.0166\n",
      "recall    = 0.7055 ±0.0302\n",
      "f1        = 0.3899 ±0.0191\n",
      "auc        = 0.8377 ±0.0152\n",
      "run_time    = 0.001468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- Training Run 7 of 100 --------------\n",
      "precision = 0.2703 ±0.0229\n",
      "recall    = 0.7083 ±0.0324\n",
      "f1        = 0.3901 ±0.0214\n",
      "auc        = 0.8391 ±0.0158\n",
      "run_time    = 0.001665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- Training Run 8 of 100 --------------\n",
      "precision = 0.2703 ±0.0237\n",
      "recall    = 0.7036 ±0.0321\n",
      "f1        = 0.3902 ±0.0284\n",
      "auc        = 0.8368 ±0.0164\n",
      "run_time    = 0.001395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- Training Run 9 of 100 --------------\n",
      "precision = 0.2682 ±0.0120\n",
      "recall    = 0.7064 ±0.0295\n",
      "f1        = 0.3883 ±0.0108\n",
      "auc        = 0.8381 ±0.0147\n",
      "run_time    = 0.002746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- Training Run 10 of 100 --------------\n",
      "precision = 0.2724 ±0.0207\n",
      "recall    = 0.7068 ±0.0334\n",
      "f1        = 0.3925 ±0.0199\n",
      "auc        = 0.8384 ±0.0166\n",
      "run_time    = 0.001322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- Training Run 11 of 100 --------------\n",
      "precision = 0.2703 ±0.0184\n",
      "recall    = 0.7069 ±0.0145\n",
      "f1        = 0.3907 ±0.0197\n",
      "auc        = 0.8384 ±0.0073\n",
      "run_time    = 0.001300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- Training Run 12 of 100 --------------\n",
      "precision = 0.2703 ±0.0226\n",
      "recall    = 0.7049 ±0.0500\n",
      "f1        = 0.3906 ±0.0304\n",
      "auc        = 0.8374 ±0.0254\n",
      "run_time    = 0.001274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- Training Run 13 of 100 --------------\n",
      "precision = 0.2714 ±0.0286\n",
      "recall    = 0.7043 ±0.0312\n",
      "f1        = 0.3910 ±0.0324\n",
      "auc        = 0.8371 ±0.0158\n",
      "run_time    = 0.001321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- Training Run 14 of 100 --------------\n",
      "precision = 0.2734 ±0.0126\n",
      "recall    = 0.7077 ±0.0487\n",
      "f1        = 0.3937 ±0.0105\n",
      "auc        = 0.8388 ±0.0242\n",
      "run_time    = 0.001329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- Training Run 15 of 100 --------------\n",
      "precision = 0.2708 ±0.0203\n",
      "recall    = 0.7076 ±0.0280\n",
      "f1        = 0.3909 ±0.0196\n",
      "auc        = 0.8387 ±0.0138\n",
      "run_time    = 0.001277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- Training Run 16 of 100 --------------\n",
      "precision = 0.2729 ±0.0094\n",
      "recall    = 0.7091 ±0.0316\n",
      "f1        = 0.3937 ±0.0073\n",
      "auc        = 0.8395 ±0.0157\n",
      "run_time    = 0.001317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- Training Run 17 of 100 --------------\n",
      "precision = 0.2729 ±0.0294\n",
      "recall    = 0.7059 ±0.0202\n",
      "f1        = 0.3928 ±0.0312\n",
      "auc        = 0.8380 ±0.0103\n",
      "run_time    = 0.001254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- Training Run 18 of 100 --------------\n",
      "precision = 0.2693 ±0.0205\n",
      "recall    = 0.7040 ±0.0263\n",
      "f1        = 0.3890 ±0.0228\n",
      "auc        = 0.8369 ±0.0132\n",
      "run_time    = 0.001324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- Training Run 19 of 100 --------------\n",
      "precision = 0.2693 ±0.0394\n",
      "recall    = 0.7005 ±0.0242\n",
      "f1        = 0.3878 ±0.0437\n",
      "auc        = 0.8352 ±0.0127\n",
      "run_time    = 0.001347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- Training Run 20 of 100 --------------\n",
      "precision = 0.2724 ±0.0221\n",
      "recall    = 0.7048 ±0.0325\n",
      "f1        = 0.3928 ±0.0275\n",
      "auc        = 0.8374 ±0.0167\n",
      "run_time    = 0.001391\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- Training Run 21 of 100 --------------\n",
      "precision = 0.2729 ±0.0251\n",
      "recall    = 0.7081 ±0.0317\n",
      "f1        = 0.3932 ±0.0275\n",
      "auc        = 0.8391 ±0.0159\n",
      "run_time    = 0.001315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- Training Run 22 of 100 --------------\n",
      "precision = 0.2687 ±0.0140\n",
      "recall    = 0.7060 ±0.0473\n",
      "f1        = 0.3891 ±0.0206\n",
      "auc        = 0.8379 ±0.0239\n",
      "run_time    = 0.001419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- Training Run 23 of 100 --------------\n",
      "precision = 0.2719 ±0.0326\n",
      "recall    = 0.7049 ±0.0255\n",
      "f1        = 0.3916 ±0.0366\n",
      "auc        = 0.8374 ±0.0132\n",
      "run_time    = 0.001284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- Training Run 24 of 100 --------------\n",
      "precision = 0.2719 ±0.0168\n",
      "recall    = 0.7062 ±0.0247\n",
      "f1        = 0.3924 ±0.0197\n",
      "auc        = 0.8381 ±0.0126\n",
      "run_time    = 0.001395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- Training Run 25 of 100 --------------\n",
      "precision = 0.2724 ±0.0167\n",
      "recall    = 0.7038 ±0.0369\n",
      "f1        = 0.3924 ±0.0200\n",
      "auc        = 0.8369 ±0.0186\n",
      "run_time    = 0.001330\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- Training Run 26 of 100 --------------\n",
      "precision = 0.2729 ±0.0149\n",
      "recall    = 0.7051 ±0.0402\n",
      "f1        = 0.3933 ±0.0194\n",
      "auc        = 0.8376 ±0.0203\n",
      "run_time    = 0.001346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- Training Run 27 of 100 --------------\n",
      "precision = 0.2682 ±0.0314\n",
      "recall    = 0.7045 ±0.0288\n",
      "f1        = 0.3878 ±0.0353\n",
      "auc        = 0.8372 ±0.0148\n",
      "run_time    = 0.001278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- Training Run 28 of 100 --------------\n",
      "precision = 0.2703 ±0.0191\n",
      "recall    = 0.7048 ±0.0343\n",
      "f1        = 0.3901 ±0.0208\n",
      "auc        = 0.8374 ±0.0171\n",
      "run_time    = 0.002588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- Training Run 29 of 100 --------------\n",
      "precision = 0.2735 ±0.0221\n",
      "recall    = 0.7055 ±0.0367\n",
      "f1        = 0.3936 ±0.0256\n",
      "auc        = 0.8377 ±0.0185\n",
      "run_time    = 0.001299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- Training Run 30 of 100 --------------\n",
      "precision = 0.2724 ±0.0109\n",
      "recall    = 0.7055 ±0.0357\n",
      "f1        = 0.3929 ±0.0154\n",
      "auc        = 0.8377 ±0.0180\n",
      "run_time    = 0.001314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- Training Run 31 of 100 --------------\n",
      "precision = 0.2719 ±0.0172\n",
      "recall    = 0.7018 ±0.0199\n",
      "f1        = 0.3918 ±0.0207\n",
      "auc        = 0.8359 ±0.0103\n",
      "run_time    = 0.001326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- Training Run 32 of 100 --------------\n",
      "precision = 0.2735 ±0.0145\n",
      "recall    = 0.7064 ±0.0253\n",
      "f1        = 0.3939 ±0.0156\n",
      "auc        = 0.8382 ±0.0126\n",
      "run_time    = 0.001286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- Training Run 33 of 100 --------------\n",
      "precision = 0.2708 ±0.0310\n",
      "recall    = 0.7024 ±0.0220\n",
      "f1        = 0.3902 ±0.0348\n",
      "auc        = 0.8362 ±0.0114\n",
      "run_time    = 0.001330\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- Training Run 34 of 100 --------------\n",
      "precision = 0.2713 ±0.0282\n",
      "recall    = 0.7050 ±0.0333\n",
      "f1        = 0.3911 ±0.0314\n",
      "auc        = 0.8375 ±0.0169\n",
      "run_time    = 0.001417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- Training Run 35 of 100 --------------\n",
      "precision = 0.2703 ±0.0256\n",
      "recall    = 0.7034 ±0.0277\n",
      "f1        = 0.3901 ±0.0297\n",
      "auc        = 0.8367 ±0.0142\n",
      "run_time    = 0.001431\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- Training Run 36 of 100 --------------\n",
      "precision = 0.2698 ±0.0317\n",
      "recall    = 0.7019 ±0.0492\n",
      "f1        = 0.3892 ±0.0392\n",
      "auc        = 0.8359 ±0.0251\n",
      "run_time    = 0.001212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- Training Run 37 of 100 --------------\n",
      "precision = 0.2703 ±0.0295\n",
      "recall    = 0.7042 ±0.0294\n",
      "f1        = 0.3898 ±0.0323\n",
      "auc        = 0.8371 ±0.0149\n",
      "run_time    = 0.001364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- Training Run 38 of 100 --------------\n",
      "precision = 0.2714 ±0.0212\n",
      "recall    = 0.7033 ±0.0470\n",
      "f1        = 0.3911 ±0.0260\n",
      "auc        = 0.8366 ±0.0237\n",
      "run_time    = 0.001299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- Training Run 39 of 100 --------------\n",
      "precision = 0.2729 ±0.0143\n",
      "recall    = 0.7065 ±0.0321\n",
      "f1        = 0.3936 ±0.0180\n",
      "auc        = 0.8383 ±0.0162\n",
      "run_time    = 0.001282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- Training Run 40 of 100 --------------\n",
      "precision = 0.2703 ±0.0086\n",
      "recall    = 0.7102 ±0.0444\n",
      "f1        = 0.3910 ±0.0081\n",
      "auc        = 0.8400 ±0.0222\n",
      "run_time    = 0.001250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- Training Run 41 of 100 --------------\n",
      "precision = 0.2714 ±0.0219\n",
      "recall    = 0.7079 ±0.0269\n",
      "f1        = 0.3917 ±0.0233\n",
      "auc        = 0.8389 ±0.0134\n",
      "run_time    = 0.001334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- Training Run 42 of 100 --------------\n",
      "precision = 0.2713 ±0.0381\n",
      "recall    = 0.7013 ±0.0257\n",
      "f1        = 0.3904 ±0.0431\n",
      "auc        = 0.8356 ±0.0136\n",
      "run_time    = 0.001333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- Training Run 43 of 100 --------------\n",
      "precision = 0.2693 ±0.0137\n",
      "recall    = 0.7055 ±0.0490\n",
      "f1        = 0.3892 ±0.0167\n",
      "auc        = 0.8377 ±0.0245\n",
      "run_time    = 0.001255\n",
      "-------------- Training Run 44 of 100 --------------\n",
      "precision = 0.2708 ±0.0234\n",
      "recall    = 0.7037 ±0.0221\n",
      "f1        = 0.3907 ±0.0272\n",
      "auc        = 0.8368 ±0.0114\n",
      "run_time    = 0.001171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- Training Run 45 of 100 --------------\n",
      "precision = 0.2719 ±0.0319\n",
      "recall    = 0.7048 ±0.0578\n",
      "f1        = 0.3921 ±0.0415\n",
      "auc        = 0.8374 ±0.0295\n",
      "run_time    = 0.001370\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- Training Run 46 of 100 --------------\n",
      "precision = 0.2692 ±0.0321\n",
      "recall    = 0.7020 ±0.0178\n",
      "f1        = 0.3882 ±0.0354\n",
      "auc        = 0.8359 ±0.0091\n",
      "run_time    = 0.001549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- Training Run 47 of 100 --------------\n",
      "precision = 0.2713 ±0.0149\n",
      "recall    = 0.7058 ±0.0221\n",
      "f1        = 0.3916 ±0.0152\n",
      "auc        = 0.8379 ±0.0110\n",
      "run_time    = 0.001333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- Training Run 48 of 100 --------------\n",
      "precision = 0.2719 ±0.0109\n",
      "recall    = 0.7070 ±0.0279\n",
      "f1        = 0.3923 ±0.0094\n",
      "auc        = 0.8385 ±0.0138\n",
      "run_time    = 0.001312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- Training Run 49 of 100 --------------\n",
      "precision = 0.2708 ±0.0110\n",
      "recall    = 0.7064 ±0.0166\n",
      "f1        = 0.3914 ±0.0127\n",
      "auc        = 0.8382 ±0.0084\n",
      "run_time    = 0.001395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- Training Run 50 of 100 --------------\n",
      "precision = 0.2703 ±0.0185\n",
      "recall    = 0.7046 ±0.0245\n",
      "f1        = 0.3905 ±0.0222\n",
      "auc        = 0.8372 ±0.0125\n",
      "run_time    = 0.001277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- Training Run 51 of 100 --------------\n",
      "precision = 0.2703 ±0.0203\n",
      "recall    = 0.7034 ±0.0201\n",
      "f1        = 0.3902 ±0.0238\n",
      "auc        = 0.8367 ±0.0103\n",
      "run_time    = 0.001264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- Training Run 52 of 100 --------------\n",
      "precision = 0.2682 ±0.0053\n",
      "recall    = 0.7027 ±0.0170\n",
      "f1        = 0.3882 ±0.0063\n",
      "auc        = 0.8363 ±0.0085\n",
      "run_time    = 0.001264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- Training Run 53 of 100 --------------\n",
      "precision = 0.2714 ±0.0243\n",
      "recall    = 0.7052 ±0.0238\n",
      "f1        = 0.3915 ±0.0281\n",
      "auc        = 0.8376 ±0.0122\n",
      "run_time    = 0.001789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- Training Run 54 of 100 --------------\n",
      "precision = 0.2714 ±0.0254\n",
      "recall    = 0.7059 ±0.0187\n",
      "f1        = 0.3916 ±0.0284\n",
      "auc        = 0.8379 ±0.0098\n",
      "run_time    = 0.001337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- Training Run 55 of 100 --------------\n",
      "precision = 0.2708 ±0.0147\n",
      "recall    = 0.7048 ±0.0254\n",
      "f1        = 0.3908 ±0.0144\n",
      "auc        = 0.8373 ±0.0126\n",
      "run_time    = 0.001573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- Training Run 56 of 100 --------------\n",
      "precision = 0.2703 ±0.0206\n",
      "recall    = 0.7026 ±0.0224\n",
      "f1        = 0.3901 ±0.0241\n",
      "auc        = 0.8362 ±0.0115\n",
      "run_time    = 0.001321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- Training Run 57 of 100 --------------\n",
      "precision = 0.2719 ±0.0218\n",
      "recall    = 0.7042 ±0.0265\n",
      "f1        = 0.3919 ±0.0249\n",
      "auc        = 0.8371 ±0.0135\n",
      "run_time    = 0.001293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- Training Run 58 of 100 --------------\n",
      "precision = 0.2703 ±0.0168\n",
      "recall    = 0.7025 ±0.0139\n",
      "f1        = 0.3902 ±0.0194\n",
      "auc        = 0.8362 ±0.0073\n",
      "run_time    = 0.001230\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- Training Run 59 of 100 --------------\n",
      "precision = 0.2703 ±0.0132\n",
      "recall    = 0.7035 ±0.0184\n",
      "f1        = 0.3903 ±0.0141\n",
      "auc        = 0.8367 ±0.0092\n",
      "run_time    = 0.001381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- Training Run 60 of 100 --------------\n",
      "precision = 0.2703 ±0.0206\n",
      "recall    = 0.7053 ±0.0151\n",
      "f1        = 0.3905 ±0.0235\n",
      "auc        = 0.8376 ±0.0079\n",
      "run_time    = 0.001350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- Training Run 61 of 100 --------------\n",
      "precision = 0.2713 ±0.0121\n",
      "recall    = 0.7029 ±0.0176\n",
      "f1        = 0.3914 ±0.0143\n",
      "auc        = 0.8364 ±0.0089\n",
      "run_time    = 0.001282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- Training Run 62 of 100 --------------\n",
      "precision = 0.2703 ±0.0051\n",
      "recall    = 0.7043 ±0.0282\n",
      "f1        = 0.3905 ±0.0055\n",
      "auc        = 0.8371 ±0.0141\n",
      "run_time    = 0.002818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- Training Run 63 of 100 --------------\n",
      "precision = 0.2729 ±0.0189\n",
      "recall    = 0.7070 ±0.0233\n",
      "f1        = 0.3933 ±0.0191\n",
      "auc        = 0.8385 ±0.0116\n",
      "run_time    = 0.001317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- Training Run 64 of 100 --------------\n",
      "precision = 0.2713 ±0.0352\n",
      "recall    = 0.7032 ±0.0241\n",
      "f1        = 0.3907 ±0.0395\n",
      "auc        = 0.8366 ±0.0126\n",
      "run_time    = 0.001289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- Training Run 65 of 100 --------------\n",
      "precision = 0.2713 ±0.0308\n",
      "recall    = 0.7027 ±0.0361\n",
      "f1        = 0.3905 ±0.0328\n",
      "auc        = 0.8363 ±0.0182\n",
      "run_time    = 0.003039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- Training Run 66 of 100 --------------\n",
      "precision = 0.2719 ±0.0179\n",
      "recall    = 0.7063 ±0.0357\n",
      "f1        = 0.3924 ±0.0228\n",
      "auc        = 0.8381 ±0.0181\n",
      "run_time    = 0.001311\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- Training Run 67 of 100 --------------\n",
      "precision = 0.2713 ±0.0195\n",
      "recall    = 0.7049 ±0.0403\n",
      "f1        = 0.3912 ±0.0215\n",
      "auc        = 0.8374 ±0.0202\n",
      "run_time    = 0.001311\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- Training Run 68 of 100 --------------\n",
      "precision = 0.2703 ±0.0217\n",
      "recall    = 0.7031 ±0.0332\n",
      "f1        = 0.3903 ±0.0272\n",
      "auc        = 0.8365 ±0.0170\n",
      "run_time    = 0.001458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- Training Run 69 of 100 --------------\n",
      "precision = 0.2708 ±0.0212\n",
      "recall    = 0.7045 ±0.0188\n",
      "f1        = 0.3908 ±0.0230\n",
      "auc        = 0.8372 ±0.0095\n",
      "run_time    = 0.001277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- Training Run 70 of 100 --------------\n",
      "precision = 0.2703 ±0.0165\n",
      "recall    = 0.7027 ±0.0238\n",
      "f1        = 0.3903 ±0.0202\n",
      "auc        = 0.8363 ±0.0122\n",
      "run_time    = 0.001222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- Training Run 71 of 100 --------------\n",
      "precision = 0.2692 ±0.0204\n",
      "recall    = 0.7059 ±0.0363\n",
      "f1        = 0.3890 ±0.0203\n",
      "auc        = 0.8379 ±0.0180\n",
      "run_time    = 0.001339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- Training Run 72 of 100 --------------\n",
      "precision = 0.2719 ±0.0122\n",
      "recall    = 0.7090 ±0.0513\n",
      "f1        = 0.3927 ±0.0181\n",
      "auc        = 0.8395 ±0.0257\n",
      "run_time    = 0.001618\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- Training Run 73 of 100 --------------\n",
      "precision = 0.2698 ±0.0217\n",
      "recall    = 0.7027 ±0.0325\n",
      "f1        = 0.3891 ±0.0217\n",
      "auc        = 0.8363 ±0.0162\n",
      "run_time    = 0.001342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- Training Run 74 of 100 --------------\n",
      "precision = 0.2713 ±0.0118\n",
      "recall    = 0.7063 ±0.0211\n",
      "f1        = 0.3919 ±0.0132\n",
      "auc        = 0.8381 ±0.0106\n",
      "run_time    = 0.001390\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- Training Run 75 of 100 --------------\n",
      "precision = 0.2734 ±0.0152\n",
      "recall    = 0.7054 ±0.0216\n",
      "f1        = 0.3937 ±0.0150\n",
      "auc        = 0.8377 ±0.0108\n",
      "run_time    = 0.001578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- Training Run 76 of 100 --------------\n",
      "precision = 0.2713 ±0.0151\n",
      "recall    = 0.7057 ±0.0114\n",
      "f1        = 0.3917 ±0.0165\n",
      "auc        = 0.8378 ±0.0058\n",
      "run_time    = 0.001338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- Training Run 77 of 100 --------------\n",
      "precision = 0.2708 ±0.0216\n",
      "recall    = 0.7056 ±0.0445\n",
      "f1        = 0.3910 ±0.0268\n",
      "auc        = 0.8378 ±0.0225\n",
      "run_time    = 0.001259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- Training Run 78 of 100 --------------\n",
      "precision = 0.2708 ±0.0179\n",
      "recall    = 0.7034 ±0.0297\n",
      "f1        = 0.3909 ±0.0217\n",
      "auc        = 0.8367 ±0.0151\n",
      "run_time    = 0.001386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- Training Run 79 of 100 --------------\n",
      "precision = 0.2693 ±0.0221\n",
      "recall    = 0.7017 ±0.0249\n",
      "f1        = 0.3886 ±0.0242\n",
      "auc        = 0.8358 ±0.0125\n",
      "run_time    = 0.001375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- Training Run 80 of 100 --------------\n",
      "precision = 0.2719 ±0.0195\n",
      "recall    = 0.7044 ±0.0335\n",
      "f1        = 0.3923 ±0.0253\n",
      "auc        = 0.8372 ±0.0171\n",
      "run_time    = 0.001683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- Training Run 81 of 100 --------------\n",
      "precision = 0.2724 ±0.0219\n",
      "recall    = 0.7061 ±0.0150\n",
      "f1        = 0.3927 ±0.0246\n",
      "auc        = 0.8381 ±0.0077\n",
      "run_time    = 0.001398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- Training Run 82 of 100 --------------\n",
      "precision = 0.2692 ±0.0234\n",
      "recall    = 0.7041 ±0.0118\n",
      "f1        = 0.3890 ±0.0249\n",
      "auc        = 0.8370 ±0.0060\n",
      "run_time    = 0.001407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- Training Run 83 of 100 --------------\n",
      "precision = 0.2719 ±0.0139\n",
      "recall    = 0.7030 ±0.0369\n",
      "f1        = 0.3920 ±0.0188\n",
      "auc        = 0.8365 ±0.0186\n",
      "run_time    = 0.001460\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- Training Run 84 of 100 --------------\n",
      "precision = 0.2692 ±0.0264\n",
      "recall    = 0.7030 ±0.0359\n",
      "f1        = 0.3889 ±0.0308\n",
      "auc        = 0.8364 ±0.0182\n",
      "run_time    = 0.001362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- Training Run 85 of 100 --------------\n",
      "precision = 0.2714 ±0.0153\n",
      "recall    = 0.7050 ±0.0248\n",
      "f1        = 0.3917 ±0.0183\n",
      "auc        = 0.8375 ±0.0125\n",
      "run_time    = 0.001345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- Training Run 86 of 100 --------------\n",
      "precision = 0.2724 ±0.0135\n",
      "recall    = 0.7084 ±0.0200\n",
      "f1        = 0.3932 ±0.0133\n",
      "auc        = 0.8392 ±0.0099\n",
      "run_time    = 0.001328\n",
      "-------------- Training Run 87 of 100 --------------\n",
      "precision = 0.2708 ±0.0238\n",
      "recall    = 0.6994 ±0.0253\n",
      "f1        = 0.3902 ±0.0285\n",
      "auc        = 0.8347 ±0.0131\n",
      "run_time    = 0.001269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- Training Run 88 of 100 --------------\n",
      "precision = 0.2719 ±0.0144\n",
      "recall    = 0.7050 ±0.0122\n",
      "f1        = 0.3923 ±0.0161\n",
      "auc        = 0.8375 ±0.0062\n",
      "run_time    = 0.001314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- Training Run 89 of 100 --------------\n",
      "precision = 0.2693 ±0.0194\n",
      "recall    = 0.7045 ±0.0386\n",
      "f1        = 0.3894 ±0.0245\n",
      "auc        = 0.8372 ±0.0196\n",
      "run_time    = 0.001354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- Training Run 90 of 100 --------------\n",
      "precision = 0.2740 ±0.0235\n",
      "recall    = 0.7103 ±0.0468\n",
      "f1        = 0.3946 ±0.0261\n",
      "auc        = 0.8402 ±0.0235\n",
      "run_time    = 0.001293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- Training Run 91 of 100 --------------\n",
      "precision = 0.2734 ±0.0223\n",
      "recall    = 0.7050 ±0.0441\n",
      "f1        = 0.3939 ±0.0295\n",
      "auc        = 0.8375 ±0.0224\n",
      "run_time    = 0.001340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- Training Run 92 of 100 --------------\n",
      "precision = 0.2703 ±0.0118\n",
      "recall    = 0.7063 ±0.0257\n",
      "f1        = 0.3908 ±0.0143\n",
      "auc        = 0.8381 ±0.0130\n",
      "run_time    = 0.001387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- Training Run 93 of 100 --------------\n",
      "precision = 0.2724 ±0.0162\n",
      "recall    = 0.7116 ±0.0613\n",
      "f1        = 0.3926 ±0.0136\n",
      "auc        = 0.8408 ±0.0305\n",
      "run_time    = 0.001237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- Training Run 94 of 100 --------------\n",
      "precision = 0.2714 ±0.0172\n",
      "recall    = 0.7037 ±0.0205\n",
      "f1        = 0.3912 ±0.0174\n",
      "auc        = 0.8368 ±0.0102\n",
      "run_time    = 0.001667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- Training Run 95 of 100 --------------\n",
      "precision = 0.2713 ±0.0277\n",
      "recall    = 0.7051 ±0.0395\n",
      "f1        = 0.3915 ±0.0333\n",
      "auc        = 0.8375 ±0.0202\n",
      "run_time    = 0.001316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- Training Run 96 of 100 --------------\n",
      "precision = 0.2734 ±0.0174\n",
      "recall    = 0.7043 ±0.0329\n",
      "f1        = 0.3938 ±0.0223\n",
      "auc        = 0.8372 ±0.0167\n",
      "run_time    = 0.001421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- Training Run 97 of 100 --------------\n",
      "precision = 0.2740 ±0.0341\n",
      "recall    = 0.7044 ±0.0202\n",
      "f1        = 0.3936 ±0.0392\n",
      "auc        = 0.8372 ±0.0106\n",
      "run_time    = 0.001258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- Training Run 98 of 100 --------------\n",
      "precision = 0.2745 ±0.0208\n",
      "recall    = 0.7065 ±0.0141\n",
      "f1        = 0.3949 ±0.0216\n",
      "auc        = 0.8383 ±0.0070\n",
      "run_time    = 0.001685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- Training Run 99 of 100 --------------\n",
      "precision = 0.2692 ±0.0134\n",
      "recall    = 0.7020 ±0.0312\n",
      "f1        = 0.3890 ±0.0168\n",
      "auc        = 0.8359 ±0.0157\n",
      "run_time    = 0.001414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- Training Run 100 of 100 --------------\n",
      "precision = 0.2719 ±0.0275\n",
      "recall    = 0.7070 ±0.0562\n",
      "f1        = 0.3916 ±0.0306\n",
      "auc        = 0.8385 ±0.0282\n",
      "run_time    = 0.001299\n",
      "-------------- Average Over All Runs --------------\n",
      "precision = 0.2712 ±0.0014\n",
      "recall    = 0.7049 ±0.0021\n",
      "f1        = 0.3912 ±0.0017\n",
      "auc        = 0.8374 ±0.0011\n",
      "run_time    = 0.001414\n"
     ]
    }
   ],
   "source": [
    "lr_model = LogisticRegression(C=1, max_iter=100, penalty='l1', solver='liblinear')\n",
    "features = twisted_df.drop(['Domain', 'Homograph', 'Registered'], axis=1)\n",
    "model_eval(features[['EMBD_EUC','EMBD_L1', 'EMBD_COS']], twisted_df['Registered'] , lr_model, training_runs=100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- Training Run 1 of 100 --------------\n",
      "precision = 0.6411 ±0.0272\n",
      "recall    = 0.7063 ±0.0283\n",
      "f1        = 0.6713 ±0.0153\n",
      "auc        = 0.8456 ±0.0139\n",
      "run_time    = 0.007884\n",
      "-------------- Training Run 2 of 100 --------------\n",
      "precision = 0.6527 ±0.0254\n",
      "recall    = 0.7087 ±0.0212\n",
      "f1        = 0.6793 ±0.0192\n",
      "auc        = 0.8470 ±0.0108\n",
      "run_time    = 0.007647\n",
      "-------------- Training Run 3 of 100 --------------\n",
      "precision = 0.6553 ±0.0377\n",
      "recall    = 0.7125 ±0.0227\n",
      "f1        = 0.6825 ±0.0293\n",
      "auc        = 0.8490 ±0.0120\n",
      "run_time    = 0.007825\n",
      "-------------- Training Run 4 of 100 --------------\n",
      "precision = 0.6364 ±0.0309\n",
      "recall    = 0.7023 ±0.0124\n",
      "f1        = 0.6672 ±0.0172\n",
      "auc        = 0.8435 ±0.0061\n",
      "run_time    = 0.007622\n",
      "-------------- Training Run 5 of 100 --------------\n",
      "precision = 0.6532 ±0.0105\n",
      "recall    = 0.7176 ±0.0136\n",
      "f1        = 0.6839 ±0.0111\n",
      "auc        = 0.8515 ±0.0070\n",
      "run_time    = 0.008068\n",
      "-------------- Training Run 6 of 100 --------------\n",
      "precision = 0.6532 ±0.0074\n",
      "recall    = 0.7070 ±0.0193\n",
      "f1        = 0.6789 ±0.0102\n",
      "auc        = 0.8462 ±0.0097\n",
      "run_time    = 0.007940\n",
      "-------------- Training Run 7 of 100 --------------\n",
      "precision = 0.6506 ±0.0253\n",
      "recall    = 0.7181 ±0.0271\n",
      "f1        = 0.6824 ±0.0220\n",
      "auc        = 0.8517 ±0.0138\n",
      "run_time    = 0.008192\n",
      "-------------- Training Run 8 of 100 --------------\n",
      "precision = 0.6522 ±0.0131\n",
      "recall    = 0.7208 ±0.0167\n",
      "f1        = 0.6846 ±0.0113\n",
      "auc        = 0.8531 ±0.0084\n",
      "run_time    = 0.007776\n",
      "-------------- Training Run 9 of 100 --------------\n",
      "precision = 0.6459 ±0.0169\n",
      "recall    = 0.7046 ±0.0082\n",
      "f1        = 0.6739 ±0.0113\n",
      "auc        = 0.8448 ±0.0043\n",
      "run_time    = 0.008112\n",
      "-------------- Training Run 10 of 100 --------------\n",
      "precision = 0.6516 ±0.0306\n",
      "recall    = 0.7181 ±0.0166\n",
      "f1        = 0.6826 ±0.0152\n",
      "auc        = 0.8517 ±0.0081\n",
      "run_time    = 0.007758\n",
      "-------------- Training Run 11 of 100 --------------\n",
      "precision = 0.6490 ±0.0335\n",
      "recall    = 0.7075 ±0.0162\n",
      "f1        = 0.6767 ±0.0227\n",
      "auc        = 0.8464 ±0.0085\n",
      "run_time    = 0.007976\n",
      "-------------- Training Run 12 of 100 --------------\n",
      "precision = 0.6553 ±0.0130\n",
      "recall    = 0.7118 ±0.0137\n",
      "f1        = 0.6823 ±0.0113\n",
      "auc        = 0.8486 ±0.0070\n",
      "run_time    = 0.007827\n",
      "-------------- Training Run 13 of 100 --------------\n",
      "precision = 0.6496 ±0.0252\n",
      "recall    = 0.7101 ±0.0115\n",
      "f1        = 0.6781 ±0.0133\n",
      "auc        = 0.8477 ±0.0056\n",
      "run_time    = 0.007641\n",
      "-------------- Training Run 14 of 100 --------------\n",
      "precision = 0.6496 ±0.0105\n",
      "recall    = 0.7186 ±0.0236\n",
      "f1        = 0.6820 ±0.0082\n",
      "auc        = 0.8519 ±0.0116\n",
      "run_time    = 0.008065\n",
      "-------------- Training Run 15 of 100 --------------\n",
      "precision = 0.6506 ±0.0283\n",
      "recall    = 0.7115 ±0.0264\n",
      "f1        = 0.6793 ±0.0230\n",
      "auc        = 0.8484 ±0.0134\n",
      "run_time    = 0.007699\n",
      "-------------- Training Run 16 of 100 --------------\n",
      "precision = 0.6454 ±0.0255\n",
      "recall    = 0.7054 ±0.0206\n",
      "f1        = 0.6737 ±0.0173\n",
      "auc        = 0.8452 ±0.0104\n",
      "run_time    = 0.008059\n",
      "-------------- Training Run 17 of 100 --------------\n",
      "precision = 0.6542 ±0.0343\n",
      "recall    = 0.7210 ±0.0267\n",
      "f1        = 0.6850 ±0.0174\n",
      "auc        = 0.8532 ±0.0131\n",
      "run_time    = 0.007769\n",
      "-------------- Training Run 18 of 100 --------------\n",
      "precision = 0.6454 ±0.0167\n",
      "recall    = 0.7104 ±0.0213\n",
      "f1        = 0.6762 ±0.0168\n",
      "auc        = 0.8477 ±0.0109\n",
      "run_time    = 0.008078\n",
      "-------------- Training Run 19 of 100 --------------\n",
      "precision = 0.6485 ±0.0187\n",
      "recall    = 0.7120 ±0.0328\n",
      "f1        = 0.6779 ±0.0101\n",
      "auc        = 0.8486 ±0.0162\n",
      "run_time    = 0.007805\n",
      "-------------- Training Run 20 of 100 --------------\n",
      "precision = 0.6474 ±0.0244\n",
      "recall    = 0.7040 ±0.0179\n",
      "f1        = 0.6742 ±0.0157\n",
      "auc        = 0.8445 ±0.0090\n",
      "run_time    = 0.008541\n",
      "-------------- Training Run 21 of 100 --------------\n",
      "precision = 0.6522 ±0.0305\n",
      "recall    = 0.7106 ±0.0302\n",
      "f1        = 0.6797 ±0.0247\n",
      "auc        = 0.8480 ±0.0154\n",
      "run_time    = 0.007795\n",
      "-------------- Training Run 22 of 100 --------------\n",
      "precision = 0.6396 ±0.0158\n",
      "recall    = 0.7158 ±0.0250\n",
      "f1        = 0.6752 ±0.0139\n",
      "auc        = 0.8503 ±0.0125\n",
      "run_time    = 0.008004\n",
      "-------------- Training Run 23 of 100 --------------\n",
      "precision = 0.6543 ±0.0220\n",
      "recall    = 0.7116 ±0.0198\n",
      "f1        = 0.6813 ±0.0127\n",
      "auc        = 0.8485 ±0.0098\n",
      "run_time    = 0.008111\n",
      "-------------- Training Run 24 of 100 --------------\n",
      "precision = 0.6537 ±0.0287\n",
      "recall    = 0.7188 ±0.0185\n",
      "f1        = 0.6842 ±0.0147\n",
      "auc        = 0.8521 ±0.0091\n",
      "run_time    = 0.009105\n",
      "-------------- Training Run 25 of 100 --------------\n",
      "precision = 0.6417 ±0.0102\n",
      "recall    = 0.7069 ±0.0335\n",
      "f1        = 0.6721 ±0.0115\n",
      "auc        = 0.8459 ±0.0166\n",
      "run_time    = 0.007756\n",
      "-------------- Training Run 26 of 100 --------------\n",
      "precision = 0.6511 ±0.0143\n",
      "recall    = 0.7104 ±0.0101\n",
      "f1        = 0.6794 ±0.0100\n",
      "auc        = 0.8478 ±0.0051\n",
      "run_time    = 0.008283\n",
      "-------------- Training Run 27 of 100 --------------\n",
      "precision = 0.6480 ±0.0104\n",
      "recall    = 0.7188 ±0.0175\n",
      "f1        = 0.6814 ±0.0093\n",
      "auc        = 0.8520 ±0.0087\n",
      "run_time    = 0.007829\n",
      "-------------- Training Run 28 of 100 --------------\n",
      "precision = 0.6517 ±0.0203\n",
      "recall    = 0.7150 ±0.0192\n",
      "f1        = 0.6814 ±0.0087\n",
      "auc        = 0.8502 ±0.0094\n",
      "run_time    = 0.008008\n",
      "-------------- Training Run 29 of 100 --------------\n",
      "precision = 0.6380 ±0.0135\n",
      "recall    = 0.7032 ±0.0142\n",
      "f1        = 0.6688 ±0.0087\n",
      "auc        = 0.8440 ±0.0070\n",
      "run_time    = 0.008082\n",
      "-------------- Training Run 30 of 100 --------------\n",
      "precision = 0.6527 ±0.0225\n",
      "recall    = 0.7186 ±0.0153\n",
      "f1        = 0.6835 ±0.0057\n",
      "auc        = 0.8520 ±0.0072\n",
      "run_time    = 0.007925\n",
      "-------------- Training Run 31 of 100 --------------\n",
      "precision = 0.6501 ±0.0255\n",
      "recall    = 0.7080 ±0.0114\n",
      "f1        = 0.6776 ±0.0161\n",
      "auc        = 0.8466 ±0.0059\n",
      "run_time    = 0.007912\n",
      "-------------- Training Run 32 of 100 --------------\n",
      "precision = 0.6453 ±0.0361\n",
      "recall    = 0.7042 ±0.0127\n",
      "f1        = 0.6731 ±0.0237\n",
      "auc        = 0.8446 ±0.0069\n",
      "run_time    = 0.008281\n",
      "-------------- Training Run 33 of 100 --------------\n",
      "precision = 0.6433 ±0.0247\n",
      "recall    = 0.7108 ±0.0178\n",
      "f1        = 0.6751 ±0.0189\n",
      "auc        = 0.8479 ±0.0092\n",
      "run_time    = 0.008196\n",
      "-------------- Training Run 34 of 100 --------------\n",
      "precision = 0.6338 ±0.0242\n",
      "recall    = 0.7126 ±0.0185\n",
      "f1        = 0.6706 ±0.0161\n",
      "auc        = 0.8486 ±0.0093\n",
      "run_time    = 0.008550\n",
      "-------------- Training Run 35 of 100 --------------\n",
      "precision = 0.6517 ±0.0131\n",
      "recall    = 0.7148 ±0.0130\n",
      "f1        = 0.6816 ±0.0079\n",
      "auc        = 0.8501 ±0.0064\n",
      "run_time    = 0.007572\n",
      "-------------- Training Run 36 of 100 --------------\n",
      "precision = 0.6428 ±0.0299\n",
      "recall    = 0.7142 ±0.0178\n",
      "f1        = 0.6763 ±0.0217\n",
      "auc        = 0.8496 ±0.0093\n",
      "run_time    = 0.007686\n",
      "-------------- Training Run 37 of 100 --------------\n",
      "precision = 0.6537 ±0.0173\n",
      "recall    = 0.7177 ±0.0299\n",
      "f1        = 0.6838 ±0.0156\n",
      "auc        = 0.8516 ±0.0149\n",
      "run_time    = 0.008084\n",
      "-------------- Training Run 38 of 100 --------------\n",
      "precision = 0.6501 ±0.0151\n",
      "recall    = 0.7140 ±0.0343\n",
      "f1        = 0.6801 ±0.0172\n",
      "auc        = 0.8496 ±0.0172\n",
      "run_time    = 0.007565\n",
      "-------------- Training Run 39 of 100 --------------\n",
      "precision = 0.6475 ±0.0275\n",
      "recall    = 0.7071 ±0.0301\n",
      "f1        = 0.6756 ±0.0239\n",
      "auc        = 0.8461 ±0.0152\n",
      "run_time    = 0.008031\n",
      "-------------- Training Run 40 of 100 --------------\n",
      "precision = 0.6537 ±0.0247\n",
      "recall    = 0.7060 ±0.0266\n",
      "f1        = 0.6786 ±0.0220\n",
      "auc        = 0.8457 ±0.0136\n",
      "run_time    = 0.008795\n",
      "-------------- Training Run 41 of 100 --------------\n",
      "precision = 0.6480 ±0.0220\n",
      "recall    = 0.7188 ±0.0139\n",
      "f1        = 0.6812 ±0.0122\n",
      "auc        = 0.8520 ±0.0069\n",
      "run_time    = 0.008171\n",
      "-------------- Training Run 42 of 100 --------------\n",
      "precision = 0.6490 ±0.0195\n",
      "recall    = 0.7143 ±0.0219\n",
      "f1        = 0.6796 ±0.0100\n",
      "auc        = 0.8498 ±0.0107\n",
      "run_time    = 0.007890\n",
      "-------------- Training Run 43 of 100 --------------\n",
      "precision = 0.6412 ±0.0220\n",
      "recall    = 0.7097 ±0.0134\n",
      "f1        = 0.6735 ±0.0148\n",
      "auc        = 0.8473 ±0.0068\n",
      "run_time    = 0.007917\n",
      "-------------- Training Run 44 of 100 --------------\n",
      "precision = 0.6480 ±0.0188\n",
      "recall    = 0.7105 ±0.0169\n",
      "f1        = 0.6776 ±0.0126\n",
      "auc        = 0.8478 ±0.0085\n",
      "run_time    = 0.007922\n",
      "-------------- Training Run 45 of 100 --------------\n",
      "precision = 0.6401 ±0.0317\n",
      "recall    = 0.7099 ±0.0138\n",
      "f1        = 0.6729 ±0.0218\n",
      "auc        = 0.8474 ±0.0073\n",
      "run_time    = 0.008231\n",
      "-------------- Training Run 46 of 100 --------------\n",
      "precision = 0.6496 ±0.0234\n",
      "recall    = 0.7135 ±0.0162\n",
      "f1        = 0.6799 ±0.0181\n",
      "auc        = 0.8494 ±0.0084\n",
      "run_time    = 0.008021\n",
      "-------------- Training Run 47 of 100 --------------\n",
      "precision = 0.6585 ±0.0325\n",
      "recall    = 0.7117 ±0.0104\n",
      "f1        = 0.6837 ±0.0199\n",
      "auc        = 0.8486 ±0.0055\n",
      "run_time    = 0.008178\n",
      "-------------- Training Run 48 of 100 --------------\n",
      "precision = 0.6380 ±0.0141\n",
      "recall    = 0.7163 ±0.0091\n",
      "f1        = 0.6747 ±0.0055\n",
      "auc        = 0.8505 ±0.0044\n",
      "run_time    = 0.008319\n",
      "-------------- Training Run 49 of 100 --------------\n",
      "precision = 0.6469 ±0.0098\n",
      "recall    = 0.7054 ±0.0164\n",
      "f1        = 0.6747 ±0.0077\n",
      "auc        = 0.8452 ±0.0082\n",
      "run_time    = 0.008032\n",
      "-------------- Training Run 50 of 100 --------------\n",
      "precision = 0.6480 ±0.0087\n",
      "recall    = 0.7074 ±0.0107\n",
      "f1        = 0.6763 ±0.0065\n",
      "auc        = 0.8463 ±0.0053\n",
      "run_time    = 0.008161\n",
      "-------------- Training Run 51 of 100 --------------\n",
      "precision = 0.6454 ±0.0218\n",
      "recall    = 0.7066 ±0.0184\n",
      "f1        = 0.6740 ±0.0072\n",
      "auc        = 0.8458 ±0.0089\n",
      "run_time    = 0.008063\n",
      "-------------- Training Run 52 of 100 --------------\n",
      "precision = 0.6475 ±0.0087\n",
      "recall    = 0.7098 ±0.0133\n",
      "f1        = 0.6771 ±0.0083\n",
      "auc        = 0.8474 ±0.0067\n",
      "run_time    = 0.008017\n",
      "-------------- Training Run 53 of 100 --------------\n",
      "precision = 0.6417 ±0.0245\n",
      "recall    = 0.7115 ±0.0246\n",
      "f1        = 0.6745 ±0.0197\n",
      "auc        = 0.8482 ±0.0125\n",
      "run_time    = 0.007967\n",
      "-------------- Training Run 54 of 100 --------------\n",
      "precision = 0.6386 ±0.0336\n",
      "recall    = 0.6974 ±0.0261\n",
      "f1        = 0.6664 ±0.0276\n",
      "auc        = 0.8411 ±0.0135\n",
      "run_time    = 0.008208\n",
      "-------------- Training Run 55 of 100 --------------\n",
      "precision = 0.6401 ±0.0143\n",
      "recall    = 0.7090 ±0.0189\n",
      "f1        = 0.6726 ±0.0115\n",
      "auc        = 0.8469 ±0.0095\n",
      "run_time    = 0.007978\n",
      "-------------- Training Run 56 of 100 --------------\n",
      "precision = 0.6475 ±0.0223\n",
      "recall    = 0.7152 ±0.0245\n",
      "f1        = 0.6789 ±0.0056\n",
      "auc        = 0.8502 ±0.0118\n",
      "run_time    = 0.008107\n",
      "-------------- Training Run 57 of 100 --------------\n",
      "precision = 0.6454 ±0.0164\n",
      "recall    = 0.7166 ±0.0172\n",
      "f1        = 0.6790 ±0.0138\n",
      "auc        = 0.8508 ±0.0087\n",
      "run_time    = 0.008079\n",
      "-------------- Training Run 58 of 100 --------------\n",
      "precision = 0.6464 ±0.0192\n",
      "recall    = 0.7277 ±0.0200\n",
      "f1        = 0.6844 ±0.0145\n",
      "auc        = 0.8564 ±0.0101\n",
      "run_time    = 0.007627\n",
      "-------------- Training Run 59 of 100 --------------\n",
      "precision = 0.6501 ±0.0104\n",
      "recall    = 0.7103 ±0.0186\n",
      "f1        = 0.6786 ±0.0046\n",
      "auc        = 0.8478 ±0.0091\n",
      "run_time    = 0.007949\n",
      "-------------- Training Run 60 of 100 --------------\n",
      "precision = 0.6522 ±0.0192\n",
      "recall    = 0.7115 ±0.0298\n",
      "f1        = 0.6803 ±0.0198\n",
      "auc        = 0.8484 ±0.0150\n",
      "run_time    = 0.008162\n",
      "-------------- Training Run 61 of 100 --------------\n",
      "precision = 0.6396 ±0.0213\n",
      "recall    = 0.7151 ±0.0073\n",
      "f1        = 0.6750 ±0.0109\n",
      "auc        = 0.8499 ±0.0035\n",
      "run_time    = 0.007976\n",
      "-------------- Training Run 62 of 100 --------------\n",
      "precision = 0.6469 ±0.0270\n",
      "recall    = 0.7138 ±0.0190\n",
      "f1        = 0.6784 ±0.0182\n",
      "auc        = 0.8495 ±0.0096\n",
      "run_time    = 0.007900\n",
      "-------------- Training Run 63 of 100 --------------\n",
      "precision = 0.6464 ±0.0281\n",
      "recall    = 0.7118 ±0.0277\n",
      "f1        = 0.6766 ±0.0148\n",
      "auc        = 0.8484 ±0.0136\n",
      "run_time    = 0.008161\n",
      "-------------- Training Run 64 of 100 --------------\n",
      "precision = 0.6438 ±0.0173\n",
      "recall    = 0.7139 ±0.0226\n",
      "f1        = 0.6768 ±0.0156\n",
      "auc        = 0.8494 ±0.0114\n",
      "run_time    = 0.007996\n",
      "-------------- Training Run 65 of 100 --------------\n",
      "precision = 0.6480 ±0.0268\n",
      "recall    = 0.7133 ±0.0182\n",
      "f1        = 0.6789 ±0.0216\n",
      "auc        = 0.8492 ±0.0095\n",
      "run_time    = 0.008148\n",
      "-------------- Training Run 66 of 100 --------------\n",
      "precision = 0.6532 ±0.0107\n",
      "recall    = 0.7100 ±0.0103\n",
      "f1        = 0.6803 ±0.0052\n",
      "auc        = 0.8477 ±0.0050\n",
      "run_time    = 0.007893\n",
      "-------------- Training Run 67 of 100 --------------\n",
      "precision = 0.6391 ±0.0251\n",
      "recall    = 0.7100 ±0.0141\n",
      "f1        = 0.6724 ±0.0150\n",
      "auc        = 0.8474 ±0.0071\n",
      "run_time    = 0.008097\n",
      "-------------- Training Run 68 of 100 --------------\n",
      "precision = 0.6574 ±0.0217\n",
      "recall    = 0.7243 ±0.0103\n",
      "f1        = 0.6890 ±0.0134\n",
      "auc        = 0.8549 ±0.0052\n",
      "run_time    = 0.007692\n",
      "-------------- Training Run 69 of 100 --------------\n",
      "precision = 0.6543 ±0.0235\n",
      "recall    = 0.7102 ±0.0251\n",
      "f1        = 0.6808 ±0.0204\n",
      "auc        = 0.8478 ±0.0127\n",
      "run_time    = 0.008112\n",
      "-------------- Training Run 70 of 100 --------------\n",
      "precision = 0.6433 ±0.0329\n",
      "recall    = 0.7074 ±0.0244\n",
      "f1        = 0.6732 ±0.0205\n",
      "auc        = 0.8462 ±0.0122\n",
      "run_time    = 0.007825\n",
      "-------------- Training Run 71 of 100 --------------\n",
      "precision = 0.6579 ±0.0225\n",
      "recall    = 0.7180 ±0.0286\n",
      "f1        = 0.6866 ±0.0246\n",
      "auc        = 0.8518 ±0.0147\n",
      "run_time    = 0.007739\n",
      "-------------- Training Run 72 of 100 --------------\n",
      "precision = 0.6464 ±0.0367\n",
      "recall    = 0.7153 ±0.0126\n",
      "f1        = 0.6785 ±0.0199\n",
      "auc        = 0.8502 ±0.0063\n",
      "run_time    = 0.007946\n",
      "-------------- Training Run 73 of 100 --------------\n",
      "precision = 0.6433 ±0.0181\n",
      "recall    = 0.7193 ±0.0322\n",
      "f1        = 0.6788 ±0.0196\n",
      "auc        = 0.8522 ±0.0162\n",
      "run_time    = 0.007939\n",
      "-------------- Training Run 74 of 100 --------------\n",
      "precision = 0.6496 ±0.0236\n",
      "recall    = 0.7106 ±0.0167\n",
      "f1        = 0.6784 ±0.0149\n",
      "auc        = 0.8479 ±0.0084\n",
      "run_time    = 0.008646\n",
      "-------------- Training Run 75 of 100 --------------\n",
      "precision = 0.6496 ±0.0165\n",
      "recall    = 0.7191 ±0.0215\n",
      "f1        = 0.6825 ±0.0175\n",
      "auc        = 0.8522 ±0.0110\n",
      "run_time    = 0.007181\n",
      "-------------- Training Run 76 of 100 --------------\n",
      "precision = 0.6705 ±0.0139\n",
      "recall    = 0.7149 ±0.0169\n",
      "f1        = 0.6917 ±0.0066\n",
      "auc        = 0.8505 ±0.0083\n",
      "run_time    = 0.007372\n",
      "-------------- Training Run 77 of 100 --------------\n",
      "precision = 0.6485 ±0.0142\n",
      "recall    = 0.7187 ±0.0158\n",
      "f1        = 0.6817 ±0.0133\n",
      "auc        = 0.8519 ±0.0081\n",
      "run_time    = 0.007417\n",
      "-------------- Training Run 78 of 100 --------------\n",
      "precision = 0.6606 ±0.0239\n",
      "recall    = 0.7140 ±0.0134\n",
      "f1        = 0.6861 ±0.0173\n",
      "auc        = 0.8499 ±0.0070\n",
      "run_time    = 0.007998\n",
      "-------------- Training Run 79 of 100 --------------\n",
      "precision = 0.6417 ±0.0183\n",
      "recall    = 0.7197 ±0.0148\n",
      "f1        = 0.6782 ±0.0117\n",
      "auc        = 0.8523 ±0.0074\n",
      "run_time    = 0.007249\n",
      "-------------- Training Run 80 of 100 --------------\n",
      "precision = 0.6495 ±0.0309\n",
      "recall    = 0.7082 ±0.0088\n",
      "f1        = 0.6772 ±0.0180\n",
      "auc        = 0.8467 ±0.0046\n",
      "run_time    = 0.009904\n",
      "-------------- Training Run 81 of 100 --------------\n",
      "precision = 0.6433 ±0.0378\n",
      "recall    = 0.7093 ±0.0242\n",
      "f1        = 0.6739 ±0.0245\n",
      "auc        = 0.8471 ±0.0122\n",
      "run_time    = 0.009143\n",
      "-------------- Training Run 82 of 100 --------------\n",
      "precision = 0.6412 ±0.0180\n",
      "recall    = 0.7130 ±0.0342\n",
      "f1        = 0.6746 ±0.0178\n",
      "auc        = 0.8489 ±0.0171\n",
      "run_time    = 0.008823\n",
      "-------------- Training Run 83 of 100 --------------\n",
      "precision = 0.6391 ±0.0193\n",
      "recall    = 0.7108 ±0.0173\n",
      "f1        = 0.6729 ±0.0157\n",
      "auc        = 0.8478 ±0.0088\n",
      "run_time    = 0.009443\n",
      "-------------- Training Run 84 of 100 --------------\n",
      "precision = 0.6522 ±0.0253\n",
      "recall    = 0.7108 ±0.0256\n",
      "f1        = 0.6799 ±0.0207\n",
      "auc        = 0.8481 ±0.0130\n",
      "run_time    = 0.009341\n",
      "-------------- Training Run 85 of 100 --------------\n",
      "precision = 0.6496 ±0.0273\n",
      "recall    = 0.7082 ±0.0105\n",
      "f1        = 0.6773 ±0.0172\n",
      "auc        = 0.8467 ±0.0055\n",
      "run_time    = 0.009354\n",
      "-------------- Training Run 86 of 100 --------------\n",
      "precision = 0.6485 ±0.0280\n",
      "recall    = 0.7107 ±0.0042\n",
      "f1        = 0.6779 ±0.0152\n",
      "auc        = 0.8480 ±0.0020\n",
      "run_time    = 0.009005\n",
      "-------------- Training Run 87 of 100 --------------\n",
      "precision = 0.6459 ±0.0271\n",
      "recall    = 0.7228 ±0.0182\n",
      "f1        = 0.6818 ±0.0168\n",
      "auc        = 0.8539 ±0.0091\n",
      "run_time    = 0.008082\n",
      "-------------- Training Run 88 of 100 --------------\n",
      "precision = 0.6527 ±0.0209\n",
      "recall    = 0.7154 ±0.0187\n",
      "f1        = 0.6825 ±0.0179\n",
      "auc        = 0.8504 ±0.0096\n",
      "run_time    = 0.008272\n",
      "-------------- Training Run 89 of 100 --------------\n",
      "precision = 0.6422 ±0.0295\n",
      "recall    = 0.7098 ±0.0116\n",
      "f1        = 0.6740 ±0.0198\n",
      "auc        = 0.8474 ±0.0062\n",
      "run_time    = 0.009005\n",
      "-------------- Training Run 90 of 100 --------------\n",
      "precision = 0.6491 ±0.0214\n",
      "recall    = 0.7215 ±0.0162\n",
      "f1        = 0.6833 ±0.0186\n",
      "auc        = 0.8534 ±0.0085\n",
      "run_time    = 0.008249\n",
      "-------------- Training Run 91 of 100 --------------\n",
      "precision = 0.6590 ±0.0144\n",
      "recall    = 0.7109 ±0.0157\n",
      "f1        = 0.6839 ±0.0134\n",
      "auc        = 0.8482 ±0.0080\n",
      "run_time    = 0.008643\n",
      "-------------- Training Run 92 of 100 --------------\n",
      "precision = 0.6506 ±0.0229\n",
      "recall    = 0.7143 ±0.0090\n",
      "f1        = 0.6807 ±0.0139\n",
      "auc        = 0.8498 ±0.0046\n",
      "run_time    = 0.008298\n",
      "-------------- Training Run 93 of 100 --------------\n",
      "precision = 0.6496 ±0.0101\n",
      "recall    = 0.7161 ±0.0296\n",
      "f1        = 0.6810 ±0.0177\n",
      "auc        = 0.8507 ±0.0150\n",
      "run_time    = 0.008020\n",
      "-------------- Training Run 94 of 100 --------------\n",
      "precision = 0.6464 ±0.0369\n",
      "recall    = 0.7130 ±0.0174\n",
      "f1        = 0.6778 ±0.0265\n",
      "auc        = 0.8490 ±0.0093\n",
      "run_time    = 0.007987\n",
      "-------------- Training Run 95 of 100 --------------\n",
      "precision = 0.6427 ±0.0162\n",
      "recall    = 0.7176 ±0.0357\n",
      "f1        = 0.6776 ±0.0182\n",
      "auc        = 0.8513 ±0.0179\n",
      "run_time    = 0.007965\n",
      "-------------- Training Run 96 of 100 --------------\n",
      "precision = 0.6375 ±0.0295\n",
      "recall    = 0.7129 ±0.0300\n",
      "f1        = 0.6725 ±0.0203\n",
      "auc        = 0.8488 ±0.0150\n",
      "run_time    = 0.007942\n",
      "-------------- Training Run 97 of 100 --------------\n",
      "precision = 0.6427 ±0.0202\n",
      "recall    = 0.7136 ±0.0129\n",
      "f1        = 0.6761 ±0.0125\n",
      "auc        = 0.8493 ±0.0065\n",
      "run_time    = 0.007684\n",
      "-------------- Training Run 98 of 100 --------------\n",
      "precision = 0.6542 ±0.0315\n",
      "recall    = 0.7175 ±0.0167\n",
      "f1        = 0.6842 ±0.0240\n",
      "auc        = 0.8515 ±0.0089\n",
      "run_time    = 0.008158\n",
      "-------------- Training Run 99 of 100 --------------\n",
      "precision = 0.6569 ±0.0244\n",
      "recall    = 0.7167 ±0.0260\n",
      "f1        = 0.6848 ±0.0133\n",
      "auc        = 0.8511 ±0.0128\n",
      "run_time    = 0.007877\n",
      "-------------- Training Run 100 of 100 --------------\n",
      "precision = 0.6475 ±0.0321\n",
      "recall    = 0.7041 ±0.0184\n",
      "f1        = 0.6742 ±0.0215\n",
      "auc        = 0.8446 ±0.0094\n",
      "run_time    = 0.007581\n",
      "-------------- Average Over All Runs --------------\n",
      "precision = 0.6480 ±0.0060\n",
      "recall    = 0.7126 ±0.0052\n",
      "f1        = 0.6784 ±0.0045\n",
      "auc        = 0.8489 ±0.0026\n",
      "run_time    = 0.008089\n"
     ]
    }
   ],
   "source": [
    "xgb_model = GradientBoostingClassifier(learning_rate=0.1, max_depth=6, n_estimators=50)\n",
    "features = twisted_df.drop(['Domain', 'Homograph', 'Registered'], axis=1)\n",
    "model_eval(features[['MSE', 'SSM','EMBD_EUC']], twisted_df['Registered'] , xgb_model, training_runs=100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svm_model = SVC()\n",
    "# features = twisted_df.drop(['Domain', 'Homograph', 'Registered'], axis=1)\n",
    "# model_eval(features[['EMBD_EUC','EMBD_L1', 'EMBD_COS']], twisted_df['Registered'] , svm_model);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_fp, rf_tp, rf_auc = model_eval(features[['MSE', 'SSM','EMBD_EUC']], twisted_df['Registered'] , rf_model, curves=True);   \n",
    "mlp_fp, mlp_tp, mlp_auc = model_eval(features[['MSE', 'SSM','EMBD_EUC']], twisted_df['Registered'] , mlp_model, curves=True);   \n",
    "lr_fp, lr_tp, lr_auc = model_eval(features[['MSE', 'SSM','EMBD_EUC']], twisted_df['Registered'] , lr_model, curves=True);  \n",
    "xgb_fp, xgb_tp, xgb_auc = model_eval(features[['MSE', 'SSM','EMBD_EUC']], twisted_df['Registered'] , xgb_model, curves=True);  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(rf_fp, rf_tp, label='Random Forest (AUC = %0.2f)' % rf_auc)\n",
    "plt.plot(mlp_fp, mlp_tp, label='MLP (AUC = %0.2f)' % mlp_auc)\n",
    "plt.plot(lr_fp, lr_tp, label='Logistic Regression (AUC = %0.2f)' % lr_auc)\n",
    "plt.plot(xgb_fp, xgb_tp, label='XGBoost (AUC = %0.2f)' % xgb_auc)\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Classifier Experimentation')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision = 0.6281 ±0.0206\n",
      "recall    = 0.7365 ±0.0095\n",
      "f1        = 0.6778 ±0.0142\n",
      "auc        = 0.8604 ±0.0050\n",
      "[0.02, 0.03, 0.09, 0.29, 0.01, 0.02, 0.01, 0.0, 0.14, 0.9, 0.0, 0.99, 0.97, 0.86, 0.92, 0.95, 0.99, 0.05, 1.0, 0.85, 0.0, 1.0, 1.0, 0.55, 1.0, 0.98, 1.0, 0.0, 0.34, 0.0, 0.03, 0.17, 0.1, 0.08, 0.18, 0.05, 0.06, 0.35, 0.45, 0.1, 0.1, 0.05, 0.1, 0.54, 0.54, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99, 1.0, 1.0, 1.0, 1.0, 0.98, 1.0, 0.98, 1.0, 0.73, 0.97, 0.82, 1.0, 0.93, 0.99, 1.0, 1.0, 0.97, 1.0, 1.0, 1.0, 1.0, 1.0, 0.93, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.95, 1.0, 0.98, 1.0, 1.0, 1.0, 1.0, 0.88, 0.99, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.82, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.93, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 0.91, 0.84, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96, 1.0, 1.0, 1.0, 0.98, 1.0, 1.0, 0.92, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94, 1.0, 1.0, 0.94, 0.97, 1.0, 1.0, 1.0, 1.0, 0.96, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 0.94, 0.88, 0.97, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.86, 1.0, 0.99, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.98, 1.0, 1.0, 0.96, 1.0, 1.0, 0.98, 0.94, 0.97, 0.98, 0.99, 1.0, 0.97, 1.0, 0.94, 1.0, 0.94, 0.99, 0.74, 1.0, 1.0, 0.23, 0.98, 1.0, 1.0, 1.0, 1.0, 0.99, 1.0, 1.0, 1.0, 1.0, 0.99, 0.98, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94, 1.0, 1.0, 0.99, 1.0, 1.0, 1.0, 1.0, 0.95, 0.59, 0.98, 0.97, 0.88, 0.94, 0.96, 1.0, 1.0, 1.0, 0.99, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.98, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99, 1.0, 0.99, 1.0, 0.98, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.98, 0.85, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.98, 1.0, 1.0, 0.99, 0.74, 1.0, 1.0, 1.0, 1.0, 1.0, 0.91, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.78, 1.0, 1.0, 1.0, 1.0, 0.98, 1.0, 1.0, 1.0, 1.0, 0.86, 0.59, 0.9, 1.0, 0.74, 1.0, 1.0, 0.98, 0.88, 1.0, 1.0, 1.0, 1.0, 0.99, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94, 1.0, 1.0, 1.0, 1.0, 0.91, 1.0, 0.7, 1.0, 0.86, 0.96, 1.0, 0.99, 1.0, 0.95, 0.98, 0.99, 1.0, 1.0, 1.0, 1.0, 0.98, 0.91, 1.0, 1.0, 0.99, 1.0, 0.97, 0.99, 1.0, 0.86, 0.67, 0.99, 1.0, 1.0, 1.0, 0.91, 1.0, 1.0, 0.97, 0.94, 1.0, 0.91, 0.93, 0.98, 0.02, 0.11, 1.0, 0.45, 0.13, 0.99, 0.14, 0.18, 0.97, 0.95, 0.19, 0.43, 0.43, 0.09, 0.0, 0.57, 0.01, 0.0, 0.27, 0.67, 0.09, 0.01, 1.0, 1.0, 0.65, 0.28, 0.97, 1.0, 0.0, 1.0, 0.39, 0.3, 0.67, 0.69, 1.0, 0.78, 0.36, 0.62, 0.61, 0.09, 0.61, 0.94, 0.62, 0.12, 0.91, 0.13, 0.08, 0.07, 0.0, 0.52, 0.31, 0.41, 0.23, 0.06, 0.23, 0.44, 0.64, 0.62, 0.84, 0.47, 0.68, 0.54, 0.59, 0.42, 0.89, 0.68, 0.39, 0.14, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.98, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.98, 1.0, 0.96, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96, 1.0, 1.0, 1.0, 0.99, 1.0, 1.0, 0.99, 0.99, 1.0, 1.0, 1.0, 0.96, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.86, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.91, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99, 1.0, 0.97, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.97, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.78, 0.99, 1.0, 0.83, 1.0, 1.0, 1.0, 1.0, 1.0, 0.92, 1.0, 1.0, 1.0, 1.0, 0.99, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.98, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94, 1.0, 1.0, 0.75, 1.0, 1.0, 0.82, 1.0, 1.0, 0.97, 1.0, 1.0, 1.0, 1.0, 0.96, 1.0, 0.99, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94, 1.0, 0.98, 1.0, 1.0, 0.98, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.93, 1.0, 1.0, 0.99, 1.0, 1.0, 0.99, 1.0, 1.0, 0.98, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99, 1.0, 0.96, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.81, 1.0, 1.0, 0.98, 0.89, 1.0, 1.0, 0.99, 1.0, 1.0, 0.99, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.98, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99, 1.0, 1.0, 1.0, 0.98, 0.99, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.86, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.97, 1.0, 0.99, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99, 0.13, 0.99, 0.0, 0.35, 0.99, 0.61, 0.88, 0.6, 0.71, 0.97, 0.84, 0.58, 0.92, 0.71, 0.7, 0.57, 0.33, 0.72, 0.66, 0.37, 0.59, 0.87, 0.53, 0.38, 0.52, 0.7, 0.24, 0.24, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.92, 0.98, 1.0, 1.0, 1.0, 0.99, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99, 1.0, 1.0, 1.0, 1.0, 0.97, 1.0, 0.98, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.98, 0.98, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.97, 1.0, 1.0, 0.95, 1.0, 0.92, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.98, 1.0, 0.95, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.87, 0.99, 1.0, 1.0, 0.97, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96, 1.0, 1.0, 1.0, 1.0, 0.99, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.98, 1.0, 1.0, 0.95, 1.0, 1.0, 1.0, 1.0, 0.99, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99, 1.0, 1.0, 1.0, 1.0, 0.98, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.98, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.93, 1.0, 1.0, 1.0, 0.97, 0.98, 1.0, 0.81, 1.0, 1.0, 1.0, 0.91, 0.99, 1.0, 0.99, 1.0, 0.99, 1.0, 0.94, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94, 1.0, 1.0, 1.0, 0.94, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.93, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.91, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.97, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.84, 1.0, 0.99, 1.0, 1.0, 1.0, 0.99, 1.0, 0.97, 1.0, 1.0, 1.0, 1.0, 0.73, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.98, 0.98, 1.0, 1.0, 0.85, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.86, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99, 0.98, 1.0, 0.97, 0.98, 1.0, 1.0, 1.0, 1.0, 0.93, 0.97, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.73, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.93, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.98, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96, 1.0, 1.0, 0.91, 0.98, 0.98, 0.92, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99, 1.0, 0.99, 1.0, 0.99, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.87, 0.89, 1.0, 1.0, 0.87, 0.94, 1.0, 1.0, 1.0, 0.82, 1.0, 0.51, 1.0, 0.97, 1.0, 1.0, 1.0, 1.0, 0.97, 1.0, 0.98, 0.96, 1.0, 0.88, 0.87, 1.0, 0.88, 0.93, 0.92, 1.0, 1.0, 0.96, 0.95, 0.88, 1.0, 1.0, 1.0, 0.99, 0.64, 1.0, 0.96, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96, 0.77, 0.99, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99, 1.0, 1.0, 0.83, 0.99, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99, 1.0, 1.0, 0.94, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.98, 0.88, 0.3, 0.86, 0.5, 0.88, 0.91, 0.96, 0.01, 0.65, 0.99, 0.56, 0.02, 0.16, 0.7, 0.7, 0.5, 0.15, 0.14, 0.95, 0.71, 0.14, 0.14, 0.05, 0.28, 0.31, 0.07, 0.04, 0.02, 0.66, 0.07, 0.14, 0.08, 0.01, 0.15, 0.13, 0.97, 0.05, 0.03, 1.0, 0.87, 0.97, 1.0, 0.99, 0.97, 1.0, 0.01, 1.0, 1.0, 0.04, 0.8, 0.74, 0.72, 0.0, 0.12, 0.15, 0.27, 0.04, 0.6, 0.1, 0.3, 0.57, 0.68, 0.1, 0.18, 0.88, 0.04, 0.49, 0.0, 0.2, 0.56, 0.53, 0.27, 0.15, 0.0, 0.46, 0.31, 0.3, 0.02, 0.33, 0.07, 0.16, 0.0, 0.11, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.98, 1.0, 1.0, 1.0, 1.0, 0.96, 0.99, 1.0, 0.97, 1.0, 0.98, 1.0, 1.0, 1.0, 1.0, 0.98, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.97, 0.99, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96, 0.98, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.91, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.97, 1.0, 0.98, 1.0, 1.0, 0.96, 1.0, 1.0, 1.0, 0.97, 0.94, 1.0, 0.99, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.93, 1.0, 0.95, 1.0, 0.86, 0.99, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96, 1.0, 1.0, 0.98, 1.0, 0.86, 0.99, 1.0, 1.0, 1.0, 0.99, 0.98, 0.97, 1.0, 0.9, 1.0, 0.95, 1.0, 0.16, 1.0, 1.0, 0.92, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.98, 1.0, 1.0, 0.98, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.46, 0.99, 1.0, 1.0, 1.0, 1.0, 0.89, 0.88, 0.99, 1.0, 0.93, 1.0, 1.0, 1.0, 1.0, 1.0, 0.74, 1.0, 0.98, 0.98, 0.98, 0.95, 1.0, 1.0, 0.94, 1.0, 1.0, 1.0, 1.0, 0.97, 0.2, 1.0, 1.0, 1.0, 1.0, 0.78, 0.95, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99, 1.0, 1.0, 1.0, 0.93, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.77, 0.98, 1.0, 1.0, 1.0, 1.0, 0.78, 0.71, 1.0, 0.96, 0.98, 0.73, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.98, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.91, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.93, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.88, 0.52, 0.92, 0.96, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.98, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.98, 1.0, 1.0, 0.98, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.98, 1.0, 0.99, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94, 1.0, 0.63, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.91, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 1.0, 1.0, 1.0, 0.97, 1.0, 1.0, 1.0, 1.0, 1.0, 0.88, 1.0, 1.0, 0.78, 0.99, 1.0, 0.58, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.97, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.71, 0.91, 1.0, 1.0, 1.0, 1.0, 0.99, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.91, 1.0, 0.8, 1.0, 1.0, 1.0, 0.96, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.97, 1.0, 1.0, 0.99, 1.0, 1.0, 1.0, 0.91, 1.0, 1.0, 1.0, 1.0, 1.0, 0.92, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99, 1.0, 1.0, 1.0, 0.99, 1.0, 1.0, 1.0, 1.0, 0.95, 1.0, 1.0, 0.89, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.86, 0.62, 0.77, 1.0, 1.0, 0.63, 0.97, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.98, 1.0, 0.83, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.95, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.91, 0.99, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99, 0.98, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99, 1.0, 1.0, 0.7, 0.01, 0.25, 0.66, 1.0, 0.78, 0.78, 0.7, 0.38, 0.25, 1.0, 0.1, 0.04, 0.08, 0.47, 0.05, 0.8, 0.66, 0.19, 0.96, 0.82, 0.91, 0.98, 0.79, 1.0, 0.79, 1.0, 1.0, 1.0, 0.35, 0.96, 0.66, 0.71, 0.07, 0.05, 0.52, 0.74, 0.95, 0.21, 0.71, 1.0, 0.87, 0.32, 0.13, 0.98, 0.15, 0.96, 1.0, 0.2, 1.0, 0.58, 0.85, 0.77, 0.26, 0.64, 0.28, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.98, 1.0, 1.0, 1.0, 1.0, 1.0, 0.83, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.91, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.82, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.97, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.97, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99, 1.0, 1.0, 1.0, 1.0, 1.0, 0.97, 0.99, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.97, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.92, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.88, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.97, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.98, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.95, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.98, 1.0, 0.97, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.93, 0.99, 1.0, 1.0, 0.97, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.98, 1.0, 1.0, 1.0, 1.0, 0.99, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.91, 1.0, 1.0, 1.0, 1.0, 0.95, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.97, 1.0, 1.0, 0.94, 0.98, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.98, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.98, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.92, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.97, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.92, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.84, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99, 1.0, 1.0, 1.0, 0.97, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94, 0.69, 0.98, 1.0, 1.0, 1.0, 1.0, 1.0, 0.87, 1.0, 0.94, 1.0, 1.0, 1.0, 1.0, 0.95, 0.99, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.26, 0.93, 0.99, 1.0, 0.79, 0.46, 0.93, 0.74, 0.17, 0.16, 0.51, 0.16, 0.02, 0.25, 0.16, 0.5, 0.27, 0.11, 0.32, 0.89, 0.23, 1.0, 1.0, 0.94, 1.0, 0.98, 0.71, 0.65, 0.01, 0.12, 0.01, 0.07, 0.11, 0.03, 0.69, 0.38, 0.04, 0.24, 0.66, 0.57, 0.62, 0.42, 0.05, 0.16, 0.2, 0.15, 0.41, 0.74, 0.0, 0.58, 0.01, 0.76, 0.0, 0.11, 0.62, 0.69, 0.26, 0.37, 0.36, 0.89, 0.31, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.95, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.71, 1.0, 0.99, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.98, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99, 1.0, 0.96, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.97, 1.0, 1.0, 0.94, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99, 0.97, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96, 1.0, 0.99, 1.0, 1.0, 0.98, 1.0, 1.0, 1.0, 0.97, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.95, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.93, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.98, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.84, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.93, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.76, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.91, 1.0, 1.0, 1.0, 1.0, 1.0, 0.87, 0.89, 0.91, 1.0, 1.0, 0.99, 1.0, 1.0, 1.0, 1.0, 1.0, 0.98, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99, 1.0, 1.0, 1.0, 1.0, 0.99, 0.94, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.98, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.87, 1.0, 0.97, 1.0, 1.0, 0.99, 1.0, 0.72, 0.98, 0.96, 1.0, 1.0, 0.61, 1.0, 0.99, 0.99, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.97, 1.0, 1.0, 1.0, 1.0, 0.99, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99, 0.66, 1.0, 1.0, 1.0, 0.98, 1.0, 1.0, 1.0, 0.88, 1.0, 1.0, 1.0, 0.96, 1.0, 0.97, 1.0, 1.0, 0.99, 1.0, 1.0, 0.99, 0.99, 0.91, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.85, 1.0, 1.0, 1.0, 0.92, 1.0, 0.97, 1.0, 1.0, 0.99, 1.0, 0.94, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.97, 1.0, 1.0, 0.65, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 0.1, 0.15, 0.88, 0.1, 0.78, 0.09, 0.35, 0.2, 0.03, 0.22, 0.19, 0.11, 0.03, 0.06, 0.03, 0.01, 0.12, 0.31, 0.35, 0.57, 0.02, 0.09, 0.05, 1.0, 0.96, 0.0, 0.12, 0.18, 0.13, 0.38, 0.03, 0.0, 0.0, 0.16, 0.29, 0.18, 0.21, 0.11, 0.37, 0.31, 0.13, 0.18, 0.78, 0.01, 0.06, 0.59, 0.01, 0.12, 0.54, 0.03, 0.17, 0.75, 0.13, 0.45, 0.02, 0.19, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.95, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96, 1.0, 1.0, 1.0, 0.85, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.89, 1.0, 0.96, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.95, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7, 0.96, 0.94, 1.0, 0.99, 1.0, 1.0, 1.0, 0.99, 1.0, 1.0, 1.0, 0.96, 1.0, 1.0, 0.98, 1.0, 0.98, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.83, 1.0, 0.96, 1.0, 1.0, 0.89, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.81, 0.96, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99, 0.99, 0.9, 0.78, 1.0, 0.94, 0.89, 1.0, 1.0, 1.0, 0.95, 1.0, 1.0, 0.89, 1.0, 1.0, 1.0, 0.83, 0.44, 0.96, 1.0, 0.96, 0.91, 1.0, 0.98, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99, 1.0, 1.0, 0.99, 0.98, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99, 1.0, 1.0, 0.99, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99, 1.0, 1.0, 0.9, 0.99, 0.97, 1.0, 1.0, 1.0, 1.0, 1.0, 0.93, 1.0, 0.99, 1.0, 1.0, 1.0, 1.0, 1.0, 0.27, 1.0, 0.99, 0.89, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.86, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.93, 1.0, 1.0, 0.96, 1.0, 1.0, 1.0, 1.0, 0.86, 1.0, 1.0, 1.0, 0.98, 1.0, 1.0, 0.99, 1.0, 1.0, 1.0, 0.94, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99, 1.0, 1.0, 0.96, 1.0, 1.0, 0.79, 1.0, 0.79, 0.88, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 0.72, 1.0, 0.66, 0.98, 1.0, 1.0, 0.91, 0.99, 0.61, 0.71, 0.99, 0.28, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94, 0.95, 1.0, 0.83, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.95, 1.0, 0.96, 0.97, 1.0, 0.91, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99, 0.98, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.98, 1.0, 0.98, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.65, 0.92, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99, 0.98, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.98, 1.0, 0.85, 1.0, 1.0, 1.0, 1.0, 0.99, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.98, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.95, 1.0, 1.0, 1.0, 0.94, 1.0, 1.0, 1.0, 1.0, 1.0, 0.89, 1.0, 0.91, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.93, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99, 0.99, 1.0, 0.91, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99, 0.98, 0.87, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.92, 1.0, 0.98, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.62, 0.11, 0.34, 0.0, 1.0, 0.58, 0.3, 0.87, 0.29, 0.21, 0.17, 0.1, 0.75, 0.92, 0.26, 0.9, 0.85, 1.0, 1.0, 0.99, 1.0, 1.0, 0.96, 1.0, 0.84, 1.0, 0.93, 1.0, 1.0, 0.85, 0.89, 0.98, 0.98, 1.0, 0.98, 0.96, 0.81, 0.12, 0.63, 0.87, 0.98, 0.85, 0.74, 0.96, 0.99, 0.98, 0.87, 0.96, 0.92, 0.98, 0.82, 1.0, 0.97, 0.99, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.54, 1.0, 1.0, 1.0, 0.99, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.95, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99, 1.0, 1.0, 0.96, 0.95, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96, 0.89, 1.0, 0.87, 0.95, 1.0, 0.98, 0.99, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.71, 1.0, 0.93, 1.0, 1.0, 0.75, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.95, 1.0, 1.0, 1.0, 0.98, 1.0, 1.0, 1.0, 1.0, 0.93, 1.0, 0.96, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.95, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.92, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.97, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.97, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.98, 1.0, 0.99, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.87, 1.0, 0.97, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.39, 1.0, 1.0, 0.51, 0.98, 1.0, 0.99, 0.93, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.97, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.01, 0.97, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.95, 1.0, 1.0, 1.0, 0.97, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.98, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.98, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96, 1.0, 1.0, 0.89, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99, 1.0, 0.95, 0.99, 0.97, 1.0, 1.0, 0.96, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99, 1.0, 1.0, 1.0, 0.96, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94, 0.98, 1.0, 1.0, 1.0, 1.0, 0.91, 1.0, 1.0, 1.0, 0.93, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.91, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.97, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.85, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99, 1.0, 1.0, 1.0, 0.93, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.98, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99, 1.0, 1.0, 1.0, 1.0, 0.99, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.98, 0.86, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.91, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.03, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.88, 1.0, 1.0, 1.0, 0.96, 1.0, 0.96, 0.97, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.95, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.97, 1.0, 1.0, 1.0, 0.99, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.97, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.92, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.82, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.98, 1.0, 0.99, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.81, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.85, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.92, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.98, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.98, 1.0, 0.99, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99, 1.0, 0.98, 0.61, 0.97, 0.95, 0.99, 0.98, 0.99, 0.96, 1.0, 1.0, 0.95, 0.69, 1.0, 0.76, 1.0, 0.91, 0.99, 0.5, 0.13, 0.57, 0.45, 0.84, 1.0, 1.0, 0.96, 0.78, 0.04, 1.0, 0.93, 0.43, 0.05, 0.02, 0.6, 0.03, 0.07, 0.04, 0.01, 0.06, 0.06, 0.03, 0.0, 0.98, 0.6, 0.55, 0.12, 0.03, 0.3, 0.07, 0.06, 0.04, 0.22, 0.07, 0.43, 0.47, 0.03, 0.69, 0.25, 0.08, 0.1, 0.14, 0.16, 0.06, 0.07, 0.24, 0.73, 0.43, 0.59, 0.76, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.85, 1.0, 1.0, 1.0, 0.94, 1.0, 1.0, 0.76, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.98, 0.96, 1.0, 0.86, 1.0, 0.99, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99, 1.0, 1.0, 1.0, 1.0, 0.97, 1.0, 1.0, 1.0, 1.0, 1.0, 0.92, 0.88, 1.0, 0.96, 1.0, 0.99, 1.0, 1.0, 0.98, 1.0, 0.98, 0.95, 0.88, 1.0, 0.94, 0.98, 1.0, 0.88, 0.84, 0.91, 0.77, 0.96, 0.75, 0.94, 0.99, 1.0, 0.99, 1.0, 1.0, 0.93, 1.0, 1.0, 1.0, 0.96, 1.0, 1.0, 1.0, 0.97, 1.0, 1.0, 1.0, 0.89, 0.96, 0.98, 1.0, 1.0, 1.0, 1.0, 1.0, 0.95, 0.24, 1.0, 1.0, 0.99, 0.97, 1.0, 1.0, 1.0, 1.0, 0.93, 0.99, 1.0, 1.0, 1.0, 0.94, 0.99, 1.0, 1.0, 0.99, 0.96, 0.61, 0.97, 0.51, 0.18, 0.87, 1.0, 1.0, 0.99, 1.0, 1.0, 0.94, 1.0, 0.98, 1.0, 1.0, 0.95, 1.0, 1.0, 1.0, 1.0, 1.0, 0.97, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99, 1.0, 0.97, 0.87, 0.95, 0.74, 0.78, 0.91, 0.6, 0.68, 0.95, 0.7, 1.0, 0.69, 0.73, 0.84, 0.9, 1.0, 0.98, 1.0, 0.99, 0.99, 0.96, 0.79, 1.0, 0.85, 1.0, 1.0, 0.94, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99, 1.0, 1.0, 1.0, 0.98, 1.0, 0.99, 0.97, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.88, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.96, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.95, 1.0, 0.99, 0.79, 1.0, 1.0, 0.93, 1.0, 0.84, 0.98, 0.78, 1.0, 0.98, 0.69, 0.04, 1.0, 0.63, 0.93, 0.93, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99, 1.0, 1.0, 1.0, 1.0, 1.0, 0.43, 1.0, 1.0, 0.97, 1.0, 0.89, 1.0, 0.99, 0.8, 1.0, 0.99, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.98, 1.0, 1.0, 1.0, 1.0, 1.0, 0.83, 0.99, 0.99, 1.0, 0.69, 0.39, 0.89, 0.96, 0.92, 1.0, 1.0, 1.0, 0.98, 1.0, 1.0, 0.91, 0.98, 1.0, 1.0, 1.0, 1.0, 0.96, 0.98, 1.0, 1.0, 1.0, 0.71, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99, 0.72, 0.95, 1.0, 0.59, 1.0, 0.88, 0.67, 1.0, 1.0, 1.0, 1.0, 1.0, 0.83, 1.0, 0.67, 1.0, 1.0, 1.0, 1.0, 0.24, 1.0, 1.0, 0.84, 1.0, 0.99, 0.79, 0.99, 0.98, 0.98, 0.86, 1.0, 0.77, 0.99, 0.99, 0.86, 0.99, 0.81, 0.98, 0.06, 0.05, 0.0, 0.1, 0.38, 0.84, 0.43, 0.46, 0.33, 0.25, 0.69, 0.14, 0.29, 0.43, 0.43, 0.48, 0.27, 0.14, 0.5, 1.0, 0.94, 0.84, 0.81, 0.98, 1.0, 0.96, 0.93, 1.0, 1.0, 0.99, 1.0, 0.48, 0.38, 0.0, 0.05, 0.56, 0.28, 0.98, 1.0, 0.37, 0.55, 0.26, 0.12, 0.71, 0.39, 0.8, 0.42, 0.39, 0.15, 0.82, 0.17, 0.44, 0.76, 0.96, 0.1, 0.56, 0.02, 0.28, 0.55, 0.72, 0.39, 0.26, 0.01, 0.01, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99, 1.0, 0.99, 1.0, 1.0, 1.0, 1.0, 1.0, 0.51, 1.0, 1.0, 1.0, 1.0, 0.95, 0.96, 1.0, 1.0, 1.0, 0.94, 1.0, 0.97, 1.0, 1.0, 1.0, 1.0, 0.93, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.97, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.98, 0.97, 0.99, 0.86, 1.0, 1.0, 1.0, 1.0, 0.99, 1.0, 1.0, 1.0, 1.0, 1.0, 0.78, 0.72, 1.0, 0.91, 1.0, 1.0, 1.0, 1.0, 0.93, 0.98, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.63, 0.73, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96, 1.0, 1.0, 0.98, 1.0, 1.0, 1.0, 1.0, 1.0, 0.93, 1.0, 1.0, 1.0, 1.0, 0.99, 1.0, 1.0, 0.97, 1.0, 1.0, 0.99, 1.0, 1.0, 0.93, 0.94, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.91, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.93, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.98, 1.0, 1.0, 0.99, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96, 1.0, 0.99, 1.0, 0.82, 0.99, 0.99, 0.98, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.98, 1.0, 1.0, 1.0, 1.0, 0.95, 0.99, 0.99, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.91, 1.0, 1.0, 1.0, 0.9, 0.95, 1.0, 1.0, 0.96, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.93, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.81, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.98, 1.0, 1.0, 0.99, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99, 1.0, 1.0, 1.0, 1.0, 1.0, 0.93, 1.0, 0.82, 1.0, 1.0, 1.0, 0.99, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.85, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.88, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96, 0.88, 0.96, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.67, 1.0, 0.87, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.98, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.98, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.12, 0.92, 0.49, 0.92, 0.94]\n",
      "[0.98, 0.97, 0.91, 0.71, 0.99, 0.98, 0.99, 1.0, 0.86, 0.1, 1.0, 0.01, 0.03, 0.14, 0.08, 0.05, 0.01, 0.95, 0.0, 0.15, 1.0, 0.0, 0.0, 0.45, 0.0, 0.02, 0.0, 1.0, 0.66, 1.0, 0.97, 0.83, 0.9, 0.92, 0.82, 0.95, 0.94, 0.65, 0.55, 0.9, 0.9, 0.95, 0.9, 0.46, 0.46, 0.75, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.02, 0.0, 0.02, 0.0, 0.27, 0.03, 0.18, 0.0, 0.07, 0.01, 0.0, 0.0, 0.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05, 0.0, 0.02, 0.0, 0.0, 0.0, 0.0, 0.12, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.18, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.09, 0.16, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04, 0.0, 0.0, 0.0, 0.02, 0.0, 0.0, 0.08, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06, 0.0, 0.0, 0.06, 0.03, 0.0, 0.0, 0.0, 0.0, 0.04, 0.0, 0.0, 0.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.06, 0.12, 0.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.14, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02, 0.0, 0.0, 0.04, 0.0, 0.0, 0.02, 0.06, 0.03, 0.02, 0.01, 0.0, 0.03, 0.0, 0.06, 0.0, 0.06, 0.01, 0.26, 0.0, 0.0, 0.77, 0.02, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.01, 0.02, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.05, 0.41, 0.02, 0.03, 0.12, 0.06, 0.04, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.01, 0.0, 0.02, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02, 0.15, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02, 0.0, 0.0, 0.01, 0.26, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.22, 0.0, 0.0, 0.0, 0.0, 0.02, 0.0, 0.0, 0.0, 0.0, 0.14, 0.41, 0.1, 0.0, 0.26, 0.0, 0.0, 0.02, 0.12, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06, 0.0, 0.0, 0.0, 0.0, 0.09, 0.0, 0.3, 0.0, 0.14, 0.04, 0.0, 0.01, 0.0, 0.05, 0.02, 0.01, 0.0, 0.0, 0.0, 0.0, 0.02, 0.09, 0.0, 0.0, 0.01, 0.0, 0.03, 0.01, 0.0, 0.14, 0.33, 0.01, 0.0, 0.0, 0.0, 0.09, 0.0, 0.0, 0.03, 0.06, 0.0, 0.09, 0.07, 0.02, 0.98, 0.89, 0.0, 0.55, 0.87, 0.01, 0.86, 0.82, 0.03, 0.05, 0.81, 0.57, 0.57, 0.91, 1.0, 0.43, 0.99, 1.0, 0.73, 0.33, 0.91, 0.99, 0.0, 0.0, 0.35, 0.72, 0.03, 0.0, 1.0, 0.0, 0.61, 0.7, 0.33, 0.31, 0.0, 0.22, 0.64, 0.38, 0.39, 0.91, 0.39, 0.06, 0.38, 0.88, 0.09, 0.87, 0.92, 0.93, 1.0, 0.48, 0.69, 0.59, 0.77, 0.94, 0.77, 0.56, 0.36, 0.38, 0.16, 0.53, 0.32, 0.46, 0.41, 0.58, 0.11, 0.32, 0.61, 0.86, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02, 0.0, 0.04, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.01, 0.01, 0.0, 0.0, 0.0, 0.04, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.14, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.22, 0.01, 0.0, 0.17, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06, 0.0, 0.0, 0.25, 0.0, 0.0, 0.18, 0.0, 0.0, 0.03, 0.0, 0.0, 0.0, 0.0, 0.04, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06, 0.0, 0.02, 0.0, 0.0, 0.02, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07, 0.0, 0.0, 0.01, 0.0, 0.0, 0.01, 0.0, 0.0, 0.02, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.04, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19, 0.0, 0.0, 0.02, 0.11, 0.0, 0.0, 0.01, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.02, 0.01, 0.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.14, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.87, 0.01, 1.0, 0.65, 0.01, 0.39, 0.12, 0.4, 0.29, 0.03, 0.16, 0.42, 0.08, 0.29, 0.3, 0.43, 0.67, 0.28, 0.34, 0.63, 0.41, 0.13, 0.47, 0.62, 0.48, 0.3, 0.76, 0.76, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08, 0.02, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.03, 0.0, 0.02, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02, 0.02, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03, 0.0, 0.0, 0.05, 0.0, 0.08, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02, 0.0, 0.05, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13, 0.01, 0.0, 0.0, 0.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02, 0.0, 0.0, 0.05, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.02, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07, 0.0, 0.0, 0.0, 0.03, 0.02, 0.0, 0.19, 0.0, 0.0, 0.0, 0.09, 0.01, 0.0, 0.01, 0.0, 0.01, 0.0, 0.06, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06, 0.0, 0.0, 0.0, 0.06, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16, 0.0, 0.01, 0.0, 0.0, 0.0, 0.01, 0.0, 0.03, 0.0, 0.0, 0.0, 0.0, 0.27, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02, 0.02, 0.0, 0.0, 0.15, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.14, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.02, 0.0, 0.03, 0.02, 0.0, 0.0, 0.0, 0.0, 0.07, 0.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.27, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04, 0.0, 0.0, 0.09, 0.02, 0.02, 0.08, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.01, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13, 0.11, 0.0, 0.0, 0.13, 0.06, 0.0, 0.0, 0.0, 0.18, 0.0, 0.49, 0.0, 0.03, 0.0, 0.0, 0.0, 0.0, 0.03, 0.0, 0.02, 0.04, 0.0, 0.12, 0.13, 0.0, 0.12, 0.07, 0.08, 0.0, 0.0, 0.04, 0.05, 0.12, 0.0, 0.0, 0.0, 0.01, 0.36, 0.0, 0.04, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04, 0.23, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.17, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.06, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02, 0.12, 0.7, 0.14, 0.5, 0.12, 0.09, 0.04, 0.99, 0.35, 0.01, 0.44, 0.98, 0.84, 0.3, 0.3, 0.5, 0.85, 0.86, 0.05, 0.29, 0.86, 0.86, 0.95, 0.72, 0.69, 0.93, 0.96, 0.98, 0.34, 0.93, 0.86, 0.92, 0.99, 0.85, 0.87, 0.03, 0.95, 0.97, 0.0, 0.13, 0.03, 0.0, 0.01, 0.03, 0.0, 0.99, 0.0, 0.0, 0.96, 0.2, 0.26, 0.28, 1.0, 0.88, 0.85, 0.73, 0.96, 0.4, 0.9, 0.7, 0.43, 0.32, 0.9, 0.82, 0.12, 0.96, 0.51, 1.0, 0.8, 0.44, 0.47, 0.73, 0.85, 1.0, 0.54, 0.69, 0.7, 0.98, 0.67, 0.93, 0.84, 1.0, 0.89, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02, 0.0, 0.0, 0.0, 0.0, 0.04, 0.01, 0.0, 0.03, 0.0, 0.02, 0.0, 0.0, 0.0, 0.0, 0.02, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04, 0.02, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03, 0.0, 0.02, 0.0, 0.0, 0.04, 0.0, 0.0, 0.0, 0.03, 0.06, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07, 0.0, 0.05, 0.0, 0.14, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04, 0.0, 0.0, 0.02, 0.0, 0.14, 0.01, 0.0, 0.0, 0.0, 0.01, 0.02, 0.03, 0.0, 0.1, 0.0, 0.05, 0.0, 0.84, 0.0, 0.0, 0.08, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02, 0.0, 0.0, 0.02, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.54, 0.01, 0.0, 0.0, 0.0, 0.0, 0.11, 0.12, 0.01, 0.0, 0.07, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26, 0.0, 0.02, 0.02, 0.02, 0.05, 0.0, 0.0, 0.06, 0.0, 0.0, 0.0, 0.0, 0.03, 0.8, 0.0, 0.0, 0.0, 0.0, 0.22, 0.05, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.07, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.23, 0.02, 0.0, 0.0, 0.0, 0.0, 0.22, 0.29, 0.0, 0.04, 0.02, 0.27, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12, 0.48, 0.08, 0.04, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02, 0.0, 0.0, 0.02, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06, 0.0, 0.37, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.75, 0.0, 0.0, 0.0, 0.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12, 0.0, 0.0, 0.22, 0.01, 0.0, 0.42, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.29, 0.09, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09, 0.0, 0.2, 0.0, 0.0, 0.0, 0.04, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.09, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.05, 0.0, 0.0, 0.11, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.14, 0.38, 0.23, 0.0, 0.0, 0.37, 0.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02, 0.0, 0.17, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.02, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.3, 0.99, 0.75, 0.34, 0.0, 0.22, 0.22, 0.3, 0.62, 0.75, 0.0, 0.9, 0.96, 0.92, 0.53, 0.95, 0.2, 0.34, 0.81, 0.04, 0.18, 0.09, 0.02, 0.21, 0.0, 0.21, 0.0, 0.0, 0.0, 0.65, 0.04, 0.34, 0.29, 0.93, 0.95, 0.48, 0.26, 0.05, 0.79, 0.29, 0.0, 0.13, 0.68, 0.87, 0.02, 0.85, 0.04, 0.0, 0.8, 0.0, 0.42, 0.15, 0.23, 0.74, 0.36, 0.72, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02, 0.0, 0.0, 0.0, 0.0, 0.0, 0.17, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.18, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02, 0.0, 0.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07, 0.01, 0.0, 0.0, 0.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09, 0.0, 0.0, 0.0, 0.0, 0.05, 0.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03, 0.0, 0.0, 0.06, 0.02, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06, 0.31, 0.02, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13, 0.0, 0.06, 0.0, 0.0, 0.0, 0.0, 0.05, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.74, 0.07, 0.01, 0.0, 0.21, 0.54, 0.07, 0.26, 0.83, 0.84, 0.49, 0.84, 0.98, 0.75, 0.84, 0.5, 0.73, 0.89, 0.68, 0.11, 0.77, 0.0, 0.0, 0.06, 0.0, 0.02, 0.29, 0.35, 0.99, 0.88, 0.99, 0.93, 0.89, 0.97, 0.31, 0.62, 0.96, 0.76, 0.34, 0.43, 0.38, 0.58, 0.95, 0.84, 0.8, 0.85, 0.59, 0.26, 1.0, 0.42, 0.99, 0.24, 1.0, 0.89, 0.38, 0.31, 0.74, 0.63, 0.64, 0.11, 0.69, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.29, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.04, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03, 0.0, 0.0, 0.06, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04, 0.0, 0.01, 0.0, 0.0, 0.02, 0.0, 0.0, 0.0, 0.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.24, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13, 0.11, 0.09, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.01, 0.06, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13, 0.0, 0.03, 0.0, 0.0, 0.01, 0.0, 0.28, 0.02, 0.04, 0.0, 0.0, 0.39, 0.0, 0.01, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.34, 0.0, 0.0, 0.0, 0.02, 0.0, 0.0, 0.0, 0.12, 0.0, 0.0, 0.0, 0.04, 0.0, 0.03, 0.0, 0.0, 0.01, 0.0, 0.0, 0.01, 0.01, 0.09, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15, 0.0, 0.0, 0.0, 0.08, 0.0, 0.03, 0.0, 0.0, 0.01, 0.0, 0.06, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03, 0.0, 0.0, 0.35, 0.0, 0.0, 0.1, 0.0, 0.0, 0.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.9, 0.85, 0.12, 0.9, 0.22, 0.91, 0.65, 0.8, 0.97, 0.78, 0.81, 0.89, 0.97, 0.94, 0.97, 0.99, 0.88, 0.69, 0.65, 0.43, 0.98, 0.91, 0.95, 0.0, 0.04, 1.0, 0.88, 0.82, 0.87, 0.62, 0.97, 1.0, 1.0, 0.84, 0.71, 0.82, 0.79, 0.89, 0.63, 0.69, 0.87, 0.82, 0.22, 0.99, 0.94, 0.41, 0.99, 0.88, 0.46, 0.97, 0.83, 0.25, 0.87, 0.55, 0.98, 0.81, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04, 0.0, 0.0, 0.0, 0.15, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11, 0.0, 0.04, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3, 0.04, 0.06, 0.0, 0.01, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.04, 0.0, 0.0, 0.02, 0.0, 0.02, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.17, 0.0, 0.04, 0.0, 0.0, 0.11, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19, 0.04, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.01, 0.1, 0.22, 0.0, 0.06, 0.11, 0.0, 0.0, 0.0, 0.05, 0.0, 0.0, 0.11, 0.0, 0.0, 0.0, 0.17, 0.56, 0.04, 0.0, 0.04, 0.09, 0.0, 0.02, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.01, 0.02, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.1, 0.01, 0.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.73, 0.0, 0.01, 0.11, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.14, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07, 0.0, 0.0, 0.04, 0.0, 0.0, 0.0, 0.0, 0.14, 0.0, 0.0, 0.0, 0.02, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.06, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.04, 0.0, 0.0, 0.21, 0.0, 0.21, 0.12, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.0, 0.28, 0.0, 0.34, 0.02, 0.0, 0.0, 0.09, 0.01, 0.39, 0.29, 0.01, 0.72, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06, 0.05, 0.0, 0.17, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05, 0.0, 0.04, 0.03, 0.0, 0.09, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.02, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02, 0.0, 0.02, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.35, 0.08, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.02, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02, 0.0, 0.15, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05, 0.0, 0.0, 0.0, 0.06, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11, 0.0, 0.09, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.01, 0.0, 0.09, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.02, 0.13, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08, 0.0, 0.02, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.38, 0.89, 0.66, 1.0, 0.0, 0.42, 0.7, 0.13, 0.71, 0.79, 0.83, 0.9, 0.25, 0.08, 0.74, 0.1, 0.15, 0.0, 0.0, 0.01, 0.0, 0.0, 0.04, 0.0, 0.16, 0.0, 0.07, 0.0, 0.0, 0.15, 0.11, 0.02, 0.02, 0.0, 0.02, 0.04, 0.19, 0.88, 0.37, 0.13, 0.02, 0.15, 0.26, 0.04, 0.01, 0.02, 0.13, 0.04, 0.08, 0.02, 0.18, 0.0, 0.03, 0.01, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.46, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.04, 0.05, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04, 0.11, 0.0, 0.13, 0.05, 0.0, 0.02, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.29, 0.0, 0.07, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05, 0.0, 0.0, 0.0, 0.02, 0.0, 0.0, 0.0, 0.0, 0.07, 0.0, 0.04, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13, 0.0, 0.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.61, 0.0, 0.0, 0.49, 0.02, 0.0, 0.01, 0.07, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99, 0.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05, 0.0, 0.0, 0.0, 0.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04, 0.0, 0.0, 0.11, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.05, 0.01, 0.03, 0.0, 0.0, 0.04, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.04, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06, 0.02, 0.0, 0.0, 0.0, 0.0, 0.09, 0.0, 0.0, 0.0, 0.07, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.07, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02, 0.14, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.97, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12, 0.0, 0.0, 0.0, 0.04, 0.0, 0.04, 0.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.18, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.02, 0.39, 0.03, 0.05, 0.01, 0.02, 0.01, 0.04, 0.0, 0.0, 0.05, 0.31, 0.0, 0.24, 0.0, 0.09, 0.01, 0.5, 0.87, 0.43, 0.55, 0.16, 0.0, 0.0, 0.04, 0.22, 0.96, 0.0, 0.07, 0.57, 0.95, 0.98, 0.4, 0.97, 0.93, 0.96, 0.99, 0.94, 0.94, 0.97, 1.0, 0.02, 0.4, 0.45, 0.88, 0.97, 0.7, 0.93, 0.94, 0.96, 0.78, 0.93, 0.57, 0.53, 0.97, 0.31, 0.75, 0.92, 0.9, 0.86, 0.84, 0.94, 0.93, 0.76, 0.27, 0.57, 0.41, 0.24, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15, 0.0, 0.0, 0.0, 0.06, 0.0, 0.0, 0.24, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02, 0.04, 0.0, 0.14, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08, 0.12, 0.0, 0.04, 0.0, 0.01, 0.0, 0.0, 0.02, 0.0, 0.02, 0.05, 0.12, 0.0, 0.06, 0.02, 0.0, 0.12, 0.16, 0.09, 0.23, 0.04, 0.25, 0.06, 0.01, 0.0, 0.01, 0.0, 0.0, 0.07, 0.0, 0.0, 0.0, 0.04, 0.0, 0.0, 0.0, 0.03, 0.0, 0.0, 0.0, 0.11, 0.04, 0.02, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05, 0.76, 0.0, 0.0, 0.01, 0.03, 0.0, 0.0, 0.0, 0.0, 0.07, 0.01, 0.0, 0.0, 0.0, 0.06, 0.01, 0.0, 0.0, 0.01, 0.04, 0.39, 0.03, 0.49, 0.82, 0.13, 0.0, 0.0, 0.01, 0.0, 0.0, 0.06, 0.0, 0.02, 0.0, 0.0, 0.05, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.03, 0.13, 0.05, 0.26, 0.22, 0.09, 0.4, 0.32, 0.05, 0.3, 0.0, 0.31, 0.27, 0.16, 0.1, 0.0, 0.02, 0.0, 0.01, 0.01, 0.04, 0.21, 0.0, 0.15, 0.0, 0.0, 0.06, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.02, 0.0, 0.01, 0.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.04, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05, 0.0, 0.01, 0.21, 0.0, 0.0, 0.07, 0.0, 0.16, 0.02, 0.22, 0.0, 0.02, 0.31, 0.96, 0.0, 0.37, 0.07, 0.07, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.57, 0.0, 0.0, 0.03, 0.0, 0.11, 0.0, 0.01, 0.2, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02, 0.0, 0.0, 0.0, 0.0, 0.0, 0.17, 0.01, 0.01, 0.0, 0.31, 0.61, 0.11, 0.04, 0.08, 0.0, 0.0, 0.0, 0.02, 0.0, 0.0, 0.09, 0.02, 0.0, 0.0, 0.0, 0.0, 0.04, 0.02, 0.0, 0.0, 0.0, 0.29, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.28, 0.05, 0.0, 0.41, 0.0, 0.12, 0.33, 0.0, 0.0, 0.0, 0.0, 0.0, 0.17, 0.0, 0.33, 0.0, 0.0, 0.0, 0.0, 0.76, 0.0, 0.0, 0.16, 0.0, 0.01, 0.21, 0.01, 0.02, 0.02, 0.14, 0.0, 0.23, 0.01, 0.01, 0.14, 0.01, 0.19, 0.02, 0.94, 0.95, 1.0, 0.9, 0.62, 0.16, 0.57, 0.54, 0.67, 0.75, 0.31, 0.86, 0.71, 0.57, 0.57, 0.52, 0.73, 0.86, 0.5, 0.0, 0.06, 0.16, 0.19, 0.02, 0.0, 0.04, 0.07, 0.0, 0.0, 0.01, 0.0, 0.52, 0.62, 1.0, 0.95, 0.44, 0.72, 0.02, 0.0, 0.63, 0.45, 0.74, 0.88, 0.29, 0.61, 0.2, 0.58, 0.61, 0.85, 0.18, 0.83, 0.56, 0.24, 0.04, 0.9, 0.44, 0.98, 0.72, 0.45, 0.28, 0.61, 0.74, 0.99, 0.99, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.49, 0.0, 0.0, 0.0, 0.0, 0.05, 0.04, 0.0, 0.0, 0.0, 0.06, 0.0, 0.03, 0.0, 0.0, 0.0, 0.0, 0.07, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02, 0.03, 0.01, 0.14, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.22, 0.28, 0.0, 0.09, 0.0, 0.0, 0.0, 0.0, 0.07, 0.02, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37, 0.27, 0.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04, 0.0, 0.0, 0.02, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.03, 0.0, 0.0, 0.01, 0.0, 0.0, 0.07, 0.06, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04, 0.0, 0.01, 0.0, 0.18, 0.01, 0.01, 0.02, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02, 0.0, 0.0, 0.0, 0.0, 0.05, 0.01, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09, 0.0, 0.0, 0.0, 0.1, 0.05, 0.0, 0.0, 0.04, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07, 0.0, 0.18, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04, 0.12, 0.04, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.33, 0.0, 0.13, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.88, 0.08, 0.51, 0.08, 0.06]\n"
     ]
    }
   ],
   "source": [
    "lev_df, lev_class, probabilities = model_eval(features[['SSM', 'MSE']], twisted_df['Registered'], rf_model)\n",
    "neg = [i[0] for i in probabilities]\n",
    "pos = [i[1] for i in probabilities]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SSM</th>\n",
       "      <th>MSE</th>\n",
       "      <th>Registered</th>\n",
       "      <th>Negative Prediction</th>\n",
       "      <th>Positive Prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>(googlea.com, google.com)</th>\n",
       "      <td>0.851078</td>\n",
       "      <td>3562.883354</td>\n",
       "      <td>True</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(googlej.com, google.com)</th>\n",
       "      <td>0.849594</td>\n",
       "      <td>3890.821388</td>\n",
       "      <td>True</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(googlep.com, google.com)</th>\n",
       "      <td>0.850138</td>\n",
       "      <td>3541.100876</td>\n",
       "      <td>True</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(googleg.com, google.com)</th>\n",
       "      <td>0.847150</td>\n",
       "      <td>3639.159492</td>\n",
       "      <td>True</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(googlel.com, google.com)</th>\n",
       "      <td>0.852116</td>\n",
       "      <td>3855.488113</td>\n",
       "      <td>True</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(insqtagram.com, instagram.com)</th>\n",
       "      <td>0.834099</td>\n",
       "      <td>3613.393341</td>\n",
       "      <td>False</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(instagr1am.com, instagram.com)</th>\n",
       "      <td>0.831515</td>\n",
       "      <td>3722.531998</td>\n",
       "      <td>False</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(instagra2m.com, instagram.com)</th>\n",
       "      <td>0.832508</td>\n",
       "      <td>3693.410309</td>\n",
       "      <td>False</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(instawgram.com, instagram.com)</th>\n",
       "      <td>0.827028</td>\n",
       "      <td>4088.740196</td>\n",
       "      <td>False</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(insytagram.com, instagram.com)</th>\n",
       "      <td>0.831153</td>\n",
       "      <td>3826.378334</td>\n",
       "      <td>False</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9402 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      SSM          MSE  Registered  \\\n",
       "(googlea.com, google.com)        0.851078  3562.883354        True   \n",
       "(googlej.com, google.com)        0.849594  3890.821388        True   \n",
       "(googlep.com, google.com)        0.850138  3541.100876        True   \n",
       "(googleg.com, google.com)        0.847150  3639.159492        True   \n",
       "(googlel.com, google.com)        0.852116  3855.488113        True   \n",
       "...                                   ...          ...         ...   \n",
       "(insqtagram.com, instagram.com)  0.834099  3613.393341       False   \n",
       "(instagr1am.com, instagram.com)  0.831515  3722.531998       False   \n",
       "(instagra2m.com, instagram.com)  0.832508  3693.410309       False   \n",
       "(instawgram.com, instagram.com)  0.827028  4088.740196       False   \n",
       "(insytagram.com, instagram.com)  0.831153  3826.378334       False   \n",
       "\n",
       "                                 Negative Prediction  Positive Prediction  \n",
       "(googlea.com, google.com)                       0.02                 0.98  \n",
       "(googlej.com, google.com)                       0.03                 0.97  \n",
       "(googlep.com, google.com)                       0.09                 0.91  \n",
       "(googleg.com, google.com)                       0.29                 0.71  \n",
       "(googlel.com, google.com)                       0.01                 0.99  \n",
       "...                                              ...                  ...  \n",
       "(insqtagram.com, instagram.com)                 0.12                 0.88  \n",
       "(instagr1am.com, instagram.com)                 0.92                 0.08  \n",
       "(instagra2m.com, instagram.com)                 0.49                 0.51  \n",
       "(instawgram.com, instagram.com)                 0.92                 0.08  \n",
       "(insytagram.com, instagram.com)                 0.94                 0.06  \n",
       "\n",
       "[9402 rows x 5 columns]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lev_df['Registered'] = lev_class.values\n",
    "lev_df['Negative Prediction'] = neg\n",
    "lev_df['Positive Prediction'] = pos\n",
    "lev_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SSM</th>\n",
       "      <th>MSE</th>\n",
       "      <th>Registered</th>\n",
       "      <th>Negative Prediction</th>\n",
       "      <th>Positive Prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>(googlea.com, google.com)</th>\n",
       "      <td>0.851078</td>\n",
       "      <td>3562.883354</td>\n",
       "      <td>True</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(googlej.com, google.com)</th>\n",
       "      <td>0.849594</td>\n",
       "      <td>3890.821388</td>\n",
       "      <td>True</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(googlep.com, google.com)</th>\n",
       "      <td>0.850138</td>\n",
       "      <td>3541.100876</td>\n",
       "      <td>True</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(googleg.com, google.com)</th>\n",
       "      <td>0.847150</td>\n",
       "      <td>3639.159492</td>\n",
       "      <td>True</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(googlel.com, google.com)</th>\n",
       "      <td>0.852116</td>\n",
       "      <td>3855.488113</td>\n",
       "      <td>True</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(istagram.com, instagram.com)</th>\n",
       "      <td>0.852322</td>\n",
       "      <td>3221.088409</td>\n",
       "      <td>True</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(iinstagram.com, instagram.com)</th>\n",
       "      <td>0.821251</td>\n",
       "      <td>4741.810089</td>\n",
       "      <td>True</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(instagraam.com, instagram.com)</th>\n",
       "      <td>0.834844</td>\n",
       "      <td>3615.047722</td>\n",
       "      <td>True</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(jnstagram.com, instagram.com)</th>\n",
       "      <td>0.908763</td>\n",
       "      <td>1257.719048</td>\n",
       "      <td>True</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(instargam.com, instagram.com)</th>\n",
       "      <td>0.976039</td>\n",
       "      <td>540.016975</td>\n",
       "      <td>True</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.98</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>236 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      SSM          MSE  Registered  \\\n",
       "(googlea.com, google.com)        0.851078  3562.883354        True   \n",
       "(googlej.com, google.com)        0.849594  3890.821388        True   \n",
       "(googlep.com, google.com)        0.850138  3541.100876        True   \n",
       "(googleg.com, google.com)        0.847150  3639.159492        True   \n",
       "(googlel.com, google.com)        0.852116  3855.488113        True   \n",
       "...                                   ...          ...         ...   \n",
       "(istagram.com, instagram.com)    0.852322  3221.088409        True   \n",
       "(iinstagram.com, instagram.com)  0.821251  4741.810089        True   \n",
       "(instagraam.com, instagram.com)  0.834844  3615.047722        True   \n",
       "(jnstagram.com, instagram.com)   0.908763  1257.719048        True   \n",
       "(instargam.com, instagram.com)   0.976039   540.016975        True   \n",
       "\n",
       "                                 Negative Prediction  Positive Prediction  \n",
       "(googlea.com, google.com)                       0.02                 0.98  \n",
       "(googlej.com, google.com)                       0.03                 0.97  \n",
       "(googlep.com, google.com)                       0.09                 0.91  \n",
       "(googleg.com, google.com)                       0.29                 0.71  \n",
       "(googlel.com, google.com)                       0.01                 0.99  \n",
       "...                                              ...                  ...  \n",
       "(istagram.com, instagram.com)                   0.15                 0.85  \n",
       "(iinstagram.com, instagram.com)                 0.17                 0.83  \n",
       "(instagraam.com, instagram.com)                 0.44                 0.56  \n",
       "(jnstagram.com, instagram.com)                  0.10                 0.90  \n",
       "(instargam.com, instagram.com)                  0.02                 0.98  \n",
       "\n",
       "[236 rows x 5 columns]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create boolean mask based on the conditions\n",
    "mask = (lev_df['Registered'] & (lev_df['Positive Prediction'] > lev_df['Negative Prediction']))\n",
    "\n",
    "# Filter the DataFrame based on the mask\n",
    "filtered_df = lev_df.loc[mask]\n",
    "\n",
    "# View the filtered DataFrame\n",
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SSM</th>\n",
       "      <th>MSE</th>\n",
       "      <th>Registered</th>\n",
       "      <th>Negative Prediction</th>\n",
       "      <th>Positive Prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>(tw8itter.com, twitter.com)</th>\n",
       "      <td>0.883237</td>\n",
       "      <td>2551.699104</td>\n",
       "      <td>True</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(qoogie.com, google.com)</th>\n",
       "      <td>0.992383</td>\n",
       "      <td>122.182693</td>\n",
       "      <td>True</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(microsuft.com, microsoft.com)</th>\n",
       "      <td>0.991773</td>\n",
       "      <td>134.152702</td>\n",
       "      <td>True</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(facebooc.com, facebook.com)</th>\n",
       "      <td>0.986877</td>\n",
       "      <td>276.975258</td>\n",
       "      <td>True</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(twltter.com, twitter.com)</th>\n",
       "      <td>0.998921</td>\n",
       "      <td>23.120117</td>\n",
       "      <td>True</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(baid.com, baidu.com)</th>\n",
       "      <td>0.889837</td>\n",
       "      <td>2497.234947</td>\n",
       "      <td>True</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(emazonaws.com, amazonaws.com)</th>\n",
       "      <td>0.992621</td>\n",
       "      <td>114.690033</td>\n",
       "      <td>True</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1nstagrarn.com, instagram.com)</th>\n",
       "      <td>0.826988</td>\n",
       "      <td>4502.406700</td>\n",
       "      <td>True</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(insvagram.com, instagram.com)</th>\n",
       "      <td>0.823689</td>\n",
       "      <td>4641.342072</td>\n",
       "      <td>True</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(7outube.com, youtube.com)</th>\n",
       "      <td>0.855983</td>\n",
       "      <td>3189.605362</td>\n",
       "      <td>True</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.51</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>236 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      SSM          MSE  Registered  \\\n",
       "(tw8itter.com, twitter.com)      0.883237  2551.699104        True   \n",
       "(qoogie.com, google.com)         0.992383   122.182693        True   \n",
       "(microsuft.com, microsoft.com)   0.991773   134.152702        True   \n",
       "(facebooc.com, facebook.com)     0.986877   276.975258        True   \n",
       "(twltter.com, twitter.com)       0.998921    23.120117        True   \n",
       "...                                   ...          ...         ...   \n",
       "(baid.com, baidu.com)            0.889837  2497.234947        True   \n",
       "(emazonaws.com, amazonaws.com)   0.992621   114.690033        True   \n",
       "(1nstagrarn.com, instagram.com)  0.826988  4502.406700        True   \n",
       "(insvagram.com, instagram.com)   0.823689  4641.342072        True   \n",
       "(7outube.com, youtube.com)       0.855983  3189.605362        True   \n",
       "\n",
       "                                 Negative Prediction  Positive Prediction  \n",
       "(tw8itter.com, twitter.com)                     0.00                 1.00  \n",
       "(qoogie.com, google.com)                        0.00                 1.00  \n",
       "(microsuft.com, microsoft.com)                  0.00                 1.00  \n",
       "(facebooc.com, facebook.com)                    0.00                 1.00  \n",
       "(twltter.com, twitter.com)                      0.00                 1.00  \n",
       "...                                              ...                  ...  \n",
       "(baid.com, baidu.com)                           0.47                 0.53  \n",
       "(emazonaws.com, amazonaws.com)                  0.47                 0.53  \n",
       "(1nstagrarn.com, instagram.com)                 0.48                 0.52  \n",
       "(insvagram.com, instagram.com)                  0.48                 0.52  \n",
       "(7outube.com, youtube.com)                      0.49                 0.51  \n",
       "\n",
       "[236 rows x 5 columns]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_df = filtered_df.sort_values(by='Positive Prediction', ascending=False)\n",
    "sorted_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_homographs = list(sorted_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('delete', '8 in position 2', 'i in position 2'), ('replace', 'q in position 0', 'g in position 0'), ('replace', 'i in position 4', 'l in position 4'), ('replace', 'u in position 6', 'o in position 6'), ('replace', 'c in position 7', 'k in position 7'), ('replace', 'l in position 2', 'i in position 2'), ('delete', 'j in position 3', 't in position 3'), ('replace', 'n in position 4', 'b in position 4'), ('replace', '0 in position 5', 'o in position 5'), ('replace', '0 in position 6', 'o in position 6'), ('delete', 'p in position 1', 'o in position 1'), ('replace', 'n in position 5', 'b in position 5'), ('delete', 'h in position 4', 'l in position 4'), ('replace', 'q in position 3', 'g in position 3'), ('replace', 'n in position 5', 'o in position 5'), ('replace', 'm in position 2', 'o in position 2'), ('replace', '7 in position 2', 'u in position 2'), ('delete', 'l in position 2', 'u in position 2'), ('replace', 'l in position 2', 'i in position 2'), ('replace', 'l in position 0', 'i in position 0'), ('replace', 'i in position 0', 'm in position 0'), ('replace', 'm in position 1', 'i in position 1'), ('delete', 'e in position 5', 's in position 5'), ('replace', '0 in position 1', 'o in position 1'), ('replace', '0 in position 6', 'o in position 6'), ('delete', '- in position 6', 'o in position 6'), ('replace', 'e in position 1', 'a in position 1'), ('replace', 'e in position 4', 'o in position 4'), ('delete', 'a in position 7', '. in position 7'), ('delete', 'e in position 6', 'r in position 6'), ('replace', 'y in position 0', 't in position 0'), ('replace', 'g in position 2', 'o in position 2'), ('replace', '5 in position 4', 'u in position 4'), ('delete', 'l in position 6', '. in position 6'), ('replace', 't in position 2', 'u in position 2'), ('delete', 'a in position 6', '. in position 6'), ('delete', 't in position 7', '. in position 7'), ('replace', 'b in position 4', 'u in position 4'), ('replace', 'u in position 5', 'b in position 5'), ('delete', 'n in position 9', '. in position 9'), ('delete', 'x in position 5', '. in position 5'), ('replace', 'g in position 1', 'w in position 1'), ('replace', 'r in position 5', 'g in position 5'), ('replace', 'g in position 6', 'r in position 6'), ('delete', 'r in position 6', '. in position 6'), ('delete', 'o in position 2', 'i in position 2'), ('delete', 'a in position 2', 'i in position 2'), ('delete', 'j in position 2', 'i in position 2'), ('delete', 'f in position 4', 'l in position 4'), ('replace', 'f in position 5', 'b in position 5'), ('delete', 'l in position 7', 'f in position 7'), ('delete', '6 in position 5', '. in position 5'), ('delete', 'j in position 6', '. in position 6'), ('replace', 't in position 3', 'd in position 3'), ('replace', '5 in position 6', 'r in position 6'), ('delete', '4 in position 7', '. in position 7'), ('delete', 't in position 7', '. in position 7'), ('delete', 'p in position 2', 'u in position 2'), ('delete', 'd in position 5', 'b in position 5'), ('replace', 'l in position 6', 'b in position 5'), ('delete', '8 in position 3', 'd in position 3'), ('delete', 'o in position 1', 'i in position 1'), ('replace', 'f in position 3', 'd in position 3'), ('replace', 'j in position 2', 'u in position 2'), ('replace', 'o in position 5', 'n in position 5'), ('delete', 'o in position 7', '. in position 7'), ('replace', 'r in position 7', 'f in position 7'), ('delete', 'r in position 1', 'm in position 1'), ('replace', 'n in position 2', 'm in position 1'), ('replace', 'h in position 5', 'n in position 5'), ('replace', 'i in position 2', 'o in position 2'), ('replace', 'e in position 2', 'u in position 2'), ('delete', 't in position 4', 'l in position 4'), ('replace', 'y in position 2', 'i in position 2'), ('replace', '0 in position 2', 'o in position 2'), ('replace', 'i in position 4', 'l in position 4'), ('delete', '2 in position 7', '. in position 7'), ('delete', 'r in position 5', '. in position 5'), ('replace', 'l in position 0', 'i in position 0'), ('delete', 'r in position 8', 'm in position 8'), ('replace', 'n in position 9', 'm in position 8'), ('delete', 'z in position 2', 'i in position 2'), ('delete', '2 in position 7', '. in position 7'), ('delete', 'y in position 3', 'g in position 3'), ('replace', 'e in position 1', 'a in position 1'), ('replace', 'l in position 7', 'k in position 7'), ('replace', 'r in position 0', 'b in position 0'), ('replace', 'f in position 6', 'r in position 6'), ('delete', 'd in position 5', '. in position 5'), ('replace', 'u in position 1', 'o in position 1'), ('delete', 'k in position 2', 'i in position 2'), ('delete', '1 in position 2', 'i in position 2'), ('replace', '9 in position 4', 'o in position 4'), ('delete', 'a in position 6', 'o in position 6'), ('delete', '9 in position 7', '. in position 7'), ('replace', '9 in position 6', 'o in position 6'), ('delete', '0 in position 7', '. in position 7'), ('delete', 'c in position 3', 'e in position 3'), ('replace', 'z in position 4', 'u in position 4'), ('delete', 'p in position 4', 'l in position 4'), ('replace', 'i in position 2', 'a in position 2'), ('delete', 'k in position 7', '. in position 7'), ('delete', 'p in position 6', '. in position 6'), ('delete', 'd in position 3', 'e in position 3'), ('replace', 'u in position 4', 't in position 4'), ('delete', '1 in position 8', '. in position 8'), ('replace', 'u in position 3', 'e in position 3'), ('delete', 'l in position 5', 'e in position 5'), ('delete', 'g in position 1', 'o in position 1'), ('replace', 'i in position 6', 'a in position 6'), ('delete', 'k in position 1', 'o in position 1'), ('replace', 'u in position 6', 'e in position 6'), ('replace', 'j in position 0', 'i in position 0'), ('replace', 'r in position 5', 'e in position 5'), ('replace', '7 in position 4', 'u in position 4'), ('delete', 'g in position 6', 'e in position 6'), ('delete', 'o in position 2', 'u in position 2'), ('delete', 'l in position 6', 'o in position 6'), ('delete', '6 in position 7', '. in position 7'), ('delete', 'y in position 4', 't in position 4'), ('replace', 'c in position 5', 's in position 5'), ('replace', 'e in position 6', 'o in position 6'), ('delete', '0 in position 8', '. in position 8'), ('insert', 'b in position 3', 'e in position 3'), ('replace', '6 in position 0', 't in position 0'), ('delete', '5 in position 7', '. in position 7'), ('delete', 'a in position 2', 's in position 2'), ('delete', '- in position 1', 'a in position 1'), ('delete', '- in position 6', 'r in position 6'), ('delete', 'x in position 2', 'c in position 2'), ('delete', 'n in position 5', 'b in position 5'), ('insert', 'c in position 1', 'a in position 1'), ('delete', 'k in position 2', 'i in position 2'), ('replace', 'p in position 3', 't in position 3'), ('replace', '9 in position 2', 'i in position 2'), ('delete', 'h in position 8', '. in position 8'), ('delete', 'u in position 2', 'i in position 2'), ('delete', 'w in position 8', '. in position 8'), ('replace', 'g in position 5', 'e in position 5'), ('replace', 'e in position 2', 'i in position 2'), ('replace', 'i in position 8', 'm in position 8'), ('replace', 'h in position 0', 'i in position 0'), ('delete', '6 in position 7', '. in position 7'), ('replace', 'w in position 4', 'u in position 4'), ('delete', 'h in position 5', 'b in position 5'), ('replace', 'z in position 4', 'u in position 4'), ('delete', '. in position 6', 'o in position 6'), ('insert', 's in position 1', 'n in position 1'), ('delete', 'l in position 9', '. in position 9'), ('replace', 'p in position 6', 'o in position 6'), ('delete', '4 in position 5', 'e in position 5'), ('delete', 'a in position 9', '. in position 9'), ('replace', 'a in position 1', 'o in position 1'), ('replace', 'g in position 6', 'e in position 6'), ('delete', 'k in position 2', 'o in position 2'), ('replace', 'f in position 0', 't in position 0'), ('delete', 'i in position 1', 'n in position 1'), ('delete', 'b in position 6', 'e in position 6'), ('delete', 'g in position 5', 'e in position 5'), ('delete', 'z in position 4', 'l in position 4'), ('insert', 'w in position 0', 't in position 0'), ('delete', 'x in position 8', '. in position 8'), ('delete', '- in position 5', 'e in position 5'), ('delete', '8 in position 8', '. in position 8'), ('delete', 'h in position 7', '. in position 7'), ('delete', 'e in position 8', 's in position 8'), ('replace', 'y in position 2', 'u in position 2'), ('replace', '5 in position 8', 't in position 8'), ('replace', 'f in position 7', 'd in position 7'), ('replace', 's in position 2', 'a in position 2'), ('delete', 'r in position 3', 't in position 3'), ('delete', 'r in position 4', 'u in position 4'), ('delete', 'l in position 7', '. in position 7'), ('replace', 'i in position 6', 'o in position 6'), ('replace', 'r in position 3', 'e in position 3'), ('replace', 'b in position 7', 'f in position 7'), ('delete', '4 in position 4', 'o in position 4'), ('delete', 'o in position 9', '. in position 9'), ('replace', 'c in position 8', 's in position 8'), ('delete', '4 in position 9', '. in position 9'), ('replace', '8 in position 2', 'i in position 2'), ('delete', 'e in position 3', 't in position 3'), ('replace', 'e in position 0', 'a in position 0'), ('replace', 'i in position 4', 'a in position 4'), ('replace', 'j in position 7', 'k in position 7'), ('replace', '5 in position 3', 't in position 3'), ('replace', 'k in position 1', 'i in position 1'), ('delete', 'g in position 4', 'u in position 4'), ('delete', 'i in position 4', 'b in position 4'), ('replace', 'l in position 0', 'i in position 0'), ('replace', 'n in position 8', 'm in position 8'), ('delete', 'a in position 7', '. in position 7'), ('replace', 'm in position 0', 'i in position 0'), ('delete', 'g in position 6', '. in position 6'), ('replace', 'e in position 7', 'd in position 7'), ('delete', '6 in position 5', 'e in position 5'), ('insert', 'o in position 0', 'y in position 0'), ('replace', 't in position 2', 'u in position 2'), ('replace', 'u in position 3', 't in position 3'), ('delete', 'c in position 4', 'u in position 4'), ('delete', '- in position 2', 'c in position 2'), ('replace', 'd in position 9', 'e in position 9'), ('delete', '5 in position 7', '. in position 7'), ('delete', 'y in position 7', '. in position 7'), ('delete', 'q in position 1', 'w in position 1'), ('replace', 'u in position 3', 't in position 3'), ('replace', 't in position 4', 'u in position 4'), ('replace', 'z in position 1', 'a in position 1'), ('replace', 'g in position 4', 'o in position 4'), ('replace', 'o in position 2', 'u in position 2'), ('delete', '6 in position 9', '. in position 9'), ('delete', 'b in position 3', 'g in position 3'), ('delete', 'o in position 7', '. in position 7'), ('replace', 'e in position 0', 'a in position 0'), ('delete', 's in position 2', 'a in position 2'), ('insert', 'o in position 0', 'g in position 0'), ('delete', 'l in position 5', 'o in position 5'), ('delete', 'm in position 2', 's in position 2'), ('delete', 'u in position 3', 't in position 3'), ('replace', 'm in position 4', 'o in position 4'), ('delete', 'o in position 2', 'c in position 2'), ('replace', 'm in position 1', 'n in position 1'), ('delete', 'd in position 6', 'r in position 6'), ('delete', '- in position 1', 'a in position 1'), ('delete', 'z in position 2', 's in position 2'), ('delete', 'b in position 6', 'r in position 6'), ('replace', 'z in position 3', 'e in position 3'), ('delete', '. in position 5', 's in position 5'), ('delete', '4 in position 7', 'a in position 7'), ('replace', '9 in position 1', 'i in position 1'), ('delete', 'e in position 8', '. in position 8'), ('delete', 'r in position 8', '. in position 8'), ('replace', 'i in position 7', 'a in position 7'), ('replace', 'j in position 1', 'n in position 1'), ('delete', '8 in position 9', '. in position 9'), ('delete', 'u in position 2', 'i in position 2'), ('replace', 'b in position 3', 'e in position 3'), ('replace', 'e in position 4', 'b in position 4'), ('delete', 'a in position 8', 'm in position 8'), ('insert', 'g in position 2', 'o in position 2'), ('delete', 'o in position 8', '. in position 8'), ('replace', 'o in position 2', 'i in position 2'), ('delete', 'c in position 9', '. in position 9'), ('delete', '. in position 1', 'o in position 1'), ('insert', '. in position 4', 'u in position 4'), ('replace', 'e in position 0', 'a in position 0'), ('replace', '1 in position 0', 'i in position 0'), ('delete', 'r in position 8', 'm in position 8'), ('replace', 'n in position 9', 'm in position 8'), ('replace', 'v in position 3', 't in position 3'), ('replace', '7 in position 0', 'y in position 0')]\n"
     ]
    }
   ],
   "source": [
    "import Levenshtein\n",
    "edits = []\n",
    "for pair in list(top_homographs):\n",
    "    output =  (Levenshtein.editops(pair[0], pair[1]))\n",
    "    for item in output:\n",
    "        edits.append((item[0], f'{pair[0][item[1]]} in position {item[1]}', f'{pair[1][item[2]]} in position {item[2]}'))\n",
    "print (edits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(replace, e in position 0, a in position 0)    3\n",
       "(replace, l in position 0, i in position 0)    3\n",
       "(delete, t in position 7, . in position 7)     2\n",
       "(delete, k in position 2, i in position 2)     2\n",
       "(replace, e in position 1, a in position 1)    2\n",
       "(replace, u in position 3, t in position 3)    2\n",
       "(replace, t in position 2, u in position 2)    2\n",
       "(replace, z in position 4, u in position 4)    2\n",
       "(replace, n in position 9, m in position 8)    2\n",
       "(delete, r in position 8, m in position 8)     2\n",
       "(delete, 6 in position 7, . in position 7)     2\n",
       "(delete, 2 in position 7, . in position 7)     2\n",
       "(delete, o in position 7, . in position 7)     2\n",
       "(delete, u in position 2, i in position 2)     2\n",
       "(replace, 0 in position 6, o in position 6)    2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "series_edits = pd.Series(edits)\n",
    "series_edits.value_counts()[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Levenshtein\n",
    "edits = []\n",
    "for pair in list(top_homographs):\n",
    "    output =  (Levenshtein.editops(pair[0], pair[1]))\n",
    "    for item in output:\n",
    "        edits.append((item[0], pair[0][item[1]], pair[1][item[2]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(replace, e, a)    5\n",
       "(replace, l, i)    5\n",
       "(replace, 0, o)    5\n",
       "(replace, n, m)    4\n",
       "(delete, a, .)     4\n",
       "(delete, 6, .)     4\n",
       "(delete, o, .)     4\n",
       "(replace, i, a)    4\n",
       "(delete, r, m)     3\n",
       "(replace, u, t)    3\n",
       "(delete, l, .)     3\n",
       "(replace, t, u)    3\n",
       "(delete, r, .)     3\n",
       "(delete, l, o)     2\n",
       "(delete, g, e)     2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "series_edits = pd.Series(edits)\n",
    "series_edits.value_counts()[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
